# Machine Learning Toolbox
***
Notes taken during/inspired by the DataCamp course 'Machine Learning Toolbox' by Zachary Deane-Mayer and Max Kuhn.

**_Course Handouts_**

* [Part 1 - Regression models: fitting them and evaluating their performance](./files/MLToolbox/ch1-pdf-slides.pdf)
* [Part 2 - Classification models: fitting them and evaluating their performance] () 
* [Part 3 - Tuning model parameters to improve performance] () 
* [Part 4 - Preprocessing your data] ()
* [Part 5 - Selecting models: a case study in churn prediction] ()

**_Other useful links_**

* [Max Kuhn: Applied Predictive Modeling NYC Talk](https://www.youtube.com/watch?v=dB-JHhEJvQA)
* [Data Chat - Interview With Max Kuhn](https://www.youtube.com/watch?v=YVMlyOh_eyk)
* [The caret package website](https://topepo.github.io/caret/)
* [Bagging graphical explanation](https://www.youtube.com/watch?v=2Mg8QD0F1dQ)
* [Boosting graphical explanation](https://www.youtube.com/watch?v=GM3CDQfQ4sw)

## Regression models: fitting them and evaluating their performance

CAret has been developed by Max for over 10 years.  It automates supervised machine learning (aka predictive modelling) is ML when you have a target variable or something specific you want to predict like species and churn, these could be classification or regression.  We then use metrics to evaluate how accuratley our models predict on new data.  

For linear regression we will use RMSE as our evaluation metric.  Typically this is done on our training (in-sample) data, however this can be too optomistic and lead to over-fitting.  It is better to calculate this out-of-sample using caret.

Next we will calculate in-sample RMSE for the diamonds dataset from ggplot2.

```{r}
# Load the data
diamonds <- ggplot2::diamonds

# Fit lm model: model
model <- lm(price ~ ., diamonds)

# Predict on full data: p
p <- predict(model, diamonds)

# Compute errors: error
error <- p - diamonds$price

# Calculate in-sample RMSE
sqrt(mean(error ^ 2))
```

The course focuses on predictive accuracy, that is to say does the model perform well When presented with new data.  The best way to answer this is to test the model on new data using test data.  This mimics the real world, where you do not actually know the outcome.  We simulate this with a test/train split.

One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.

```{r}
# Set seed
set.seed(42)

# Shuffle row indices: rows
rows <- sample(nrow(diamonds))

# Randomly order data
diamonds <- diamonds[rows,]
```

Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data.

```{r}
# Determine row to split on: split
split <- round(nrow(diamonds) * .80)

# Create train
train <- diamonds[1:split, ]

# Create test
test <- diamonds[(split + 1):nrow(diamonds), ]
```

Now that you have a randomly split training set and test set, you can use the lm() function as you did in the first exercise to fit a model to your training set, rather than the entire dataset. Recall that you can use the formula interface to the linear regression function to fit a model with a specified target variable using all other variables in the dataset as predictors:

> mod <- lm(y ~ ., training_data)

You can use the predict() function to make predictions from that model on new data. The new dataset must have all of the columns from the training data, but they can be in a different order with different values. Here, rather than re-predicting on the training set, you can predict on the test set, which you did not use for training the model. This will allow you to determine the out-of-sample error for the model in the next exercise:

> p <- predict(model, new_data)

```{r}
# Fit lm model on train: model
model <- lm(price ~ ., train)

# Predict on test: p
p <- predict(model,test)
```

Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.

```{r}
# Compute errors: error
error <- (p - test$price)

# Calculate RMSE
sqrt(mean(error^2))
```

### Cross-validation

Using a simple text:train split can be tricky, particularly if there are some outliers in one of the two datasets.  A better approach is to use multiple test sets and average out of sample error.  One way of achieving this is cross-validation where we use folds, 10 folds would mean we split our data in to ten sections with a single observation only occuring once.  Cross validation is only used to calculate error metrics (out of sample), we then start again using on the full data.  With 10 folds, we therefore have 11 models to fit - the 10 re-sampled models plus the final model.  An alterntive which is available in caret is the bootstrap, but in practice this yields similar results to cross-validation.

caret package makes this very easy to do:

> model <- train(y ~ ., my_data)

caret supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the trainControl() function, which you pass to the trControl argument in train():

> model <- train(
  y ~ ., my_data,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 10,
    verboseIter = TRUE
  )
)

It's important to note that you pass the method for modeling to the main train() function and the method for cross-validation to the trainControl() function.

```{r}

library(caret)

# Fit lm model using 10-fold CV: model
model <- train(
  price ~ ., diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 10,
    verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

Next we use a different dataset - Boston house prices - then use a 5 fold cross validation in the trainControl.

```{r}

Boston <- MASS::Boston

# Fit lm model using 5-fold CV: model
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.

One of the awesome things about the train() function in caret is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model's out-of-sample accuracy

```{r}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    repeats = 5, verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

Finally, the model you fit with the train() function has the exact same predict() interface as the linear regression models you fit earlier in this chapter.

After fitting a model with train(), you can simply call predict() with new data, e.g:

```{r}
# Predict on full Boston dataset and display just the first 20 items
head (predict (model, Boston), n = 20)
```

## Classification models: fitting them and evaluating their performance

In classification models you are trying to predict a categorical target e.g. whether a customer is satisfied or not.  It it still a form of supervised learning and we can use a train/test split to evaluate performance.  In this sectgion we will use the Sonar dataset which contains charecteristics of a sonar signal for objects that are either rocks or mines.  

The variables explain some part of the signal, then the class of either Rock or Mine - the data below shows the first 7 variables and the class, for a random sample of 10 rows.  As the dataset is very small, we use a 40% test split.

```{r}
library(mlbench)
data(Sonar)
Sonar[sample(1:nrow(Sonar), 10, replace=FALSE), c(1:7, 61)]
```

TO mkae our 60/40 split we do the following.

```{r}
# Shuffle row indices: rows
rows <- sample(nrow(Sonar))

# Randomly order data: Sonar
Sonar <- Sonar[rows, ]

# Identify row to split on: split
split <- round(nrow(Sonar) * 0.6)

# Create train
train <- Sonar[1:split, ]

# Create test
test <- Sonar[(split + 1):nrow(Sonar), ]
```

Now you can fit a logistic regression model to your training set using the glm() function. glm() is a more advanced version of lm() that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.

Don't worry about warnings like glm.fit: algorithm did not converge or glm.fit: fitted probabilities numerically 0 or 1 occurred. These are common on smaller datasets and usually don't cause any issues. They typically mean your dataset is perfectly seperable, which can cause problems for the math behind the model, but R's glm() function is almost always robust enough to handle this case with no problems.

```{r}
# Fit glm model: model
model <- glm(formula = Class ~ ., family = "binomial", data = train)

# Predict on test: p
p <- predict(model, test, type = "response")
```

### Confusion Matrix

A confusion Matrix is a table of the predicted results vs the actual results.  It is called a Confusion Matrix as it explains how confused the model is between the two classes and highlights instances where one class is confused for the other.  The items on the main diagonal are the cases where the model is correct. 

```{r Confusion Matrix, echo = FALSE, fig.cap='Confusion Matrix'}
knitr::include_graphics("images/MLToolbox/Confusion.png")
```

You could make such a contingency table with the table() function in base R, but confusionMatrix() in caret yields a lot of useful ancillary statistics in addition to the base rates in the table. 

```{r}
# Calculate class probabilities: p_class
p_class <- ifelse(p > .5, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test$Class)
```

We see that the No Information rate is around 52%, that is just predicting the dominant class all the time, mines.  Our accuracy so far is below this which suggests that using a dummy model that always predicts mines is more accurate than the current model.  The other values such as sensitivity and specificity give more elements of the confusion matrix:

* Specificity - true negative rate  
* Sensitivity - true positive rate

Setting our threshold rate - currently 50% - is an exercise in deciding what is more important and depends on the cost-benefit analysis of the problem at hand.  Do we want to flag more non-mines as mines, or do we want to be more accurate in our prediction but potentially miss some mines?

Now what happens if we apply a 90% threshold to our model?

```{r}
# Apply threshold of 0.9: p_class
p_class <- ifelse(p > .9, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test$Class)
```
Or at 10%

```{r}
# Apply threshold of 0.10: p_class
p_class <- ifelse(p > .1, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test$Class)
```

### The ROC Curve

Rather than manually calculating the true positive and true negative rate for many hundreds of different models - at carying classification cut offs as we previously did - we can plot a ROC (Receiver Operator Charecteristic) curve automatically for every possible threshold.  Originally the ROC curve was designed to look at the trade off with different thresholds when trying to differentiate between radar signals of flocks of birds vs planes.

The ROC curve plots the false positive rate (X Axis) vs the true positive rate (Y axis) with each point representing a different threshold.

There are a number of different ways of calculating ROC curves, one we will use here is the caTools package and calculates the AUC or Area under the curve.

```{r}
library(caTools)

# Predict on test: p
p <- predict(model, test, type = "response")

# Make ROC curve
colAUC(p, test$Class, plotROC = TRUE)
```

Whne looking at the ROC curve, a line close to the diagonal would be considered bad as it is following something not far off random predictions.  Perfection seperation would produce a box, with a single point at 1, 0 or the top left hand corner which would be a 100% true positive rate and a 0% false positive rate.

The AUC for the 'perfect' model is 1.  For the random model it is 0.5.  The AUC statistics is a single number accuracy measure, it summarises the performance of the model across all possible classification thresholds.  Most models are in the 0.5 to 1 range, some bad models can be in the 0.4 range.  We can think of them as an exam grade e.g.

* A = 0.9  
* B = 0.8  
* C = 0.7  
* ...  
* F = 0.5  

0.8 or above is good, some models in the 0.7 range are useful.  The caret package will calculate the AUC for us.

You can use the trainControl() function in caret to use AUC (instead of acccuracy), to tune the parameters of your models. The twoClassSummary() convenience function allows you to do this easily.

When using twoClassSummary(), be sure to always include the argument classProbs = TRUE or your model will throw an error. You cannot calculate AUC with just class predictions. You need to have class probabilities as well.

```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
```

Now that you have a custom trainControl object, it's easy to fit caret models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom trainControl object to the train() function via the trControl argument, e.g.:

> train(<standard arguments here>, trControl = myControl)

This syntax gives you a convenient way to store a lot of custom modeling parameters and then use them across multiple different calls to train()

```{r}
# Train glm with custom trainControl: model
model <- train(Class ~ ., data = Sonar, 
                 method = "glm", 
                 trControl = myControl)

# Print model to console
model
```

### Tuning model parameters to improve performance

Next we will look at random forests as these are robust against over-fitting.  They can yield accurate, non-linear models without much additional work.  However, unlinke linear models they have so called hyperparameters to tune and they have to be manually specified by the data scientist as inputs in to the model.  These parameters can vary from dataset to dataset, the defaults can be ok, but often need adjusting.  

With random forests we fit a different tree to each bootstrap sample of the data, called bagging - we can think of this as draw a random 'bag' of observations, at random with replacement, from the original TRAIN dataset many times - to which we think fit a tree on this subset of data.  This effectivly creates multiple train/test splits on the TRAIN data and trains a different model, so we end up with an ensemble of different models.  

Note - boosting is similar to bagging, in that we take a random 'bag' of data and fit a model.  However we fit the models iteratively with boosting, with subsequent models fitted to those observations which were not previously fitted as well.  When we draw another sample or bag, we weight the probability of observation selection based on how well the previous model fitted those observations, if they were poorly fitted, the new model is more likely to sample those and therefore more likely to accuratley predict them.  We then re-inputt all the TRAIN data to our two models and see which observations were then, based on this two model ensemble, not predicted as well.  When then try to build a new model in which these previously under-predicted observations are better predicted, then build the ensemble again and iterate as neccessary.

Random forests take bagging a step further, by randomly sampling the columns (variables) at each split and helps to yield more accurate models.