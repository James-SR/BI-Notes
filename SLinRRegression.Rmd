# Supervised Learning In R Regression
***
Notes taken during/inspired by the DataCamp course 'Supervised Learning In R Classification' by Nina Zumel and John Mount.

**_Course Handouts_**

* [Part 1 - What is Regression?](./files/SLinRRegression/chapter1.pdf)
* [Part 2 - Training and Evaluating Regression Models] (./files/SLinRRegression/chapter2.pdf)


**_Other useful links_**

* []()

## What is Regression?

In this course we are interested in predicting numerical outcomes (rather than discreet) from a set of input variables.  Classification on the other hand is the task of making discreet predictions.  WE can break down the modelling process in to two camps:

* Scientific mindset - we try to understand causal mechanisms 
* Engineering mindset - we try to accuratley predict

Machine Learning is more in line with the Engineering mindset.

In the first task, the goal is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is female_unemployment, and the input is male_unemployment.

```{r}
# Load the unemployment data
unemployment <- readRDS("./files/SLinRRegression/unemployment.rds")

# unemployment is loaded in the workspace
summary(unemployment)

# Define a formula to express female_unemployment as a function of male_unemployment
fmla <- as.formula("female_unemployment ~ male_unemployment")

# Print it
fmla

# Use the formula to fit a model: unemployment_model
unemployment_model <- lm(formula = fmla, data = unemployment)

# Print it
unemployment_model
```

We can see the relationship is positive - female unemployment increases as male unemployment does.  We can then use some diagnostics on the model.  There are different ways and packages to achieve this.

```{r}
# Print unemployment_model
unemployment_model

# Call summary() on unemployment_model to get more details
summary(unemployment_model)

# Call glance() on unemployment_model to see the details in a tidier form
broom::glance(unemployment_model)

# Call wrapFTest() on unemployment_model to see the most relevant details
sigr::wrapFTest(unemployment_model)
```

In the next section we use the unemployment model unemployment_model to make predictions from the unemployment data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, unemployment. You will also use your model to predict on the new data in newrates.

```{r}
newrates <- data.frame(male_unemployment = 5)

# Predict female unemployment in the unemployment data set
unemployment$prediction <-  predict(unemployment_model, unemployment)

# load the ggplot2 package
library(ggplot2)

# Make a plot to compare predictions to actual (prediction on x axis). 
ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
  geom_point() +
  geom_abline(color = "blue")

# Predict female unemployment rate when male unemployment is 5%
pred <- predict(unemployment_model, newrates)
# Print it
pred
```

Next we will look at a model of blood pressure as a function of age and weight.

```{r}
# Load the blood data
bloodpressure <- readRDS("./files/SLinRRegression/bloodpressure.rds")

# bloodpressure is in the workspace
summary(bloodpressure)

# Create the formula and print it
fmla <- as.formula("blood_pressure ~ age + weight")
fmla

# Fit the model: bloodpressure_model
bloodpressure_model <- lm(fmla, bloodpressure)

# Print bloodpressure_model and call summary() 
bloodpressure_model
summary(bloodpressure_model)
```

Both variables are positive suggesting that increases in them also lead to increases in blood pressure.

Next we can use the model to make preditions.

```{r}
# predict blood pressure using bloodpressure_model :prediction
bloodpressure$prediction <- predict(bloodpressure_model, bloodpressure)

# plot the results
ggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + 
    geom_point() +
    geom_abline(color = "blue")
```

** Key Points **

* Linear models are easy to fit and can be less prone to overfitting.  
* They are also generally easier to understand
* However they cannot express complex relationships in the data
* In addition we can get collinearity with some variables e.g. weight increases with age

## Training and Evaluating Regression Models

A model with a good fit will have its points close to the line.  Models which don't fit well will have points which systemtically don't fit welland are all over the place. This may mean you are missing variables in your model.  A residual plot can help understand if there are any systematic errors, we are typically looking for a random cloud of residuals rather than anything which may resemble a trend.  We can also use a Gain Curve Plot

```{r}
summary(unemployment)

# Make predictions from the model
unemployment$predictions <- predict(unemployment_model, unemployment)

# Plot predictions (on x-axis) versus the female_unemployment rates
ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
  geom_point() + 
  geom_abline()

# Calculate residuals
unemployment$residuals <- unemployment$female_unemployment - unemployment$predictions

# Predictions (on x-axis) versus the actuals with residuals
ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + geom_point() +
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("actuals vs. linear model prediction with residual")

# Fill in the blanks to plot predictions (on x-axis) versus the residuals
ggplot(unemployment, aes(x = predictions, y = residuals)) + 
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("residuals vs. linear model prediction")
```

We can also plot a gain curve using the WVPlots package.  The syntax is:

> GainCurvePlot(frame, xvar, truthvar, title)

where

* frame is a data frame
* xvar and truthvar are strings naming the prediction and actual outcome columns of frame
* title is the title of the plot

```{r}
library(WVPlots)

# Plot the Gain Curve
GainCurvePlot(unemployment, "predictions", "female_unemployment", "Unemployment model")
```

A relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones.

Another way to evaluate our model is to use the RMSE. You want RMSE to be small. One heuristic is to compare the RMSE to the standard deviation of the outcome. With a good model, the RMSE should be smaller.

```{r}
# For convenience put the residuals in the variable res
res <- unemployment$residuals

# Calculate RMSE, assign it to the variable rmse and print it
(rmse <- sqrt(mean(res^2)))

# Calculate the standard deviation of female_unemployment and print it
(sd_unemployment <- sd(unemployment$female_unemployment))

# Calculate it as a fraction < 1 is good
rmse / sd_unemployment

```

An RMSE much smaller than the outcome's standard deviation suggests a model that predicts well.

Another way of evaluating a model is R squared. We can calculate this as the 1 - RSS / TSS RSS = Residual Sum of Squares TSS is Total Sum of Squares.

```{r R2, echo = FALSE, fig.cap='R Squared'}
knitr::include_graphics("images/SLinRRegression/R2.png")
```

Most packages or regression functions come with built in R squared metrics, the summary will print it out or we can use the glance() function from broom.  Here we calculate them manually.

```{r}
# Calculate mean female_unemployment: fe_mean. Print it
(fe_mean <- mean(unemployment$female_unemployment))

# Calculate total sum of squares: tss. Print it
(tss <- sum((unemployment$female_unemployment - fe_mean)^2))

# Calculate residual sum of squares: rss. Print it
(rss <- sum((unemployment$residuals)^2))

# Calculate R-squared: rsq. Print it. Is it a good fit?
(rsq <- 1 - (rss/tss))

# Get R-squared from glance. Print it
(rsq_glance <- broom::glance(unemployment_model)$r.squared)
```

Correlation or rho shows the strength of the linear relationship between two variables.

```{r}
# Get the correlation between the prediction and true outcome: rho and print it
(rho <- cor(unemployment$predictions, unemployment$female_unemployment))
rho

# Square rho: rho2 and print it
(rho2 <- rho ^ 2)

# Get R-squared from glance and print it
(rsq_glance <- broom::glance(unemployment_model)$r.squared)
```

So far we have only looked at how well our data fits the observed variables, without new data.  A better way of evaluating our model is to split out data in to a training and test set of data, then see how well the model predicts to the test data, having built the model on the train data.  WE want our RMSE on the test data to be similar to the RMSE on the train data, if it is much lower than we may have overfitted our model.  If the number of observations is too small to do a split, we can use cross validation to acheive a similar result. 

In the following code we will split mpg into a training set mpg_train (75% of the data) and a test set mpg_test (25% of the data). One way to do this is to generate a column of uniform random numbers between 0 and 1, using the function runif().  There other ways, such as sample (see Supervised LEarning In R - Classification notes, do search for 0.75).  

If using run if, we do:

* Generate a vector of uniform random numbers: gp = runif(N).
* dframe[gp < X,] will be about the right size.
* dframe[gp >= X,] will be the complement.

```{r}
# Load the mpg data
mpg <- ggplot2::mpg

# take a look at the data
summary(mpg)
dim(mpg)

# Use nrow to get the number of rows in mpg (N) and print it
(N <- nrow(mpg))
N

# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer
(target <- round(N * 0.75))
target

# Create the vector of N uniform random variables: gp
gp <- runif(N)

# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
mpg_train <- mpg[gp < 0.75, ]
mpg_test <- mpg[gp >= 0.75, ]

# Use nrow() to examine mpg_train and mpg_test
nrow(mpg_train)
nrow(mpg_test)

```

It is likely our target number of rows will slightly different than what is sampled, but they should be close enough.

Next we use these datasets to create models for prediction.

```{r}
# mpg_train is in the workspace
summary(mpg_train)

# Create a formula to express cty as a function of hwy: fmla and print it.
(fmla <- as.formula("cty ~ hwy"))

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy 
mpg_model <- lm(fmla, data = mpg_train)

# Use summary() to examine the model
summary(mpg_model)
```

Next we will test the model mpg_model on the test data, mpg_test.  We will use two functions rather than calculate the rmse and r_squared manually:

* Metrics::rmse(predcol, ycol)
* tsensembler::r_squared(r_squared(y, y_hat)

```{r}

# predict cty from hwy for the training set
mpg_train$pred <- predict(mpg_model, mpg_train)

# predict cty from hwy for the test set
mpg_test$pred <- predict(mpg_model, mpg_test)

# Evaluate the rmse on both training and test data and print them
(rmse_train <- Metrics::rmse(mpg_train$pred, mpg_train$cty))
(rmse_test <-  Metrics::rmse(mpg_test$pred, mpg_test$cty))


# Evaluate the r-squared on both training and test data.and print them
(rsq_train <- tsensembler::r_squared(mpg_train$pred, mpg_train$cty))
(rsq_test <- tsensembler::r_squared(mpg_test$pred, mpg_test$cty))

# Plot the predictions (on the x-axis) against the outcome (cty) on the test data
ggplot(mpg_test, aes(x = pred, y = cty)) + 
  geom_point() + 
  geom_abline()
```

