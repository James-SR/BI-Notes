<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>BI Notes</title>
  <meta name="description" content="Study notes taken from BI courses and self learning.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="BI Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/BI-Notes" />
  
  <meta property="og:description" content="Study notes taken from BI courses and self learning." />
  <meta name="github-repo" content="James-SR/BI-Notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="BI Notes" />
  
  <meta name="twitter:description" content="Study notes taken from BI courses and self learning." />
  

<meta name="author" content="James Solomon-Rounce">


<meta name="date" content="2018-04-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="supervised-learning-in-r-classification.html">
<link rel="next" href="machine-learning-toolbox.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Business Intelligence Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html"><i class="fa fa-check"></i><b>1</b> Querying Data with Transact-SQL</a><ul>
<li class="chapter" data-level="1.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#introduction-to-transact-sql"><i class="fa fa-check"></i><b>1.1</b> Introduction to Transact-SQL</a><ul>
<li class="chapter" data-level="1.1.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#data-types"><i class="fa fa-check"></i><b>1.1.1</b> Data Types</a></li>
<li class="chapter" data-level="1.1.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#working-with-nulls"><i class="fa fa-check"></i><b>1.1.2</b> Working with NULLs</a></li>
<li class="chapter" data-level="1.1.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises"><i class="fa fa-check"></i><b>1.1.3</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#querying-tables-with-select"><i class="fa fa-check"></i><b>1.2</b> Querying Tables with SELECT</a><ul>
<li class="chapter" data-level="1.2.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#removing-duplicates"><i class="fa fa-check"></i><b>1.2.1</b> Removing Duplicates</a></li>
<li class="chapter" data-level="1.2.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#sorting-results"><i class="fa fa-check"></i><b>1.2.2</b> Sorting Results</a></li>
<li class="chapter" data-level="1.2.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#paging-through-results"><i class="fa fa-check"></i><b>1.2.3</b> Paging through results</a></li>
<li class="chapter" data-level="1.2.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#filtering-and-using-predicates"><i class="fa fa-check"></i><b>1.2.4</b> Filtering and Using Predicates</a></li>
<li class="chapter" data-level="1.2.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-1"><i class="fa fa-check"></i><b>1.2.5</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#querying-tables-with-joins"><i class="fa fa-check"></i><b>1.3</b> Querying Tables with Joins</a><ul>
<li class="chapter" data-level="1.3.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#inner-joins"><i class="fa fa-check"></i><b>1.3.1</b> INNER Joins</a></li>
<li class="chapter" data-level="1.3.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#outer-joins"><i class="fa fa-check"></i><b>1.3.2</b> OUTER Joins</a></li>
<li class="chapter" data-level="1.3.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#cross-joins"><i class="fa fa-check"></i><b>1.3.3</b> Cross Joins</a></li>
<li class="chapter" data-level="1.3.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#self-joins"><i class="fa fa-check"></i><b>1.3.4</b> Self Joins</a></li>
<li class="chapter" data-level="1.3.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-2"><i class="fa fa-check"></i><b>1.3.5</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-set-operators"><i class="fa fa-check"></i><b>1.4</b> Using Set Operators</a><ul>
<li class="chapter" data-level="1.4.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#intersect-and-except-queries"><i class="fa fa-check"></i><b>1.4.1</b> INTERSECT and EXCEPT Queries</a></li>
<li class="chapter" data-level="1.4.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-3"><i class="fa fa-check"></i><b>1.4.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-functions-and-aggregating-data"><i class="fa fa-check"></i><b>1.5</b> Using Functions and Aggregating Data</a><ul>
<li class="chapter" data-level="1.5.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#scalar-functions"><i class="fa fa-check"></i><b>1.5.1</b> Scalar Functions</a></li>
<li class="chapter" data-level="1.5.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#logical-functions"><i class="fa fa-check"></i><b>1.5.2</b> Logical Functions</a></li>
<li class="chapter" data-level="1.5.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#window-functions"><i class="fa fa-check"></i><b>1.5.3</b> Window Functions</a></li>
<li class="chapter" data-level="1.5.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#aggregate-functions"><i class="fa fa-check"></i><b>1.5.4</b> Aggregate Functions</a></li>
<li class="chapter" data-level="1.5.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#filtering-groups"><i class="fa fa-check"></i><b>1.5.5</b> Filtering Groups</a></li>
<li class="chapter" data-level="1.5.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-4"><i class="fa fa-check"></i><b>1.5.6</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#sub-queries-and-apply"><i class="fa fa-check"></i><b>1.6</b> Sub-queries and Apply</a><ul>
<li class="chapter" data-level="1.6.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#self-contained-or-correlated-query"><i class="fa fa-check"></i><b>1.6.1</b> Self Contained or Correlated Query</a></li>
<li class="chapter" data-level="1.6.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#the-apply-operator"><i class="fa fa-check"></i><b>1.6.2</b> The Apply Operator</a></li>
<li class="chapter" data-level="1.6.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-5"><i class="fa fa-check"></i><b>1.6.3</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-table-expressions"><i class="fa fa-check"></i><b>1.7</b> Using Table Expressions</a><ul>
<li class="chapter" data-level="1.7.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#views"><i class="fa fa-check"></i><b>1.7.1</b> Views</a></li>
<li class="chapter" data-level="1.7.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-temporary-tables-and-table-variables"><i class="fa fa-check"></i><b>1.7.2</b> Using Temporary Tables and Table Variables</a></li>
<li class="chapter" data-level="1.7.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#table-value-functions-tvf"><i class="fa fa-check"></i><b>1.7.3</b> Table Value Functions (TVF)</a></li>
<li class="chapter" data-level="1.7.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#derived-tables"><i class="fa fa-check"></i><b>1.7.4</b> Derived Tables</a></li>
<li class="chapter" data-level="1.7.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-common-table-expressions-ctes"><i class="fa fa-check"></i><b>1.7.5</b> Using Common Table Expressions (CTEs)</a></li>
<li class="chapter" data-level="1.7.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-6"><i class="fa fa-check"></i><b>1.7.6</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#grouping-sets-and-pivoting-data"><i class="fa fa-check"></i><b>1.8</b> Grouping Sets and Pivoting Data</a><ul>
<li class="chapter" data-level="1.8.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#pivoting-data"><i class="fa fa-check"></i><b>1.8.1</b> Pivoting Data</a></li>
<li class="chapter" data-level="1.8.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-7"><i class="fa fa-check"></i><b>1.8.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#modifying-data"><i class="fa fa-check"></i><b>1.9</b> Modifying Data</a><ul>
<li class="chapter" data-level="1.9.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#updating-and-deleting-data"><i class="fa fa-check"></i><b>1.9.1</b> Updating and Deleting Data</a></li>
<li class="chapter" data-level="1.9.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-8"><i class="fa fa-check"></i><b>1.9.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#programming-with-transact-sql"><i class="fa fa-check"></i><b>1.10</b> Programming with Transact-SQL</a><ul>
<li class="chapter" data-level="1.10.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#batches"><i class="fa fa-check"></i><b>1.10.1</b> Batches</a></li>
<li class="chapter" data-level="1.10.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#comments"><i class="fa fa-check"></i><b>1.10.2</b> Comments</a></li>
<li class="chapter" data-level="1.10.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#variables"><i class="fa fa-check"></i><b>1.10.3</b> Variables</a></li>
<li class="chapter" data-level="1.10.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#conditional-branching"><i class="fa fa-check"></i><b>1.10.4</b> Conditional Branching</a></li>
<li class="chapter" data-level="1.10.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#looping"><i class="fa fa-check"></i><b>1.10.5</b> Looping</a></li>
<li class="chapter" data-level="1.10.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#stored-procedures"><i class="fa fa-check"></i><b>1.10.6</b> Stored Procedures</a></li>
<li class="chapter" data-level="1.10.7" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-9"><i class="fa fa-check"></i><b>1.10.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#error-handling-and-transactions"><i class="fa fa-check"></i><b>1.11</b> Error Handling and Transactions</a><ul>
<li class="chapter" data-level="1.11.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#raising-or-throwing-errors"><i class="fa fa-check"></i><b>1.11.1</b> Raising or Throwing Errors</a></li>
<li class="chapter" data-level="1.11.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#catching-and-handling-errors"><i class="fa fa-check"></i><b>1.11.2</b> Catching and Handling Errors</a></li>
<li class="chapter" data-level="1.11.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#transactions"><i class="fa fa-check"></i><b>1.11.3</b> Transactions</a></li>
<li class="chapter" data-level="1.11.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-10"><i class="fa fa-check"></i><b>1.11.4</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#final-assessment"><i class="fa fa-check"></i><b>1.12</b> Final Assessment</a><ul>
<li class="chapter" data-level="1.12.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-1"><i class="fa fa-check"></i><b>1.12.1</b> Section 1</a></li>
<li class="chapter" data-level="1.12.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-2"><i class="fa fa-check"></i><b>1.12.2</b> Section 2</a></li>
<li class="chapter" data-level="1.12.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-3"><i class="fa fa-check"></i><b>1.12.3</b> Section 3</a></li>
<li class="chapter" data-level="1.12.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-4"><i class="fa fa-check"></i><b>1.12.4</b> Section 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html"><i class="fa fa-check"></i><b>2</b> Supervised Learning In R Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>2.1</b> k-Nearest Neighbors (kNN)</a></li>
<li class="chapter" data-level="2.2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.2</b> Naive Bayes</a><ul>
<li class="chapter" data-level="2.2.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#putting-the-naivety-in-naive-bayes"><i class="fa fa-check"></i><b>2.2.1</b> Putting the Naivety in Naive Bayes</a></li>
<li class="chapter" data-level="2.2.2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#applying-naive-bayes-nb-to-other-problems"><i class="fa fa-check"></i><b>2.2.2</b> Applying Naive Bayes (NB) to other problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#logistic-regression---binary-predictions-with-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression - binary predictions with regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#dummy-variables-missing-data-and-interactions"><i class="fa fa-check"></i><b>2.3.1</b> Dummy variables, missing data, and interactions</a></li>
<li class="chapter" data-level="2.3.2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#automatic-feature-selection"><i class="fa fa-check"></i><b>2.3.2</b> Automatic feature selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#classification-trees"><i class="fa fa-check"></i><b>2.4</b> Classification Trees</a><ul>
<li class="chapter" data-level="2.4.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#tending-to-classification-trees"><i class="fa fa-check"></i><b>2.4.1</b> Tending to classification trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html"><i class="fa fa-check"></i><b>3</b> Supervised Learning In R Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#what-is-regression"><i class="fa fa-check"></i><b>3.1</b> What is Regression?</a></li>
<li class="chapter" data-level="3.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#training-and-evaluating-regression-models"><i class="fa fa-check"></i><b>3.2</b> Training and Evaluating Regression Models</a></li>
<li class="chapter" data-level="3.3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#issues-to-consider"><i class="fa fa-check"></i><b>3.3</b> Issues to Consider</a><ul>
<li class="chapter" data-level="3.3.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#interactions"><i class="fa fa-check"></i><b>3.3.1</b> Interactions</a></li>
<li class="chapter" data-level="3.3.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#transforming-the-response-before-modeling"><i class="fa fa-check"></i><b>3.3.2</b> Transforming the response before modeling</a></li>
<li class="chapter" data-level="3.3.3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#transforming-input-variables"><i class="fa fa-check"></i><b>3.3.3</b> Transforming Input variables</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#dealing-with-non-linear-responses"><i class="fa fa-check"></i><b>3.4</b> Dealing with Non-Linear Responses</a><ul>
<li class="chapter" data-level="3.4.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#count-data-with-poisson-and-quasipoisson-regression"><i class="fa fa-check"></i><b>3.4.1</b> Count data with poisson and quasipoisson regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#generalised-additive-model-gam"><i class="fa fa-check"></i><b>3.4.2</b> Generalised Additive Model (GAM)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#tree-based-methods"><i class="fa fa-check"></i><b>3.5</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="3.5.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#random-forests"><i class="fa fa-check"></i><b>3.5.1</b> Random Forests</a></li>
<li class="chapter" data-level="3.5.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#one-hot-encoding"><i class="fa fa-check"></i><b>3.5.2</b> One-hot encoding</a></li>
<li class="chapter" data-level="3.5.3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>3.5.3</b> Gradient Boosting Machines</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html"><i class="fa fa-check"></i><b>4</b> Machine Learning Toolbox</a><ul>
<li class="chapter" data-level="4.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#regression-models-fitting-them-and-evaluating-their-performance"><i class="fa fa-check"></i><b>4.1</b> Regression models: fitting them and evaluating their performance</a><ul>
<li class="chapter" data-level="4.1.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#cross-validation"><i class="fa fa-check"></i><b>4.1.1</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#classification-models-fitting-them-and-evaluating-their-performance"><i class="fa fa-check"></i><b>4.2</b> Classification models: fitting them and evaluating their performance</a><ul>
<li class="chapter" data-level="4.2.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#confusion-matrix"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.2.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#the-roc-curve"><i class="fa fa-check"></i><b>4.2.2</b> The ROC Curve</a></li>
<li class="chapter" data-level="4.2.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#tuning-model-parameters-to-improve-performance"><i class="fa fa-check"></i><b>4.2.3</b> Tuning model parameters to improve performance</a></li>
<li class="chapter" data-level="4.2.4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#introducing-glmnet"><i class="fa fa-check"></i><b>4.2.4</b> Introducing glmnet</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#preprocessing-your-data"><i class="fa fa-check"></i><b>4.3</b> Preprocessing your data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#mutiple-pre-processing-steps"><i class="fa fa-check"></i><b>4.3.1</b> Mutiple pre-processing steps</a></li>
<li class="chapter" data-level="4.3.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#handling-low-information-predictors"><i class="fa fa-check"></i><b>4.3.2</b> Handling low-Information Predictors</a></li>
<li class="chapter" data-level="4.3.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#principle-components-analysis-pca"><i class="fa fa-check"></i><b>4.3.3</b> Principle Components Analysis (PCA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#selecting-models-a-case-study-in-churn-prediction"><i class="fa fa-check"></i><b>4.4</b> Selecting models: a case study in churn prediction</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">BI Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning-in-r-regression" class="section level1">
<h1><span class="header-section-number">3</span> Supervised Learning In R Regression</h1>
<hr />
<p>Notes taken during/inspired by the DataCamp course ‘Supervised Learning In R Classification’ by Nina Zumel and John Mount.</p>
<p><strong><em>Course Handouts</em></strong></p>
<ul>
<li><a href="./files/SLinRRegression/chapter1.pdf">Part 1 - What is Regression?</a></li>
<li>[Part 2 - Training and Evaluating Regression Models] (./files/SLinRRegression/chapter2.pdf)</li>
<li>[Part 3 - Issues to Consider] (./files/SLinRRegression/chapter3.pdf)</li>
<li>[Part 4 - Dealing with Non-Linear Responses] (./files/SLinRRegression/chapter4.pdf)</li>
<li>[Part 5 - Tree-Based Methods] (./files/SLinRRegression/chapter5.pdf)</li>
</ul>
<p><strong><em>Other useful links</em></strong></p>
<ul>
<li><a href="http://appliedpredictivemodeling.com/blog/2013/10/23/the-basics-of-encoding-categorical-data-for-predictive-models">The Basics of Encoding Categorical Data for Predictive Models</a></li>
</ul>
<div id="what-is-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> What is Regression?</h2>
<p>In this course we are interested in predicting numerical outcomes (rather than discreet) from a set of input variables. Classification on the other hand is the task of making discreet predictions. WE can break down the modelling process in to two camps:</p>
<ul>
<li>Scientific mindset - we try to understand causal mechanisms</li>
<li>Engineering mindset - we try to accuratley predict</li>
</ul>
<p>Machine Learning is more in line with the Engineering mindset.</p>
<p>In the first task, the goal is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is female_unemployment, and the input is male_unemployment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the unemployment data</span>
unemployment &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./files/SLinRRegression/unemployment.rds&quot;</span>)

<span class="co"># unemployment is loaded in the workspace</span>
<span class="kw">summary</span>(unemployment)</code></pre></div>
<pre><code>##  male_unemployment female_unemployment
##  Min.   :2.900     Min.   :4.000      
##  1st Qu.:4.900     1st Qu.:4.400      
##  Median :6.000     Median :5.200      
##  Mean   :5.954     Mean   :5.569      
##  3rd Qu.:6.700     3rd Qu.:6.100      
##  Max.   :9.800     Max.   :7.900</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define a formula to express female_unemployment as a function of male_unemployment</span>
fmla &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;female_unemployment ~ male_unemployment&quot;</span>)

<span class="co"># Print it</span>
fmla</code></pre></div>
<pre><code>## female_unemployment ~ male_unemployment</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use the formula to fit a model: unemployment_model</span>
unemployment_model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> fmla, <span class="dt">data =</span> unemployment)

<span class="co"># Print it</span>
unemployment_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = unemployment)
## 
## Coefficients:
##       (Intercept)  male_unemployment  
##            1.4341             0.6945</code></pre>
<p>We can see the relationship is positive - female unemployment increases as male unemployment does. We can then use some diagnostics on the model. There are different ways and packages to achieve this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print unemployment_model</span>
unemployment_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = unemployment)
## 
## Coefficients:
##       (Intercept)  male_unemployment  
##            1.4341             0.6945</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call summary() on unemployment_model to get more details</span>
<span class="kw">summary</span>(unemployment_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = unemployment)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77621 -0.34050 -0.09004  0.27911  1.31254 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        1.43411    0.60340   2.377   0.0367 *  
## male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5803 on 11 degrees of freedom
## Multiple R-squared:  0.8213, Adjusted R-squared:  0.8051 
## F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call glance() on unemployment_model to see the details in a tidier form</span>
broom<span class="op">::</span><span class="kw">glance</span>(unemployment_model)</code></pre></div>
<pre><code>##   r.squared adj.r.squared     sigma statistic      p.value df    logLik
## 1 0.8213157     0.8050716 0.5802596  50.56108 1.965985e-05  2 -10.28471
##        AIC      BIC deviance df.residual
## 1 26.56943 28.26428 3.703714          11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call wrapFTest() on unemployment_model to see the most relevant details</span>
sigr<span class="op">::</span><span class="kw">wrapFTest</span>(unemployment_model)</code></pre></div>
<pre><code>## [1] &quot;F Test summary: (R2=0.821, F(1,11)=50.6, p=2e-05).&quot;</code></pre>
<p>In the next section we use the unemployment model unemployment_model to make predictions from the unemployment data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, unemployment. You will also use your model to predict on the new data in newrates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newrates &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">male_unemployment =</span> <span class="dv">5</span>)

<span class="co"># Predict female unemployment in the unemployment data set</span>
unemployment<span class="op">$</span>prediction &lt;-<span class="st">  </span><span class="kw">predict</span>(unemployment_model, unemployment)

<span class="co"># load the ggplot2 package</span>
<span class="kw">library</span>(ggplot2)

<span class="co"># Make a plot to compare predictions to actual (prediction on x axis). </span>
<span class="kw">ggplot</span>(unemployment, <span class="kw">aes</span>(<span class="dt">x =</span> prediction, <span class="dt">y =</span> female_unemployment)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predict female unemployment rate when male unemployment is 5%</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(unemployment_model, newrates)
<span class="co"># Print it</span>
pred</code></pre></div>
<pre><code>##        1 
## 4.906757</code></pre>
<p>Next we will look at a model of blood pressure as a function of age and weight.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the blood data</span>
bloodpressure &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./files/SLinRRegression/bloodpressure.rds&quot;</span>)

<span class="co"># bloodpressure is in the workspace</span>
<span class="kw">summary</span>(bloodpressure)</code></pre></div>
<pre><code>##  blood_pressure       age            weight   
##  Min.   :128.0   Min.   :46.00   Min.   :167  
##  1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
##  Median :153.0   Median :64.00   Median :194  
##  Mean   :150.1   Mean   :62.45   Mean   :195  
##  3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
##  Max.   :168.0   Max.   :74.00   Max.   :220</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the formula and print it</span>
fmla &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;blood_pressure ~ age + weight&quot;</span>)
fmla</code></pre></div>
<pre><code>## blood_pressure ~ age + weight</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the model: bloodpressure_model</span>
bloodpressure_model &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla, bloodpressure)

<span class="co"># Print bloodpressure_model and call summary() </span>
bloodpressure_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = bloodpressure)
## 
## Coefficients:
## (Intercept)          age       weight  
##     30.9941       0.8614       0.3349</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(bloodpressure_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = bloodpressure)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4640 -1.1949 -0.4078  1.8511  2.6981 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  30.9941    11.9438   2.595  0.03186 * 
## age           0.8614     0.2482   3.470  0.00844 **
## weight        0.3349     0.1307   2.563  0.03351 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.318 on 8 degrees of freedom
## Multiple R-squared:  0.9768, Adjusted R-squared:  0.9711 
## F-statistic: 168.8 on 2 and 8 DF,  p-value: 2.874e-07</code></pre>
<p>Both variables are positive suggesting that increases in them also lead to increases in blood pressure.</p>
<p>Next we can use the model to make preditions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict blood pressure using bloodpressure_model :prediction</span>
bloodpressure<span class="op">$</span>prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(bloodpressure_model, bloodpressure)

<span class="co"># plot the results</span>
<span class="kw">ggplot</span>(bloodpressure, <span class="kw">aes</span>(<span class="dt">x =</span> prediction, <span class="dt">y =</span> blood_pressure)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>** Key Points **</p>
<ul>
<li>Linear models are easy to fit and can be less prone to overfitting.<br />
</li>
<li>They are also generally easier to understand</li>
<li>However they cannot express complex relationships in the data</li>
<li>In addition we can get collinearity with some variables e.g. weight increases with age</li>
</ul>
</div>
<div id="training-and-evaluating-regression-models" class="section level2">
<h2><span class="header-section-number">3.2</span> Training and Evaluating Regression Models</h2>
<p>A model with a good fit will have its points close to the line. Models which don’t fit well will have points which systemtically don’t fit welland are all over the place. This may mean you are missing variables in your model. A residual plot can help understand if there are any systematic errors, we are typically looking for a random cloud of residuals rather than anything which may resemble a trend. We can also use a Gain Curve Plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(unemployment)</code></pre></div>
<pre><code>##  male_unemployment female_unemployment   prediction   
##  Min.   :2.900     Min.   :4.000       Min.   :3.448  
##  1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837  
##  Median :6.000     Median :5.200       Median :5.601  
##  Mean   :5.954     Mean   :5.569       Mean   :5.569  
##  3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087  
##  Max.   :9.800     Max.   :7.900       Max.   :8.240</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make predictions from the model</span>
unemployment<span class="op">$</span>predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(unemployment_model, unemployment)

<span class="co"># Plot predictions (on x-axis) versus the female_unemployment rates</span>
<span class="kw">ggplot</span>(unemployment, <span class="kw">aes</span>(<span class="dt">x =</span> predictions, <span class="dt">y =</span> female_unemployment)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate residuals</span>
unemployment<span class="op">$</span>residuals &lt;-<span class="st"> </span>unemployment<span class="op">$</span>female_unemployment <span class="op">-</span><span class="st"> </span>unemployment<span class="op">$</span>predictions

<span class="co"># Predictions (on x-axis) versus the actuals with residuals</span>
<span class="kw">ggplot</span>(unemployment, <span class="kw">aes</span>(<span class="dt">x =</span> predictions, <span class="dt">y =</span> female_unemployment)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> <span class="dv">0</span>, <span class="dt">ymax =</span> residuals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;actuals vs. linear model prediction with residual&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fill in the blanks to plot predictions (on x-axis) versus the residuals</span>
<span class="kw">ggplot</span>(unemployment, <span class="kw">aes</span>(<span class="dt">x =</span> predictions, <span class="dt">y =</span> residuals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> <span class="dv">0</span>, <span class="dt">ymax =</span> residuals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;residuals vs. linear model prediction&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-6-3.png" width="672" /></p>
<p>We can also plot a gain curve using the WVPlots package. The syntax is:</p>
<blockquote>
<p>GainCurvePlot(frame, xvar, truthvar, title)</p>
</blockquote>
<p>where</p>
<ul>
<li>frame is a data frame</li>
<li>xvar and truthvar are strings naming the prediction and actual outcome columns of frame</li>
<li>title is the title of the plot</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(WVPlots)</code></pre></div>
<pre><code>## Warning: package &#39;WVPlots&#39; was built under R version 3.4.4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the Gain Curve</span>
<span class="kw">GainCurvePlot</span>(unemployment, <span class="st">&quot;predictions&quot;</span>, <span class="st">&quot;female_unemployment&quot;</span>, <span class="st">&quot;Unemployment model&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>A relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones.</p>
<p>Another way to evaluate our model is to use the RMSE. You want RMSE to be small. One heuristic is to compare the RMSE to the standard deviation of the outcome. With a good model, the RMSE should be smaller.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># For convenience put the residuals in the variable res</span>
res &lt;-<span class="st"> </span>unemployment<span class="op">$</span>residuals

<span class="co"># Calculate RMSE, assign it to the variable rmse and print it</span>
(rmse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(res<span class="op">^</span><span class="dv">2</span>)))</code></pre></div>
<pre><code>## [1] 0.5337612</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate the standard deviation of female_unemployment and print it</span>
(sd_unemployment &lt;-<span class="st"> </span><span class="kw">sd</span>(unemployment<span class="op">$</span>female_unemployment))</code></pre></div>
<pre><code>## [1] 1.314271</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate it as a fraction &lt; 1 is good</span>
rmse <span class="op">/</span><span class="st"> </span>sd_unemployment</code></pre></div>
<pre><code>## [1] 0.4061273</code></pre>
<p>An RMSE much smaller than the outcome’s standard deviation suggests a model that predicts well.</p>
<p>Another way of evaluating a model is R squared. We can calculate this as the 1 - RSS / TSS RSS = Residual Sum of Squares TSS is Total Sum of Squares.</p>
<div class="figure"><span id="fig:R2"></span>
<img src="images/SLinRRegression/R2.png" alt="R Squared" width="256" />
<p class="caption">
Figure 3.1: R Squared
</p>
</div>
<p>Most packages or regression functions come with built in R squared metrics, the summary will print it out or we can use the glance() function from broom. Here we calculate them manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate mean female_unemployment: fe_mean. Print it</span>
(fe_mean &lt;-<span class="st"> </span><span class="kw">mean</span>(unemployment<span class="op">$</span>female_unemployment))</code></pre></div>
<pre><code>## [1] 5.569231</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate total sum of squares: tss. Print it</span>
(tss &lt;-<span class="st"> </span><span class="kw">sum</span>((unemployment<span class="op">$</span>female_unemployment <span class="op">-</span><span class="st"> </span>fe_mean)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 20.72769</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate residual sum of squares: rss. Print it</span>
(rss &lt;-<span class="st"> </span><span class="kw">sum</span>((unemployment<span class="op">$</span>residuals)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 3.703714</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate R-squared: rsq. Print it. Is it a good fit?</span>
(rsq &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(rss<span class="op">/</span>tss))</code></pre></div>
<pre><code>## [1] 0.8213157</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get R-squared from glance. Print it</span>
(rsq_glance &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>(unemployment_model)<span class="op">$</span>r.squared)</code></pre></div>
<pre><code>## [1] 0.8213157</code></pre>
<p>Correlation or rho shows the strength of the linear relationship between two variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get the correlation between the prediction and true outcome: rho and print it</span>
(rho &lt;-<span class="st"> </span><span class="kw">cor</span>(unemployment<span class="op">$</span>predictions, unemployment<span class="op">$</span>female_unemployment))</code></pre></div>
<pre><code>## [1] 0.9062647</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rho</code></pre></div>
<pre><code>## [1] 0.9062647</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Square rho: rho2 and print it</span>
(rho2 &lt;-<span class="st"> </span>rho <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.8213157</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get R-squared from glance and print it</span>
(rsq_glance &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>(unemployment_model)<span class="op">$</span>r.squared)</code></pre></div>
<pre><code>## [1] 0.8213157</code></pre>
<p>So far we have only looked at how well our data fits the observed variables, without new data. A better way of evaluating our model is to split out data in to a training and test set of data, then see how well the model predicts to the test data, having built the model on the train data. WE want our RMSE on the test data to be similar to the RMSE on the train data, if it is much lower than we may have overfitted our model. If the number of observations is too small to do a split, we can use cross validation to acheive a similar result.</p>
<p>In the following code we will split mpg into a training set mpg_train (75% of the data) and a test set mpg_test (25% of the data). One way to do this is to generate a column of uniform random numbers between 0 and 1, using the function runif(). There other ways, such as sample (see Supervised LEarning In R - Classification notes, do search for 0.75).</p>
<p>If using run if, we do:</p>
<ul>
<li>Generate a vector of uniform random numbers: gp = runif(N).</li>
<li>dframe[gp &lt; X,] will be about the right size.</li>
<li>dframe[gp &gt;= X,] will be the complement.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the mpg data</span>
mpg &lt;-<span class="st"> </span>ggplot2<span class="op">::</span>mpg

<span class="co"># take a look at the data</span>
<span class="kw">summary</span>(mpg)</code></pre></div>
<pre><code>##  manufacturer          model               displ            year     
##  Length:234         Length:234         Min.   :1.600   Min.   :1999  
##  Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
##  Mode  :character   Mode  :character   Median :3.300   Median :2004  
##                                        Mean   :3.472   Mean   :2004  
##                                        3rd Qu.:4.600   3rd Qu.:2008  
##                                        Max.   :7.000   Max.   :2008  
##       cyl           trans               drv                 cty       
##  Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
##  1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
##  Median :6.000   Mode  :character   Mode  :character   Median :17.00  
##  Mean   :5.889                                         Mean   :16.86  
##  3rd Qu.:8.000                                         3rd Qu.:19.00  
##  Max.   :8.000                                         Max.   :35.00  
##       hwy             fl               class          
##  Min.   :12.00   Length:234         Length:234        
##  1st Qu.:18.00   Class :character   Class :character  
##  Median :24.00   Mode  :character   Mode  :character  
##  Mean   :23.44                                        
##  3rd Qu.:27.00                                        
##  Max.   :44.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(mpg)</code></pre></div>
<pre><code>## [1] 234  11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use nrow to get the number of rows in mpg (N) and print it</span>
(N &lt;-<span class="st"> </span><span class="kw">nrow</span>(mpg))</code></pre></div>
<pre><code>## [1] 234</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N</code></pre></div>
<pre><code>## [1] 234</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate how many rows 75% of N should be and print it</span>
<span class="co"># Hint: use round() to get an integer</span>
(target &lt;-<span class="st"> </span><span class="kw">round</span>(N <span class="op">*</span><span class="st"> </span><span class="fl">0.75</span>))</code></pre></div>
<pre><code>## [1] 176</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">target</code></pre></div>
<pre><code>## [1] 176</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the vector of N uniform random variables: gp</span>
gp &lt;-<span class="st"> </span><span class="kw">runif</span>(N)

<span class="co"># Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)</span>
mpg_train &lt;-<span class="st"> </span>mpg[gp <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.75</span>, ]
mpg_test &lt;-<span class="st"> </span>mpg[gp <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.75</span>, ]

<span class="co"># Use nrow() to examine mpg_train and mpg_test</span>
<span class="kw">nrow</span>(mpg_train)</code></pre></div>
<pre><code>## [1] 179</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(mpg_test)</code></pre></div>
<pre><code>## [1] 55</code></pre>
<p>It is likely our target number of rows will slightly different than what is sampled, but they should be close enough.</p>
<p>Next we use these datasets to create models for prediction.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mpg_train is in the workspace</span>
<span class="kw">summary</span>(mpg_train)</code></pre></div>
<pre><code>##  manufacturer          model               displ            year     
##  Length:179         Length:179         Min.   :1.600   Min.   :1999  
##  Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
##  Mode  :character   Mode  :character   Median :3.300   Median :2008  
##                                        Mean   :3.503   Mean   :2004  
##                                        3rd Qu.:4.600   3rd Qu.:2008  
##                                        Max.   :7.000   Max.   :2008  
##       cyl           trans               drv                 cty       
##  Min.   :4.000   Length:179         Length:179         Min.   : 9.00  
##  1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
##  Median :6.000   Mode  :character   Mode  :character   Median :16.00  
##  Mean   :5.916                                         Mean   :16.77  
##  3rd Qu.:8.000                                         3rd Qu.:19.00  
##  Max.   :8.000                                         Max.   :35.00  
##       hwy             fl               class          
##  Min.   :12.00   Length:179         Length:179        
##  1st Qu.:18.00   Class :character   Class :character  
##  Median :24.00   Mode  :character   Mode  :character  
##  Mean   :23.36                                        
##  3rd Qu.:27.00                                        
##  Max.   :44.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a formula to express cty as a function of hwy: fmla and print it.</span>
(fmla &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;cty ~ hwy&quot;</span>))</code></pre></div>
<pre><code>## cty ~ hwy</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy </span>
mpg_model &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla, <span class="dt">data =</span> mpg_train)

<span class="co"># Use summary() to examine the model</span>
<span class="kw">summary</span>(mpg_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = mpg_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8920 -0.6972 -0.0089  0.6145  4.6014 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.68419    0.37366   1.831   0.0688 .  
## hwy          0.68831    0.01549  44.445   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.248 on 177 degrees of freedom
## Multiple R-squared:  0.9178, Adjusted R-squared:  0.9173 
## F-statistic:  1975 on 1 and 177 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Next we will test the model mpg_model on the test data, mpg_test. We will use two functions rather than calculate the rmse and r_squared manually:</p>
<ul>
<li>Metrics::rmse(predcol, ycol)</li>
<li>tsensembler::r_squared(r_squared(y, y_hat)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict cty from hwy for the training set</span>
mpg_train<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mpg_model, mpg_train)

<span class="co"># predict cty from hwy for the test set</span>
mpg_test<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mpg_model, mpg_test)

<span class="co"># Evaluate the rmse on both training and test data and print them</span>
(rmse_train &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(mpg_train<span class="op">$</span>pred, mpg_train<span class="op">$</span>cty))</code></pre></div>
<pre><code>## [1] 1.241351</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(rmse_test &lt;-<span class="st">  </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(mpg_test<span class="op">$</span>pred, mpg_test<span class="op">$</span>cty))</code></pre></div>
<pre><code>## [1] 1.269702</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Evaluate the r-squared on both training and test data.and print them</span>
(rsq_train &lt;-<span class="st"> </span>tsensembler<span class="op">::</span><span class="kw">r_squared</span>(mpg_train<span class="op">$</span>pred, mpg_train<span class="op">$</span>cty))</code></pre></div>
<pre><code>## [1] 0.9103956</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(rsq_test &lt;-<span class="st"> </span>tsensembler<span class="op">::</span><span class="kw">r_squared</span>(mpg_test<span class="op">$</span>pred, mpg_test<span class="op">$</span>cty))</code></pre></div>
<pre><code>## [1] 0.8936488</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the predictions (on the x-axis) against the outcome (cty) on the test data</span>
<span class="kw">ggplot</span>(mpg_test, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> cty)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>There are a number of ways to create cross fold validation - the caret package being one of the most flexible. Here we use the vtreat package and KWayCrossValidation(). We first create our CV plan.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package vtreat</span>
<span class="kw">library</span>(vtreat)

<span class="co"># Get the number of rows in mpg</span>
nRows &lt;-<span class="st"> </span><span class="kw">nrow</span>(mpg)

<span class="co"># Implement the 5-fold cross-fold plan with vtreat</span>
splitPlan &lt;-<span class="st"> </span><span class="kw">kWayCrossValidation</span>(<span class="dt">nRows =</span> nRows, <span class="dt">nSplits =</span> <span class="dv">5</span>, <span class="dt">dframe =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="ot">NULL</span>)

<span class="co"># Examine the split plan</span>
<span class="kw">str</span>(splitPlan)</code></pre></div>
<pre><code>## List of 5
##  $ :List of 2
##   ..$ train: int [1:188] 3 4 5 6 7 8 9 10 11 12 ...
##   ..$ app  : int [1:46] 104 103 158 67 191 21 33 155 53 2 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 4 5 6 11 13 14 15 16 ...
##   ..$ app  : int [1:47] 54 8 42 198 20 135 149 97 131 222 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 5 6 7 8 9 10 11 ...
##   ..$ app  : int [1:47] 141 188 37 218 164 24 70 115 179 208 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 4 7 8 9 10 11 12 ...
##   ..$ app  : int [1:47] 64 108 148 196 111 220 90 165 107 5 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ app  : int [1:47] 128 74 83 207 174 47 98 145 26 110 ...
##  - attr(*, &quot;splitmethod&quot;)= chr &quot;kwaycross&quot;</code></pre>
<p>Next we iterate through our training plan, creating a new model for each split or fold.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mpg is in the workspace</span>
<span class="kw">summary</span>(mpg)</code></pre></div>
<pre><code>##  manufacturer          model               displ            year     
##  Length:234         Length:234         Min.   :1.600   Min.   :1999  
##  Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
##  Mode  :character   Mode  :character   Median :3.300   Median :2004  
##                                        Mean   :3.472   Mean   :2004  
##                                        3rd Qu.:4.600   3rd Qu.:2008  
##                                        Max.   :7.000   Max.   :2008  
##       cyl           trans               drv                 cty       
##  Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
##  1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
##  Median :6.000   Mode  :character   Mode  :character   Median :17.00  
##  Mean   :5.889                                         Mean   :16.86  
##  3rd Qu.:8.000                                         3rd Qu.:19.00  
##  Max.   :8.000                                         Max.   :35.00  
##       hwy             fl               class          
##  Min.   :12.00   Length:234         Length:234        
##  1st Qu.:18.00   Class :character   Class :character  
##  Median :24.00   Mode  :character   Mode  :character  
##  Mean   :23.44                                        
##  3rd Qu.:27.00                                        
##  Max.   :44.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># splitPlan is in the workspace</span>
<span class="kw">str</span>(splitPlan)</code></pre></div>
<pre><code>## List of 5
##  $ :List of 2
##   ..$ train: int [1:188] 3 4 5 6 7 8 9 10 11 12 ...
##   ..$ app  : int [1:46] 104 103 158 67 191 21 33 155 53 2 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 4 5 6 11 13 14 15 16 ...
##   ..$ app  : int [1:47] 54 8 42 198 20 135 149 97 131 222 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 5 6 7 8 9 10 11 ...
##   ..$ app  : int [1:47] 141 188 37 218 164 24 70 115 179 208 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 4 7 8 9 10 11 12 ...
##   ..$ app  : int [1:47] 64 108 148 196 111 220 90 165 107 5 ...
##  $ :List of 2
##   ..$ train: int [1:187] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ app  : int [1:47] 128 74 83 207 174 47 98 145 26 110 ...
##  - attr(*, &quot;splitmethod&quot;)= chr &quot;kwaycross&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run the 5-fold cross validation plan from splitPlan</span>
k &lt;-<span class="st"> </span><span class="dv">5</span> <span class="co"># Number of folds</span>
mpg<span class="op">$</span>pred.cv &lt;-<span class="st"> </span><span class="dv">0</span> 
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {
  split &lt;-<span class="st"> </span>splitPlan[[i]]
  model &lt;-<span class="st"> </span><span class="kw">lm</span>(cty <span class="op">~</span><span class="st"> </span>hwy, <span class="dt">data =</span> mpg[split<span class="op">$</span>train,])
  mpg<span class="op">$</span>pred.cv[split<span class="op">$</span>app] &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">newdata =</span> mpg[split<span class="op">$</span>app,])
}

<span class="co"># Predict from a full model</span>
mpg<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(cty <span class="op">~</span><span class="st"> </span>hwy, <span class="dt">data =</span> mpg))

<span class="co"># Get the rmse of the full model&#39;s predictions</span>
Metrics<span class="op">::</span><span class="kw">rmse</span>(mpg<span class="op">$</span>pred, mpg<span class="op">$</span>cty)</code></pre></div>
<pre><code>## [1] 1.247045</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get the rmse of the cross-validation predictions</span>
Metrics<span class="op">::</span><span class="kw">rmse</span>(mpg<span class="op">$</span>pred.cv, mpg<span class="op">$</span>cty)</code></pre></div>
<pre><code>## [1] 1.266426</code></pre>
<p>This has now calculated the models out of sample error using cross validation. CV validates the modelling process, not whether the model is a good one or not. Here we we the full model RMSE is very similar to the CV RMSE suggesting we are not over fitting the data.</p>
</div>
<div id="issues-to-consider" class="section level2">
<h2><span class="header-section-number">3.3</span> Issues to Consider</h2>
<p>When using categorical variables, n - 1 variables are created and coded as dummy vars (0 or 1), one one variable is left out as the reference model. This is usually referred to as dummy variable creation or one hot encoding. So our model then compares how the presence of one variable (one categorical var or response) affects the outcome, ceteris parabus, against the baseline cateogrical variable or response. This is best done where the number of variables is quite small, to avoid overfitting.</p>
<p>Some functions/models do this one hot or dummy variable creation automatically ‘under the hood’ whereas others need more pre-procesing prior to modelling. For some approaches (tree models) we can leave them as nominal e.g. 1,2 and 3. BUT this won’t work where the data/calculations are geometric, such as linear regression (geometric), PCA and some clustering methods (eigen space calculations). More information is available in <a href="http://appliedpredictivemodeling.com/blog/2013/10/23/the-basics-of-encoding-categorical-data-for-predictive-models">The Basics of Encoding Categorical Data for Predictive Models</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the data from Sleuth and modify for the purposes of demostration</span>
<span class="kw">library</span>(Sleuth3)</code></pre></div>
<pre><code>## Warning: package &#39;Sleuth3&#39; was built under R version 3.4.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flowers &lt;-<span class="st"> </span><span class="kw">print</span>(case0901)</code></pre></div>
<pre><code>##    Flowers Time Intensity
## 1     62.3    1       150
## 2     77.4    1       150
## 3     55.3    1       300
## 4     54.2    1       300
## 5     49.6    1       450
## 6     61.9    1       450
## 7     39.4    1       600
## 8     45.7    1       600
## 9     31.3    1       750
## 10    44.9    1       750
## 11    36.8    1       900
## 12    41.9    1       900
## 13    77.8    2       150
## 14    75.6    2       150
## 15    69.1    2       300
## 16    78.0    2       300
## 17    57.0    2       450
## 18    71.1    2       450
## 19    62.9    2       600
## 20    52.2    2       600
## 21    60.3    2       750
## 22    45.6    2       750
## 23    52.6    2       900
## 24    44.4    2       900</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flowers<span class="op">$</span>Time[flowers<span class="op">$</span>Time<span class="op">==</span><span class="dv">1</span>] &lt;-<span class="st"> &quot;Late&quot;</span>
flowers<span class="op">$</span>Time[flowers<span class="op">$</span>Time<span class="op">==</span><span class="dv">2</span>] &lt;-<span class="st"> &quot;Early&quot;</span>

<span class="co"># Call str on flowers to see the types of each column</span>
<span class="kw">str</span>(flowers)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    24 obs. of  3 variables:
##  $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
##  $ Time     : chr  &quot;Late&quot; &quot;Late&quot; &quot;Late&quot; &quot;Late&quot; ...
##  $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use unique() to see how many possible values Time takes</span>
<span class="kw">unique</span>(flowers<span class="op">$</span>Time)</code></pre></div>
<pre><code>## [1] &quot;Late&quot;  &quot;Early&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Build a formula to express Flowers as a function of Intensity and Time: fmla. Print it</span>
(fmla &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;Flowers ~ Intensity + Time&quot;</span>))</code></pre></div>
<pre><code>## Flowers ~ Intensity + Time</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use fmla and model.matrix to see how the data is represented for modeling</span>
mmat &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(fmla, flowers)

<span class="co"># Examine the first 20 lines of flowers</span>
<span class="kw">head</span>(flowers, <span class="dt">n =</span> <span class="dv">20</span>)</code></pre></div>
<pre><code>##    Flowers  Time Intensity
## 1     62.3  Late       150
## 2     77.4  Late       150
## 3     55.3  Late       300
## 4     54.2  Late       300
## 5     49.6  Late       450
## 6     61.9  Late       450
## 7     39.4  Late       600
## 8     45.7  Late       600
## 9     31.3  Late       750
## 10    44.9  Late       750
## 11    36.8  Late       900
## 12    41.9  Late       900
## 13    77.8 Early       150
## 14    75.6 Early       150
## 15    69.1 Early       300
## 16    78.0 Early       300
## 17    57.0 Early       450
## 18    71.1 Early       450
## 19    62.9 Early       600
## 20    52.2 Early       600</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Examine the first 20 lines of mmat</span>
<span class="kw">head</span>(mmat, <span class="dt">n =</span> <span class="dv">20</span>)</code></pre></div>
<pre><code>##    (Intercept) Intensity TimeLate
## 1            1       150        1
## 2            1       150        1
## 3            1       300        1
## 4            1       300        1
## 5            1       450        1
## 6            1       450        1
## 7            1       600        1
## 8            1       600        1
## 9            1       750        1
## 10           1       750        1
## 11           1       900        1
## 12           1       900        1
## 13           1       150        0
## 14           1       150        0
## 15           1       300        0
## 16           1       300        0
## 17           1       450        0
## 18           1       450        0
## 19           1       600        0
## 20           1       600        0</code></pre>
<p>Next we will fit a linear model to the flowers data, to predict Flowers as a function of Time and Intensity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># flowers in is the workspace</span>
<span class="kw">str</span>(flowers)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    24 obs. of  3 variables:
##  $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
##  $ Time     : chr  &quot;Late&quot; &quot;Late&quot; &quot;Late&quot; &quot;Late&quot; ...
##  $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fmla is in the workspace</span>
fmla</code></pre></div>
<pre><code>## Flowers ~ Intensity + Time</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit a model to predict Flowers from Intensity and Time : flower_model</span>
flower_model &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla, <span class="dt">data =</span> flowers)

<span class="co"># Use summary on mmat to remind yourself of its structure</span>
<span class="kw">summary</span>(mmat)</code></pre></div>
<pre><code>##   (Intercept)   Intensity      TimeLate  
##  Min.   :1    Min.   :150   Min.   :0.0  
##  1st Qu.:1    1st Qu.:300   1st Qu.:0.0  
##  Median :1    Median :525   Median :0.5  
##  Mean   :1    Mean   :525   Mean   :0.5  
##  3rd Qu.:1    3rd Qu.:750   3rd Qu.:1.0  
##  Max.   :1    Max.   :900   Max.   :1.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use summary to examine flower_model </span>
<span class="kw">summary</span>(flower_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla, data = flowers)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.652 -4.139 -1.558  5.632 12.165 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  83.464167   3.273772  25.495  &lt; 2e-16 ***
## Intensity    -0.040471   0.005132  -7.886 1.04e-07 ***
## TimeLate    -12.158333   2.629557  -4.624 0.000146 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.441 on 21 degrees of freedom
## Multiple R-squared:  0.7992, Adjusted R-squared:   0.78 
## F-statistic: 41.78 on 2 and 21 DF,  p-value: 4.786e-08</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predict the number of flowers on each plant</span>
flowers<span class="op">$</span>predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(flower_model, <span class="dt">data =</span> flowers)

<span class="co"># Plot predictions vs actual flowers (predictions on x-axis)</span>
<span class="kw">ggplot</span>(flowers, <span class="kw">aes</span>(<span class="dt">x =</span> predictions, <span class="dt">y =</span> Flowers)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) </code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div id="interactions" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Interactions</h3>
<p>In linear models we assume that the variables affect the model linearly and additively. But sometimes this is not the case - with interactions variables, wehn combinded together, can be more than the sum of their parts. If for instance, the effect one one variable is dependent on the level of some other variable, we say their is an interaction. The simultaneous effect on the outcome variable is no longer additive. If we want to model this in R we have different options</p>
<ul>
<li>Interaction - Colon(:) e.g. y ~ a:b</li>
<li>Main effects and interaction - Asterisk(<em>) e.g. y ~ a </em> b which is the same as y ~ a + b + a:b</li>
<li>The product of two variables - I function (I) e.g. y ~ I(a*b)</li>
</ul>
<p>In the following code shows how to use interactions to model the effect of gender and gastric activity on alcohol metabolism.</p>
<p>The data frame alcohol has columns:</p>
<ul>
<li>Metabol: the alcohol metabolism rate</li>
<li>Gastric: the rate of gastric alcohol dehydrogenase activity</li>
<li>Sex: the sex of the drinker (Male or Female)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the formula with main effects only</span>
(fmla_add &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;Metabol ~ Gastric + Sex&quot;</span>) )

<span class="co"># Create the formula with interactions</span>
(fmla_interaction &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;Metabol ~ Gastric:Sex + Gastric&quot;</span>) )

<span class="co"># Fit the main effects only model</span>
model_add &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_add, <span class="dt">data =</span> alcohol)

<span class="co"># Fit the interaction model</span>
model_interaction &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_interaction, <span class="dt">data =</span> alcohol)

<span class="co"># Call summary on both models and compare</span>
<span class="kw">summary</span>(model_add)
<span class="kw">summary</span>(model_interaction)</code></pre></div>
<p>The following code compares the performance of the interaction model previously fit to the performance of a main-effects only model. Because this data set is small, we use cross-validation to simulate making predictions on out-of-sample data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the splitting plan for 3-fold cross validation</span>
<span class="kw">set.seed</span>(<span class="dv">34245</span>)  <span class="co"># set the seed for reproducibility</span>
splitPlan &lt;-<span class="st"> </span><span class="kw">kWayCrossValidation</span>(<span class="kw">nrow</span>(alcohol), <span class="dt">nSplits =</span> <span class="dv">3</span>, <span class="dt">dframe =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="ot">NULL</span>)

<span class="co"># Sample code: Get cross-val predictions for main-effects only model</span>
alcohol<span class="op">$</span>pred_add &lt;-<span class="st"> </span><span class="dv">0</span>  <span class="co"># initialize the prediction vector</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {
  split &lt;-<span class="st"> </span>splitPlan[[i]]
  model_add &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_add, <span class="dt">data =</span> alcohol[split<span class="op">$</span>train, ])
  alcohol<span class="op">$</span>pred_add[split<span class="op">$</span>app] &lt;-<span class="st"> </span><span class="kw">predict</span>(model_add, <span class="dt">newdata =</span> alcohol[split<span class="op">$</span>app, ])
}

<span class="co"># Get the cross-val predictions for the model with interactions</span>
alcohol<span class="op">$</span>pred_interaction &lt;-<span class="st"> </span><span class="dv">0</span> <span class="co"># initialize the prediction vector</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {
  split &lt;-<span class="st"> </span>splitPlan[[i]]
  model_interaction &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_interaction, <span class="dt">data =</span> alcohol[split<span class="op">$</span>train, ])
  alcohol<span class="op">$</span>pred_interaction[split<span class="op">$</span>app] &lt;-<span class="st"> </span><span class="kw">predict</span>(model_interaction, <span class="dt">newdata =</span> alcohol[split<span class="op">$</span>app, ])
}

<span class="co"># Get RMSE using gather from dplyr</span>
alcohol <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> modeltype, <span class="dt">value =</span> pred, pred_add, pred_interaction) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residuals =</span> Metabol <span class="op">-</span><span class="st"> </span>pred) <span class="op">%&gt;%</span><span class="st">      </span>
<span class="st">  </span><span class="kw">group_by</span>(modeltype) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residuals<span class="op">^</span><span class="dv">2</span>)))</code></pre></div>
</div>
<div id="transforming-the-response-before-modeling" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Transforming the response before modeling</h3>
<p>Sometimes better models are achieved by transforming the output rather than predicting the output directly. We often want to log transform monetary values like income. This can be achied using log(y) or specify the same function within a model itself, predict using that model, then transform the data back to it’s original format i.e not transformed e.g.</p>
<ol style="list-style-type: decimal">
<li>model &lt;- lm(log(y) ~ x, data = train)<br />
</li>
<li>logpred &lt;- predict(model, data = test)</li>
<li>pred &lt;- exp(logpred)</li>
</ol>
<p>As the error is relative to the size of the outcome, we should use the root mean squared relative error to compare two models with one not being log transformed and one being log transformed. We do this by dividing the mean error / variable in quetion (log or non-log).</p>
<p>The next code section uses some toy data to demostrate how we make calculations for relative error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the demostration data</span>
fdata &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./files/SLinRRegression/fdata.csv&quot;</span>)
<span class="kw">str</span>(fdata)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    100 obs. of  3 variables:
##  $ y    : num  9.15 1.9 -3.86 2.39 1.54 ...
##  $ pred : num  6.43 3.47 1.59 3.76 9.51 ...
##  $ label: Factor w/ 2 levels &quot;large purchases&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fdata is in the workspace</span>
<span class="kw">summary</span>(fdata)</code></pre></div>
<pre><code>##        y                 pred                      label   
##  Min.   :  -5.894   Min.   :   1.072   large purchases:50  
##  1st Qu.:   5.407   1st Qu.:   6.373   small purchases:50  
##  Median :  57.374   Median :  55.693                       
##  Mean   : 306.204   Mean   : 305.905                       
##  3rd Qu.: 550.903   3rd Qu.: 547.886                       
##  Max.   :1101.619   Max.   :1098.896</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Examine the data: generate the summaries for the groups large and small:</span>
fdata <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(label) <span class="op">%&gt;%</span><span class="st">     </span><span class="co"># group by small/large purchases</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">min  =</span> <span class="kw">min</span>(y),   <span class="co"># min of y</span>
              <span class="dt">mean =</span> <span class="kw">mean</span>(y),   <span class="co"># mean of y</span>
              <span class="dt">max  =</span> <span class="kw">max</span>(y))   <span class="co"># max of y</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##             label       min       mean        max
##            &lt;fctr&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 large purchases 96.119814 605.928673 1101.61864
## 2 small purchases -5.893499   6.478254   18.62829</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fill in the blanks to add error columns</span>
fdata2 &lt;-<span class="st"> </span>fdata <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">         </span><span class="kw">group_by</span>(label) <span class="op">%&gt;%</span><span class="st">       </span><span class="co"># group by label</span>
<span class="st">           </span><span class="kw">mutate</span>(<span class="dt">residual =</span> pred <span class="op">-</span><span class="st"> </span>y,  <span class="co"># Residual</span>
                  <span class="dt">relerr   =</span> residual <span class="op">/</span><span class="st"> </span>y)  <span class="co"># Relative error</span>

<span class="co"># Compare the rmse and rmse.rel of the large and small groups:</span>
fdata2 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(label) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse     =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residual<span class="op">^</span><span class="dv">2</span>)),   <span class="co"># RMSE</span>
            <span class="dt">rmse.rel =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(relerr<span class="op">^</span><span class="dv">2</span>)))   <span class="co"># Root mean squared relative error</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##             label     rmse   rmse.rel
##            &lt;fctr&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1 large purchases 5.544439 0.01473322
## 2 small purchases 4.014969 1.24965673</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the predictions for both groups of purchases</span>
<span class="kw">ggplot</span>(fdata2, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> y, <span class="dt">color =</span> label)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>label, <span class="dt">ncol =</span> <span class="dv">1</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Outcome vs prediction&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>From this example how a model with larger RMSE might still be better as can be observed using the chart, if relative errors are more important than absolute errors as the relative error is much smaller for large purchases using the table.</p>
<p>The follwoing code uses data loaded which records subjects’ incomes in 2005, as well as the results of several aptitude tests taken by the subjects in 1981. The data is split into training and test sets. The code demostrates building a model of log(income) from the inputs, and then convert log(income) back into income.</p>
<p>When you transform the output before modeling, you have to ‘reverse transform’ the resulting predictions after applying the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the unemployment data</span>
<span class="kw">load</span>(<span class="st">&quot;./files/SLinRRegression/Income.RData&quot;</span>)

<span class="co"># Examine Income2005 in the training set</span>
<span class="kw">summary</span>(incometrain<span class="op">$</span>Income2005)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      63   23000   39000   49894   61500  703637</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Write the formula for log income as a function of the tests and print it</span>
(fmla.log &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;log(Income2005) ~  Arith + Word + Parag + Math + AFQT&quot;</span>))</code></pre></div>
<pre><code>## log(Income2005) ~ Arith + Word + Parag + Math + AFQT</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the linear model</span>
model.log &lt;-<span class="st">  </span><span class="kw">lm</span>(fmla.log, <span class="dt">data =</span> incometrain)

<span class="co"># Make predictions on income_test</span>
incometest<span class="op">$</span>logpred &lt;-<span class="st"> </span><span class="kw">predict</span>(model.log, incometest)
<span class="kw">summary</span>(incometest<span class="op">$</span>logpred)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   9.766  10.133  10.423  10.419  10.705  11.006</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert the predictions to monetary units</span>
incometest<span class="op">$</span>pred.income &lt;-<span class="st"> </span><span class="kw">exp</span>(incometest<span class="op">$</span>logpred)
<span class="kw">summary</span>(incometest<span class="op">$</span>pred.income)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   17432   25167   33615   35363   44566   60217</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Plot predicted income (x axis) vs income</span>
<span class="kw">ggplot</span>(incometest, <span class="kw">aes</span>(<span class="dt">x =</span> pred.income, <span class="dt">y =</span> Income2005)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Next we show that log-transforming a monetary output before modeling improves mean relative error (but increases RMSE) compared to modeling the monetary output directly. We compare two models, the log transmormed model from before to an absolute (i.e. not transformed) model called model.abs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr) <span class="co"># for gather commands</span>

<span class="co"># Write our model formula</span>
(fmla.abs &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;Income2005 ~ Arith + Word + Parag + Math + AFQT&quot;</span>))

<span class="co"># Build the model</span>
model.abs &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> fmla.abs, <span class="dt">data =</span> incometrain)

<span class="co"># Add predictions to the test set</span>
income_test &lt;-<span class="st"> </span>incometest <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">mutate</span>(<span class="dt">pred.absmodel =</span> <span class="kw">predict</span>(model.abs, incometest),        <span class="co"># predictions from model.abs</span>
         <span class="dt">pred.logmodel =</span> <span class="kw">exp</span>(<span class="kw">predict</span>(model.log, incometest)))          <span class="co"># predictions from model.log</span>

<span class="co"># Gather the predictions and calculate residuals and relative error</span>
income_long &lt;-<span class="st"> </span>incometest <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidyr<span class="op">::</span><span class="kw">gather</span>(<span class="dt">key =</span> modeltype, <span class="dt">value =</span> pred, pred.absmodel, pred.logmodel) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">mutate</span>(<span class="dt">residual =</span> pred <span class="op">-</span><span class="st"> </span>Income2005,   <span class="co"># residuals</span>
         <span class="dt">relerr   =</span> residual <span class="op">/</span><span class="st"> </span>Income2005)   <span class="co"># relative error</span>

<span class="co"># Calculate RMSE and relative RMSE and compare</span>
income_long <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(modeltype) <span class="op">%&gt;%</span><span class="st">      </span><span class="co"># group by modeltype</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse     =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residual<span class="op">^</span><span class="dv">2</span>)),    <span class="co"># RMSE</span>
            <span class="dt">rmse.rel =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(relerr<span class="op">^</span><span class="dv">2</span>)))    <span class="co"># Root mean squared relative error</span></code></pre></div>
<p>Modeling log(income) can reduce the relative error of the fit, at the cost of increased RMSE. Which tradeoff to make depends on the goals of the project.</p>
</div>
<div id="transforming-input-variables" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Transforming Input variables</h3>
<p>So far we’ve looked at transforming the output variables, but there are instance where we might want to transform the input variables. Usually, this is because you have some domain knowledge about the subject which suggests this is the case. You often want to transform monetary values as we say before. We might also want to create power relationships in our variables. If we do this and we don’t know which power is best, based on domain knowledge, we might try various powers then see which yields the lowest prediction error and out of sample/CV error.</p>
<p>In the next section of code we will build a model to predict price from a measure of the house’s size (surface area).</p>
<p>Because ^ is also a symbol to express interactions, we use the function I() to treat the expression x^2 “as is”: that is, as the square of x rather than the interaction of x with itself.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the house price data</span>
houseprice &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./files/SLinRRegression/houseprice.rds&quot;</span>)

<span class="co"># explore the data</span>
<span class="kw">summary</span>(houseprice)</code></pre></div>
<pre><code>##       size           price      
##  Min.   : 44.0   Min.   : 42.0  
##  1st Qu.: 73.5   1st Qu.:164.5  
##  Median : 91.0   Median :203.5  
##  Mean   : 94.3   Mean   :249.2  
##  3rd Qu.:118.5   3rd Qu.:287.8  
##  Max.   :150.0   Max.   :573.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the formula for price as a function of squared size</span>
(fmla_sqr &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;price ~ I(size^2)&quot;</span>))</code></pre></div>
<pre><code>## price ~ I(size^2)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit a model of price as a function of squared size (use fmla_sqr)</span>
model_sqr &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_sqr, <span class="dt">data =</span> houseprice)

<span class="co"># Fit a model of price as a linear function of size</span>
model_lin &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>size, <span class="dt">data =</span> houseprice)

<span class="co"># Make predictions and compare</span>
houseprice <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">mutate</span>(<span class="dt">pred_lin =</span> <span class="kw">predict</span>(model_lin),       <span class="co"># predictions from linear model</span>
           <span class="dt">pred_sqr =</span> <span class="kw">predict</span>(model_sqr)) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># predictions from quadratic model </span>
<span class="st">    </span>tidyr<span class="op">::</span><span class="kw">gather</span>(<span class="dt">key =</span> modeltype, <span class="dt">value =</span> pred, pred_lin, pred_sqr) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># gather the predictions</span>
<span class="st">    </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> size)) <span class="op">+</span><span class="st"> </span>
<span class="st">       </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> price)) <span class="op">+</span><span class="st">                   </span><span class="co"># actual prices</span>
<span class="st">       </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> modeltype)) <span class="op">+</span><span class="st"> </span><span class="co"># the predictions</span>
<span class="st">       </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>As it appears that the quadratic model better fits the house price data, we will next confirm whether this is the case when using out of sample data. As the data is small, we will use cross-validation. We will also compare the results from a linear model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># houseprice is in the workspace</span>
<span class="kw">summary</span>(houseprice)</code></pre></div>
<pre><code>##       size           price      
##  Min.   : 44.0   Min.   : 42.0  
##  1st Qu.: 73.5   1st Qu.:164.5  
##  Median : 91.0   Median :203.5  
##  Mean   : 94.3   Mean   :249.2  
##  3rd Qu.:118.5   3rd Qu.:287.8  
##  Max.   :150.0   Max.   :573.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fmla_sqr is in the workspace</span>
fmla_sqr</code></pre></div>
<pre><code>## price ~ I(size^2)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a splitting plan for 3-fold cross validation</span>
<span class="kw">set.seed</span>(<span class="dv">34245</span>)  <span class="co"># set the seed for reproducibility</span>
splitPlan &lt;-<span class="st"> </span><span class="kw">kWayCrossValidation</span>(<span class="dt">nRows =</span> <span class="kw">nrow</span>(houseprice), <span class="dt">nSplits =</span> <span class="dv">3</span>, <span class="dt">dframe =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="ot">NULL</span>)

<span class="co"># Sample code: get cross-val predictions for price ~ size</span>
houseprice<span class="op">$</span>pred_lin &lt;-<span class="st"> </span><span class="dv">0</span>  <span class="co"># initialize the prediction vector</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {
  split &lt;-<span class="st"> </span>splitPlan[[i]]
  model_lin &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>size, <span class="dt">data =</span> houseprice[split<span class="op">$</span>train,])
  houseprice<span class="op">$</span>pred_lin[split<span class="op">$</span>app] &lt;-<span class="st"> </span><span class="kw">predict</span>(model_lin, <span class="dt">newdata =</span> houseprice[split<span class="op">$</span>app,])
}

<span class="co"># Get cross-val predictions for price as a function of size^2 (use fmla_sqr)</span>
houseprice<span class="op">$</span>pred_sqr &lt;-<span class="st"> </span><span class="dv">0</span> <span class="co"># initialize the prediction vector</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {
  split &lt;-<span class="st"> </span>splitPlan[[i]]
  model_sqr &lt;-<span class="st"> </span><span class="kw">lm</span>(fmla_sqr, <span class="dt">data =</span> houseprice[split<span class="op">$</span>train, ])
  houseprice<span class="op">$</span>pred_sqr[split<span class="op">$</span>app] &lt;-<span class="st"> </span><span class="kw">predict</span>(model_sqr, <span class="dt">newdata =</span> houseprice[split<span class="op">$</span>app, ])
}

<span class="co"># Gather the predictions and calculate the residuals</span>
houseprice_long &lt;-<span class="st"> </span>houseprice <span class="op">%&gt;%</span>
<span class="st">  </span>tidyr<span class="op">::</span><span class="kw">gather</span>(<span class="dt">key =</span> modeltype, <span class="dt">value =</span> pred, pred_lin, pred_sqr) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residuals =</span> pred <span class="op">-</span><span class="st"> </span>price)

<span class="co"># Compare the cross-validated RMSE for the two models</span>
houseprice_long <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(modeltype) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># group by modeltype</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residuals<span class="op">^</span><span class="dv">2</span>)))</code></pre></div>
<pre><code>## # A tibble: 2 x 2
##   modeltype     rmse
##       &lt;chr&gt;    &lt;dbl&gt;
## 1  pred_lin 74.29993
## 2  pred_sqr 63.69409</code></pre>
</div>
</div>
<div id="dealing-with-non-linear-responses" class="section level2">
<h2><span class="header-section-number">3.4</span> Dealing with Non-Linear Responses</h2>
<p>First we will look at predicting the probability that an event occurs, based on a binary response yes/no. To do this we will use a logistic regression, so that the probabilities are in the 0 to 1 range. This builds a log-odds model which is simply a log transformed ratio that the probability occurs to the probability that it does not -</p>
<ul>
<li>for the model we have - glm(formula, data, family = binomial)</li>
<li>for the prediction we have - predict(model, newdata = test, type = “response”)</li>
</ul>
<p>When assessing accuracy we can use a deviance calculation (pseudo R squared) or a chi-squared test. We can also use a Gain Curve or ROC curce.</p>
<p>We will estimate the probability that a sparrow survives a severe winter storm, based on physical characteristics of the sparrow. The dataset sparrow is loaded into your workspace. The outcome to be predicted is status.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the sparrow data</span>
sparrow &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./files/SLinRRegression/sparrow.rds&quot;</span>)

<span class="co"># Take a look at the data</span>
<span class="kw">head</span>(sparrow) </code></pre></div>
<pre><code>##     status   age total_length wingspan weight beak_head humerus femur
## 1 Survived adult          154      241   24.5      31.2    0.69  0.67
## 2 Survived adult          160      252   26.9      30.8    0.74  0.71
## 3 Survived adult          155      243   26.9      30.6    0.73  0.70
## 4 Survived adult          154      245   24.3      31.7    0.74  0.69
## 5 Survived adult          156      247   24.1      31.5    0.71  0.71
## 6 Survived adult          161      253   26.5      31.8    0.78  0.74
##   legbone skull sternum
## 1    1.02  0.59    0.83
## 2    1.18  0.60    0.84
## 3    1.15  0.60    0.85
## 4    1.15  0.58    0.84
## 5    1.13  0.57    0.82
## 6    1.14  0.61    0.89</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sparrow is in the workspace</span>
<span class="kw">summary</span>(sparrow)</code></pre></div>
<pre><code>##       status       age             total_length      wingspan    
##  Perished:36   Length:87          Min.   :153.0   Min.   :236.0  
##  Survived:51   Class :character   1st Qu.:158.0   1st Qu.:245.0  
##                Mode  :character   Median :160.0   Median :247.0  
##                                   Mean   :160.4   Mean   :247.5  
##                                   3rd Qu.:162.5   3rd Qu.:251.0  
##                                   Max.   :167.0   Max.   :256.0  
##      weight       beak_head        humerus           femur       
##  Min.   :23.2   Min.   :29.80   Min.   :0.6600   Min.   :0.6500  
##  1st Qu.:24.7   1st Qu.:31.40   1st Qu.:0.7250   1st Qu.:0.7000  
##  Median :25.8   Median :31.70   Median :0.7400   Median :0.7100  
##  Mean   :25.8   Mean   :31.64   Mean   :0.7353   Mean   :0.7134  
##  3rd Qu.:26.7   3rd Qu.:32.10   3rd Qu.:0.7500   3rd Qu.:0.7300  
##  Max.   :31.0   Max.   :33.00   Max.   :0.7800   Max.   :0.7600  
##     legbone          skull           sternum      
##  Min.   :1.010   Min.   :0.5600   Min.   :0.7700  
##  1st Qu.:1.110   1st Qu.:0.5900   1st Qu.:0.8300  
##  Median :1.130   Median :0.6000   Median :0.8500  
##  Mean   :1.131   Mean   :0.6032   Mean   :0.8511  
##  3rd Qu.:1.160   3rd Qu.:0.6100   3rd Qu.:0.8800  
##  Max.   :1.230   Max.   :0.6400   Max.   :0.9300</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the survived column</span>
sparrow<span class="op">$</span>survived &lt;-<span class="st"> </span>sparrow<span class="op">$</span>status <span class="op">==</span><span class="st"> &quot;Survived&quot;</span>

<span class="co"># Create the formula</span>
(fmla &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;survived ~ total_length + weight + humerus&quot;</span>))</code></pre></div>
<pre><code>## survived ~ total_length + weight + humerus</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the logistic regression model</span>
sparrow_model &lt;-<span class="st"> </span><span class="kw">glm</span>(fmla, <span class="dt">data =</span> sparrow, <span class="dt">family =</span> binomial)

<span class="co"># Call summary</span>
<span class="kw">summary</span>(sparrow_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = fmla, family = binomial, data = sparrow)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1117  -0.6026   0.2871   0.6577   1.7082  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   46.8813    16.9631   2.764 0.005715 ** 
## total_length  -0.5435     0.1409  -3.858 0.000115 ***
## weight        -0.5689     0.2771  -2.053 0.040060 *  
## humerus       75.4610    19.1586   3.939 8.19e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 118.008  on 86  degrees of freedom
## Residual deviance:  75.094  on 83  degrees of freedom
## AIC: 83.094
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call glance</span>
(perf &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>(sparrow_model))</code></pre></div>
<pre><code>##   null.deviance df.null    logLik      AIC      BIC deviance df.residual
## 1      118.0084      86 -37.54718 83.09436 92.95799 75.09436          83</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate pseudo-R-squared</span>
(pseudoR2 &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(perf<span class="op">$</span>deviance <span class="op">/</span><span class="st"> </span>perf<span class="op">$</span>null.deviance)))</code></pre></div>
<pre><code>## [1] 0.3636526</code></pre>
<p>So our pseudo R squared so far is quite low at 0.36. Next we will predict with the model and show a gain curve. The gain curve show be as close to the ideal (the ‘wizzard curve’ or green line) as we can get.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sparrow is in the workspace</span>
<span class="kw">summary</span>(sparrow)</code></pre></div>
<pre><code>##       status       age             total_length      wingspan    
##  Perished:36   Length:87          Min.   :153.0   Min.   :236.0  
##  Survived:51   Class :character   1st Qu.:158.0   1st Qu.:245.0  
##                Mode  :character   Median :160.0   Median :247.0  
##                                   Mean   :160.4   Mean   :247.5  
##                                   3rd Qu.:162.5   3rd Qu.:251.0  
##                                   Max.   :167.0   Max.   :256.0  
##      weight       beak_head        humerus           femur       
##  Min.   :23.2   Min.   :29.80   Min.   :0.6600   Min.   :0.6500  
##  1st Qu.:24.7   1st Qu.:31.40   1st Qu.:0.7250   1st Qu.:0.7000  
##  Median :25.8   Median :31.70   Median :0.7400   Median :0.7100  
##  Mean   :25.8   Mean   :31.64   Mean   :0.7353   Mean   :0.7134  
##  3rd Qu.:26.7   3rd Qu.:32.10   3rd Qu.:0.7500   3rd Qu.:0.7300  
##  Max.   :31.0   Max.   :33.00   Max.   :0.7800   Max.   :0.7600  
##     legbone          skull           sternum        survived      
##  Min.   :1.010   Min.   :0.5600   Min.   :0.7700   Mode :logical  
##  1st Qu.:1.110   1st Qu.:0.5900   1st Qu.:0.8300   FALSE:36       
##  Median :1.130   Median :0.6000   Median :0.8500   TRUE :51       
##  Mean   :1.131   Mean   :0.6032   Mean   :0.8511                  
##  3rd Qu.:1.160   3rd Qu.:0.6100   3rd Qu.:0.8800                  
##  Max.   :1.230   Max.   :0.6400   Max.   :0.9300</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sparrow_model is in the workspace</span>
<span class="kw">summary</span>(sparrow_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = fmla, family = binomial, data = sparrow)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1117  -0.6026   0.2871   0.6577   1.7082  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   46.8813    16.9631   2.764 0.005715 ** 
## total_length  -0.5435     0.1409  -3.858 0.000115 ***
## weight        -0.5689     0.2771  -2.053 0.040060 *  
## humerus       75.4610    19.1586   3.939 8.19e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 118.008  on 86  degrees of freedom
## Residual deviance:  75.094  on 83  degrees of freedom
## AIC: 83.094
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make predictions</span>
sparrow<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(sparrow_model, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># Look at gain curve</span>
<span class="kw">GainCurvePlot</span>(sparrow, <span class="st">&quot;pred&quot;</span>, <span class="st">&quot;survived&quot;</span>, <span class="st">&quot;sparrow survival model&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>From the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.</p>
<div id="count-data-with-poisson-and-quasipoisson-regression" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Count data with poisson and quasipoisson regression</h3>
<p>Predicting counts is a non linear problem because counts are restricted to being non negative and integers. To predict counts we do poisson or quasipoisson regression. It is a generalised linear model (GLM) in so much as it assumes the inputs are additive and linear with respect to the log of the outcome, we use family = “poisson” or “quasipoisson”. We can use such models for things like predicting the number of website hits, the actual predict model we not predict an integer, but a rate per day. In poisson regression it is assumed the mean = variance, if this is not the case, we should use quasipoisson. More technically we could say that the event we are counting is Poisson distributed: the average count per unit is the same variance of the count, the same meaning that the mean and the variance should be of a similar order of magnitude. When the variance is much larger than the mean, the Poisson assumption doesn’t apply and we should use quasipoisson.</p>
<p>NOTE: If the counts we are trying to predict are very large, regular regression may be appropriate method also.</p>
<p>In this exercise we will build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July.</p>
<p>The data frame has the columns:</p>
<ul>
<li>cnt: the number of bikes rented in that hour (the outcome)<br />
</li>
<li>hr: the hour of the day (0-23, as a factor)<br />
</li>
<li>holiday: TRUE/FALSE<br />
</li>
<li>workingday: TRUE if neither a holiday nor a weekend, else FALSE<br />
</li>
<li>weathersit: categorical, “Clear to partly cloudy”/“Light Precipitation”/“Misty”<br />
</li>
<li>temp: normalized temperature in Celsius<br />
</li>
<li>atemp: normalized “feeling” temperature in Celsius<br />
</li>
<li>hum: normalized humidity<br />
</li>
<li>windspeed: normalized windspeed<br />
</li>
<li>instant: the time index – number of hours since beginning of data set (not a variable)<br />
</li>
<li>mnth and yr: month and year indices (not variables)</li>
</ul>
<p>We fit a quasipoisson model as the mean and variance are quite different, as calculated below. As with a logistic model, you hope for a pseudo-R2 near to 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the bikes data</span>
<span class="kw">load</span>(<span class="st">&quot;./files/SLinRRegression/Bikes.RData&quot;</span>)
outcome &lt;-<span class="st"> &quot;cnt&quot;</span>
vars &lt;-<span class="st"> </span><span class="kw">names</span>(bikesJuly)[<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>]

<span class="kw">str</span>(bikesJuly)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    744 obs. of  12 variables:
##  $ hr        : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ weathersit: chr  &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ...
##  $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
##  $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...
##  $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
##  $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...
##  $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...
##  $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...
##  $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...
##  $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The outcome column</span>
outcome </code></pre></div>
<pre><code>## [1] &quot;cnt&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The inputs to use</span>
vars </code></pre></div>
<pre><code>## [1] &quot;hr&quot;         &quot;holiday&quot;    &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot;      
## [6] &quot;atemp&quot;      &quot;hum&quot;        &quot;windspeed&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the formula string for bikes rented as a function of the inputs</span>
(fmla &lt;-<span class="st"> </span><span class="kw">paste</span>(outcome, <span class="st">&quot;~&quot;</span>, <span class="kw">paste</span>(vars, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>)))</code></pre></div>
<pre><code>## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate the mean and variance of the outcome</span>
(mean_bikes &lt;-<span class="st"> </span><span class="kw">mean</span>(bikesJuly<span class="op">$</span>cnt))</code></pre></div>
<pre><code>## [1] 273.6653</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(var_bikes &lt;-<span class="st"> </span><span class="kw">var</span>(bikesJuly<span class="op">$</span>cnt))</code></pre></div>
<pre><code>## [1] 45863.84</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the model</span>
bike_model &lt;-<span class="st"> </span><span class="kw">glm</span>(fmla, <span class="dt">data =</span> bikesJuly, <span class="dt">family =</span> quasipoisson)

<span class="co"># Call glance</span>
(perf &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>(bike_model))</code></pre></div>
<pre><code>##   null.deviance df.null logLik AIC BIC deviance df.residual
## 1      133364.9     743     NA  NA  NA  28774.9         712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate pseudo-R-squared</span>
(pseudoR2 &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(perf<span class="op">$</span>deviance <span class="op">/</span><span class="st"> </span>perf<span class="op">$</span>null.deviance)))</code></pre></div>
<pre><code>## [1] 0.7842393</code></pre>
<p>Next we will use the model you built in the previous exercise to make predictions for the month of August. The data set bikesAugust has the same columns as bikesJuly. You must specify type = “response” with predict() when predicting counts from a glm poisson or quasipoisson model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bike_model is in the workspace</span>
<span class="kw">summary</span>(bike_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = fmla, family = quasipoisson, data = bikesJuly)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -21.6117   -4.3121   -0.7223    3.5507   16.5079  
## 
## Coefficients:
##                                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                    5.934986   0.439027  13.519  &lt; 2e-16 ***
## hr1                           -0.580055   0.193354  -3.000 0.002794 ** 
## hr2                           -0.892314   0.215452  -4.142 3.86e-05 ***
## hr3                           -1.662342   0.290658  -5.719 1.58e-08 ***
## hr4                           -2.350204   0.393560  -5.972 3.71e-09 ***
## hr5                           -1.084289   0.230130  -4.712 2.96e-06 ***
## hr6                            0.211945   0.156476   1.354 0.176012    
## hr7                            1.211135   0.132332   9.152  &lt; 2e-16 ***
## hr8                            1.648361   0.127177  12.961  &lt; 2e-16 ***
## hr9                            1.155669   0.133927   8.629  &lt; 2e-16 ***
## hr10                           0.993913   0.137096   7.250 1.09e-12 ***
## hr11                           1.116547   0.136300   8.192 1.19e-15 ***
## hr12                           1.282685   0.134769   9.518  &lt; 2e-16 ***
## hr13                           1.273010   0.135872   9.369  &lt; 2e-16 ***
## hr14                           1.237721   0.136386   9.075  &lt; 2e-16 ***
## hr15                           1.260647   0.136144   9.260  &lt; 2e-16 ***
## hr16                           1.515893   0.132727  11.421  &lt; 2e-16 ***
## hr17                           1.948404   0.128080  15.212  &lt; 2e-16 ***
## hr18                           1.893915   0.127812  14.818  &lt; 2e-16 ***
## hr19                           1.669277   0.128471  12.993  &lt; 2e-16 ***
## hr20                           1.420732   0.131004  10.845  &lt; 2e-16 ***
## hr21                           1.146763   0.134042   8.555  &lt; 2e-16 ***
## hr22                           0.856182   0.138982   6.160 1.21e-09 ***
## hr23                           0.479197   0.148051   3.237 0.001265 ** 
## holidayTRUE                    0.201598   0.079039   2.551 0.010961 *  
## workingdayTRUE                 0.116798   0.033510   3.485 0.000521 ***
## weathersitLight Precipitation -0.214801   0.072699  -2.955 0.003233 ** 
## weathersitMisty               -0.010757   0.038600  -0.279 0.780572    
## temp                          -3.246001   1.148270  -2.827 0.004833 ** 
## atemp                          2.042314   0.953772   2.141 0.032589 *  
## hum                           -0.748557   0.236015  -3.172 0.001581 ** 
## windspeed                      0.003277   0.148814   0.022 0.982439    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 38.98949)
## 
##     Null deviance: 133365  on 743  degrees of freedom
## Residual deviance:  28775  on 712  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make predictions on August data</span>
bikesAugust<span class="op">$</span>pred  &lt;-<span class="st"> </span><span class="kw">predict</span>(bike_model, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">newdata =</span> bikesAugust)

<span class="co"># Calculate the RMSE</span>
bikesAugust <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> pred <span class="op">-</span><span class="st"> </span>cnt) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse  =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residual<span class="op">^</span><span class="dv">2</span>)))</code></pre></div>
<pre><code>##       rmse
## 1 112.5815</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot predictions vs cnt (pred on x-axis)</span>
<span class="kw">ggplot</span>(bikesAugust, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> cnt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;darkblue&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>As the bike rental data is time series data, you might be interested in how the model performs as a function of time. Next we will compare the predictions and actual rentals on an hourly basis, for the first 14 days of August.</p>
<p>To create the plot we use the function tidyr::gather() to consolidate the predicted and actual values from bikesAugust in a single column.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyr)

<span class="co"># Plot predictions and cnt by date/time</span>
Quasipoissonmodel &lt;-<span class="st"> </span>bikesAugust <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># set start to 0, convert unit to days</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">instant =</span> (instant <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(instant))<span class="op">/</span><span class="dv">24</span>) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="co"># gather cnt and pred into a value column</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> valuetype, <span class="dt">value =</span> value, cnt, pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(instant <span class="op">&lt;</span><span class="st"> </span><span class="dv">14</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># restric to first 14 days</span>
<span class="st">  </span><span class="co"># plot value by instant</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> instant, <span class="dt">y =</span> value, <span class="dt">color =</span> valuetype, <span class="dt">linetype =</span> valuetype)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Day&quot;</span>, <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>, <span class="dt">labels =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Predicted August bike rentals, Quasipoisson model&quot;</span>)
Quasipoissonmodel</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Using the chart it appears that this model mostly identifies the slow and busy hours of the day, although it often underestimates peak demand.</p>
</div>
<div id="generalised-additive-model-gam" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Generalised Additive Model (GAM)</h3>
<p>GAM can be used to automatically learn input variable transformations. In GAM, it depends on unknown smoothed functions of the input variables. A GAM learns what best fits the data e.g. quadratic, cubic and so on. We use the mgcv package for GAM models, which has similar functions at the GLM models we saw previously, including a family variable where gaussian (default) is used for regular regression, binomial for probabilities and poisson/quasipoisson for counts. GAMS are prone to overfitting, so are best used on large sets of data. If we want to model the data as a non-linear relationship we use the s notation e.g. anx ~ s(hassles) - this is best done with 10 or more unique values, since a spline function is fitted, so not good for categorical data.</p>
<p>NOTE: GAM is useful for when you don’t have the domain knowledge to determine the best model type so is used as a proxy of the ‘best model’ in the absence of this knowledge.</p>
<p>If you do have categorical variables, you can still use them in GAM, but you don’t specify the s function for those, e.g. if diet and sex are categorical, but age and BMI are continous, we would have:</p>
<blockquote>
<p>Wtloss ~ Diet + Sex + s(Age) + s(BMI)</p>
</blockquote>
<p>Next we will model the average leaf weight on a soybean plant as a function of time (after planting). As you will see, the soybean plant doesn’t grow at a steady rate, but rather has a “growth spurt” that eventually tapers off. Hence, leaf weight is not well described by a linear model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the Soybean data</span>
<span class="kw">load</span>(<span class="st">&quot;./files/SLinRRegression/Soybean.RData&quot;</span>)

<span class="co"># soybean_train is in the workspace</span>
<span class="kw">summary</span>(soybean_train)</code></pre></div>
<pre><code>##       Plot     Variety   Year          Time           weight       
##  1988F6 : 10   F:161   1988:124   Min.   :14.00   Min.   : 0.0290  
##  1988F7 :  9   P:169   1989:102   1st Qu.:27.00   1st Qu.: 0.6663  
##  1988P1 :  9           1990:104   Median :42.00   Median : 3.5233  
##  1988P8 :  9                      Mean   :43.56   Mean   : 6.1645  
##  1988P2 :  9                      3rd Qu.:56.00   3rd Qu.:10.3808  
##  1988F3 :  8                      Max.   :84.00   Max.   :27.3700  
##  (Other):276</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot weight vs Time (Time on x axis)</span>
<span class="kw">ggplot</span>(soybean_train, <span class="kw">aes</span>(<span class="dt">x =</span> Time, <span class="dt">y =</span> weight)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package mgcv</span>
<span class="kw">library</span>(mgcv)</code></pre></div>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## 
## Attaching package: &#39;nlme&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse</code></pre>
<pre><code>## This is mgcv 1.8-20. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the gam/non-linear formula </span>
(fmla.gam &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;weight ~ s(Time)&quot;</span>))</code></pre></div>
<pre><code>## weight ~ s(Time)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the linear formula </span>
(fmla.lin &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;weight ~ Time&quot;</span>))</code></pre></div>
<pre><code>## weight ~ Time</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the GAM Model</span>
model.gam &lt;-<span class="st"> </span><span class="kw">gam</span>(fmla.gam, <span class="dt">family =</span> gaussian, <span class="dt">data =</span> soybean_train)

<span class="co"># Create the linear formula </span>
model.lin &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> fmla.lin, <span class="dt">data =</span> soybean_train)

<span class="co"># Call summary() on model.lin and look for R-squared</span>
<span class="kw">summary</span>(model.lin)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fmla.lin, data = soybean_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3933 -1.7100 -0.3909  1.9056 11.4381 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6.559283   0.358527  -18.30   &lt;2e-16 ***
## Time         0.292094   0.007444   39.24   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.778 on 328 degrees of freedom
## Multiple R-squared:  0.8244, Adjusted R-squared:  0.8238 
## F-statistic:  1540 on 1 and 328 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call summary() on model.gam and look for R-squared</span>
<span class="kw">summary</span>(model.gam)</code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## weight ~ s(Time)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   6.1645     0.1143   53.93   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F p-value    
## s(Time) 8.495   8.93 338.2  &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.902   Deviance explained = 90.4%
## GCV = 4.4395  Scale est. = 4.3117    n = 330</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call plot() on model.gam</span>
<span class="kw">plot</span>(model.gam)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-30-2.png" width="672" /></p>
<p>So we see that the non-linear GAM model fits the data better, with a higher R squared measurement than the linear model.</p>
<p>Next we will predict with both the linear and gam model. As GAM models return a matrix for predictions, we use as.numeric to convert the output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># soybean_test is in the workspace</span>
<span class="kw">summary</span>(soybean_test)</code></pre></div>
<pre><code>##       Plot    Variety   Year         Time           weight       
##  1988F8 : 4   F:43    1988:32   Min.   :14.00   Min.   : 0.0380  
##  1988P7 : 4   P:39    1989:26   1st Qu.:23.00   1st Qu.: 0.4248  
##  1989F8 : 4           1990:24   Median :41.00   Median : 3.0025  
##  1990F8 : 4                     Mean   :44.09   Mean   : 7.1576  
##  1988F4 : 3                     3rd Qu.:69.00   3rd Qu.:15.0113  
##  1988F2 : 3                     Max.   :84.00   Max.   :30.2717  
##  (Other):60</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get predictions from linear model</span>
soybean_test<span class="op">$</span>pred.lin &lt;-<span class="st"> </span><span class="kw">predict</span>(model.lin, <span class="dt">newdata =</span> soybean_test)

<span class="co"># Get predictions from gam model</span>
soybean_test<span class="op">$</span>pred.gam &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(model.gam, <span class="dt">newdata =</span> soybean_test))

<span class="co"># Gather the predictions into a &quot;long&quot; dataset</span>
soybean_long &lt;-<span class="st"> </span>soybean_test <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> modeltype, <span class="dt">value =</span> pred, pred.lin, pred.gam)

<span class="co"># Calculate the rmse</span>
soybean_long <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> weight <span class="op">-</span><span class="st"> </span>pred) <span class="op">%&gt;%</span><span class="st">     </span><span class="co"># residuals</span>
<span class="st">  </span><span class="kw">group_by</span>(modeltype) <span class="op">%&gt;%</span><span class="st">                  </span><span class="co"># group by modeltype</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residual<span class="op">^</span><span class="dv">2</span>))) <span class="co"># calculate the RMSE</span></code></pre></div>
<pre><code>## # A tibble: 2 x 2
##   modeltype     rmse
##       &lt;chr&gt;    &lt;dbl&gt;
## 1  pred.gam 2.286451
## 2  pred.lin 3.190995</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compare the predictions against actual weights on the test data</span>
soybean_long <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Time)) <span class="op">+</span><span class="st">                          </span><span class="co"># the column for the x axis</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> weight)) <span class="op">+</span><span class="st">                    </span><span class="co"># the y-column for the scatterplot</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> modeltype)) <span class="op">+</span><span class="st">   </span><span class="co"># the y-column for the point-and-line plot</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> modeltype, <span class="dt">linetype =</span> modeltype)) <span class="op">+</span><span class="st"> </span><span class="co"># the y-column for the point-and-line plot</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Oberving the plot we can see that the GAM learns the non-linear growth function of the soybean plants, including the fact that weight is never negative, whereas the linear model intercepts below 0 i.e. a negative size.</p>
</div>
</div>
<div id="tree-based-methods" class="section level2">
<h2><span class="header-section-number">3.5</span> Tree-Based Methods</h2>
<p>Tree based models can be used for both regression and classification models. Decision Trees say ‘if a AND b AND c THEN y’. We can therefore model non-linear models and multiplicative relationships - what is the affect of this AND that when combined together.</p>
<p>We can use RMSE as a measure of accuracy of the model. The challenge with tree models is that they are interested in the model space as a whole, splitting this in to regions. Trees can have difficulting fitting linear relationships, so linear models can be better for linear relationships. Trees also have difficulty with variables that change quickly and continously.</p>
<p>We can adjust the tree depth, but there is a risk of overfitting (too deep/complex) or underfitting (too shallow/coarse).</p>
<p>An ensemble model can be built combining different trees or indeed different models together, which will usually have the outcome of being better than a sinlge tree and less prone to overfitting, but at the loss of interpretability. Two such examples of ensemble models are random forests and gradient boosted trees.</p>
<div id="random-forests" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Random Forests</h3>
<p>One example of an ensemble approach is a random forest, building multiple trees from the training data. We build slightly different trees each time to add diversity to the model, by averaging the results of multiple models together to reduce the degree of overfitting. To build a random forest we perform the following</p>
<ol style="list-style-type: decimal">
<li>Draw bootstrapped sample from training data</li>
<li>For each sample grow a tree
<ul>
<li>At each node, pick best variable to split on (from a random subset of all variables)</li>
<li>Continue until tree is grown</li>
</ul></li>
<li>To score a datum, evaluate it with all the trees and average the results.</li>
</ol>
<p>We can use the ranger package to fit random forests. If the outcome is numeric, ranger will automatically do regression rather than classification. The default is for 500 trees, a reccommended minimum is 200. The value respect.unordered.factors will handle categorical values, set it to “order” if using cateogrical values, which will convert the values to numeric values.</p>
<p>The measures of accuracy are R squared and OOB (Out of Bag or out of sample performance). You should still evaluate the model further using test data.</p>
<p>In this exercise you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July.</p>
<p>You will use the ranger package to fit the random forest model. For this exercise, the key arguments to the ranger() call are:</p>
<ul>
<li>formula</li>
<li>data</li>
<li>num.trees: the number of trees in the forest.</li>
<li>respect.unordered.factors : Specifies how to treat unordered factor variables. We recommend setting this to “order” for regression.</li>
<li>seed: because this is a random algorithm, you will set the seed to get reproducible results Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables outcome and vars, and use paste() to assemble a string representing the model formula.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bikesJuly is in the workspace</span>
<span class="kw">str</span>(bikesJuly)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    744 obs. of  12 variables:
##  $ hr        : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ weathersit: chr  &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ...
##  $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
##  $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...
##  $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
##  $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...
##  $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...
##  $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...
##  $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...
##  $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Random seed to reproduce results</span>
seed &lt;-<span class="st"> </span><span class="dv">423563</span>

<span class="co"># The outcome column</span>
(outcome &lt;-<span class="st"> &quot;cnt&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;cnt&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The input variables</span>
(vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;hr&quot;</span>, <span class="st">&quot;holiday&quot;</span>, <span class="st">&quot;workingday&quot;</span>, <span class="st">&quot;weathersit&quot;</span>, <span class="st">&quot;temp&quot;</span>, <span class="st">&quot;atemp&quot;</span>, <span class="st">&quot;hum&quot;</span>, <span class="st">&quot;windspeed&quot;</span>))</code></pre></div>
<pre><code>## [1] &quot;hr&quot;         &quot;holiday&quot;    &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot;      
## [6] &quot;atemp&quot;      &quot;hum&quot;        &quot;windspeed&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the formula string for bikes rented as a function of the inputs</span>
(fmla &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;cnt&quot;</span>, <span class="st">&quot;~&quot;</span>, <span class="kw">paste</span>(vars, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>)))</code></pre></div>
<pre><code>## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package ranger</span>
<span class="kw">library</span>(ranger)

<span class="co"># Fit and print the random forest model</span>
(bike_model_rf &lt;-<span class="st"> </span><span class="kw">ranger</span>(fmla, <span class="co"># formula </span>
                         bikesJuly, <span class="co"># data</span>
                         <span class="dt">num.trees =</span> <span class="dv">500</span>, 
                         <span class="dt">respect.unordered.factors =</span> <span class="st">&quot;order&quot;</span>, 
                         <span class="dt">seed =</span> seed))</code></pre></div>
<pre><code>## Ranger result
## 
## Call:
##  ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;,      seed = seed) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      744 
## Number of independent variables:  8 
## Mtry:                             2 
## Target node size:                 5 
## Variable importance mode:         none 
## OOB prediction error (MSE):       8230.568 
## R squared (OOB):                  0.8205434</code></pre>
<p>Here we see the R squared is very high - around 82%.</p>
<p>Next we will use the model that you fit in the previous exercise to predict bike rentals for the month of August.</p>
<p>The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. You can access predictions with the $ notation for accessing named elements of a list:</p>
<ul>
<li>predict(model, data)$predictions</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bikesAugust is in the workspace</span>
<span class="kw">str</span>(bikesAugust)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    744 obs. of  13 variables:
##  $ hr        : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ workingday: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ weathersit: chr  &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ...
##  $ temp      : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
##  $ atemp     : num  0.636 0.606 0.576 0.576 0.591 ...
##  $ hum       : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
##  $ windspeed : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
##  $ cnt       : int  47 33 13 7 4 49 185 487 681 350 ...
##  $ instant   : int  13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ...
##  $ mnth      : int  8 8 8 8 8 8 8 8 8 8 ...
##  $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ pred      : num  94.96 51.74 37.98 17.58 9.36 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bike_model_rf is in the workspace</span>
bike_model_rf</code></pre></div>
<pre><code>## Ranger result
## 
## Call:
##  ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;,      seed = seed) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      744 
## Number of independent variables:  8 
## Mtry:                             2 
## Target node size:                 5 
## Variable importance mode:         none 
## OOB prediction error (MSE):       8230.568 
## R squared (OOB):                  0.8205434</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make predictions on the August data</span>
bikesAugust<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(bike_model_rf, bikesAugust)<span class="op">$</span>predictions

<span class="co"># Calculate the RMSE of the predictions</span>
bikesAugust <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> cnt <span class="op">-</span><span class="st"> </span>pred)  <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># calculate the residual</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse  =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residual<span class="op">^</span><span class="dv">2</span>)))      <span class="co"># calculate rmse</span></code></pre></div>
<pre><code>##       rmse
## 1 97.18347</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot actual outcome vs predictions (predictions on x-axis)</span>
<span class="kw">ggplot</span>(bikesAugust, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> cnt)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>This random forest model outperforms the poisson count model on the same data; it is discovering more complex non-linear or non-additive relationships in the data.</p>
<p>Next we will visualize the random forest model’s August predictions as a function of time. We can compare the corresponding plot from the quasipoisson model that we built previously.</p>
<p>Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Quasipoissonmodel</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot predictions and cnt by date/time</span>
randomforest_plot &lt;-<span class="st"> </span>bikesAugust <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">instant =</span> (instant <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(instant))<span class="op">/</span><span class="dv">24</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># set start to 0, convert unit to days</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> valuetype, <span class="dt">value =</span> value, cnt, pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(instant <span class="op">&lt;</span><span class="st"> </span><span class="dv">14</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># first two weeks</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> instant, <span class="dt">y =</span> value, <span class="dt">color =</span> valuetype, <span class="dt">linetype =</span> valuetype)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Day&quot;</span>, <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>, <span class="dt">labels =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Predicted August bike rentals, Random Forest plot&quot;</span>)
randomforest_plot</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-34-2.png" width="672" /></p>
<p>The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement.</p>
</div>
<div id="one-hot-encoding" class="section level3">
<h3><span class="header-section-number">3.5.2</span> One-hot encoding</h3>
<p>For modelling purposes, we need to convert categorical variables to indicator variables. Some R packages do this automatically, but some non-native R packages, such as the xgboost package does not, as it is a non-R package (it’s C with a R api). So, these categorical variables need to be converted to numeric ones. We can use the vtreat package.</p>
<ul>
<li>DesignTreatmentsZ() to design a treatment plan from the training data, then</li>
<li>prepare() to created “clean” data</li>
<li>all numerical</li>
<li>no missing values</li>
<li>use prepare() with treatment plan for all future data</li>
</ul>
<p>In this exercise you will use vtreat to one-hot-encode a categorical variable on a small example. vtreat creates a treatment plan to transform categorical variables into indicator variables (coded “lev”), and to clean bad values out of numerical variables (coded “clean”).</p>
<p>To design a treatment plan use the function designTreatmentsZ() e.g.</p>
<blockquote>
<p>treatplan &lt;- designTreatmentsZ(data, varlist)</p>
</blockquote>
<p>data: the original training data frame varlist: a vector of input variables to be treated (as strings). designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables:</p>
<blockquote>
<p>scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code)</p>
</blockquote>
<p>varName: the name of the new treated variable origName: the name of the original variable that the treated variable comes from code: the type of the new variable. “clean”: a numerical variable with no NAs or NaNs “lev”: an indicator variable for a specific level of the original categorical variable.</p>
<p>(magrittr::use_series() is an alias for $ that you can use in pipes.)</p>
<p>For these exercises, we want varName where code is either “clean” or “lev”:</p>
<blockquote>
<p>newvarlist &lt;- scoreFrame %&gt;% filter(code %in% c(“clean”, “lev”) %&gt;% magrittr::use_series(varName)</p>
</blockquote>
<p>To transform the data set into all numerical and one-hot-encoded variables, use prepare():</p>
<blockquote>
<p>data.treat &lt;- prepare(treatplan, data, varRestrictions = newvarlist)</p>
</blockquote>
<p>treatplan: the treatment plan data: the data frame to be treated varRestrictions: the variables desired in the treated data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the dataframe for cleaning</span>
color &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;b&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;b&quot;</span>)
size &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">15</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">7</span>, <span class="dv">12</span>)
popularity &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.0785088</span>, <span class="fl">1.3956245</span>, <span class="fl">0.9217988</span>, <span class="fl">1.2025453</span>, <span class="fl">1.0838662</span>, <span class="fl">0.8043527</span>, <span class="fl">1.1035440</span>, <span class="fl">0.8746332</span>, <span class="fl">0.6947058</span>, <span class="fl">0.8832502</span>)
dframe &lt;-<span class="st"> </span><span class="kw">cbind</span>(color, size, popularity)
dframe &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>((dframe))

<span class="co"># dframe is in the workspace</span>
dframe</code></pre></div>
<pre><code>##    color size popularity
## 1      b   13  1.0785088
## 2      r   11  1.3956245
## 3      r   15  0.9217988
## 4      r   14  1.2025453
## 5      r   13  1.0838662
## 6      b   11  0.8043527
## 7      r    9   1.103544
## 8      g   12  0.8746332
## 9      b    7  0.6947058
## 10     b   12  0.8832502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create and print a vector of variable names</span>
(vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;color&quot;</span>, <span class="st">&quot;size&quot;</span>))</code></pre></div>
<pre><code>## [1] &quot;color&quot; &quot;size&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package vtreat</span>
<span class="kw">library</span>(vtreat)

<span class="co"># Create the treatment plan</span>
treatplan &lt;-<span class="st"> </span><span class="kw">designTreatmentsZ</span>(dframe, vars)</code></pre></div>
<pre><code>## [1] &quot;designing treatments Wed Apr 25 04:41:18 2018&quot;
## [1] &quot;designing treatments Wed Apr 25 04:41:18 2018&quot;
## [1] &quot; have level statistics Wed Apr 25 04:41:18 2018&quot;
## [1] &quot;design var color Wed Apr 25 04:41:18 2018&quot;
## [1] &quot;design var size Wed Apr 25 04:41:18 2018&quot;
## [1] &quot; scoring treatments Wed Apr 25 04:41:18 2018&quot;
## [1] &quot;have treatment plan Wed Apr 25 04:41:18 2018&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Examine the scoreFrame</span>
(scoreFrame &lt;-<span class="st"> </span>treatplan <span class="op">%&gt;%</span>
<span class="st">    </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(scoreFrame) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(varName, origName, code))</code></pre></div>
<pre><code>##          varName origName code
## 1  color_lev_x.b    color  lev
## 2  color_lev_x.g    color  lev
## 3  color_lev_x.r    color  lev
## 4     color_catP    color catP
## 5  size_lev_x.11     size  lev
## 6  size_lev_x.12     size  lev
## 7  size_lev_x.13     size  lev
## 8  size_lev_x.14     size  lev
## 9  size_lev_x.15     size  lev
## 10  size_lev_x.7     size  lev
## 11  size_lev_x.9     size  lev
## 12     size_catP     size catP</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We only want the rows with codes &quot;clean&quot; or &quot;lev&quot;</span>
(newvars &lt;-<span class="st"> </span>scoreFrame <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(code <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;clean&quot;</span>, <span class="st">&quot;lev&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(varName))</code></pre></div>
<pre><code>##  [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot;
##  [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot;
##  [9] &quot;size_lev_x.7&quot;  &quot;size_lev_x.9&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the treated training data</span>
(dframe.treat &lt;-<span class="st"> </span><span class="kw">prepare</span>(treatplan, dframe, <span class="dt">varRestriction =</span> newvars))</code></pre></div>
<pre><code>##    color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12
## 1              1             0             0             0             0
## 2              0             0             1             1             0
## 3              0             0             1             0             0
## 4              0             0             1             0             0
## 5              0             0             1             0             0
## 6              1             0             0             1             0
## 7              0             0             1             0             0
## 8              0             1             0             0             1
## 9              1             0             0             0             0
## 10             1             0             0             0             1
##    size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9
## 1              1             0             0            0            0
## 2              0             0             0            0            0
## 3              0             0             1            0            0
## 4              0             1             0            0            0
## 5              1             0             0            0            0
## 6              0             0             0            0            0
## 7              0             0             0            0            1
## 8              0             0             0            0            0
## 9              0             0             0            1            0
## 10             0             0             0            0            0</code></pre>
<p>WE have have successfully one-hot-encoded categorical data. The new indicator variables have ‘<em>lev</em>’ in their names, and the new cleaned continuous variables have ’_clean’ in their names. The treated data is all numerical, with no missing values, and is suitable for use with xgboost and other R modeling functions.</p>
<p>When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly.</p>
<p>vtreat is a “safer” alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the testframe for testing new vars</span>
color &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;r&quot;</span>)
size &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">12</span>, <span class="dv">12</span>, <span class="dv">12</span>, <span class="dv">8</span>)
popularity &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.9733920</span>, <span class="fl">0.9122529</span>, <span class="fl">1.4217153</span>, <span class="fl">1.1905828</span>, <span class="fl">0.9866464</span>, <span class="fl">1.3697515</span>, <span class="fl">1.0959387</span>, <span class="fl">0.9161547</span>, <span class="fl">1.0000460</span>, <span class="fl">1.3137360</span>)
testframe &lt;-<span class="st"> </span><span class="kw">cbind</span>(color, size, popularity)
testframe &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>((dframe))

<span class="co"># treatplan is in the workspace</span>
<span class="kw">summary</span>(treatplan)</code></pre></div>
<pre><code>##               Length Class           Mode     
## treatments    4      -none-          list     
## scoreFrame    8      data.frame      list     
## outcomename   1      -none-          character
## vtreatVersion 1      package_version list     
## outcomeType   1      -none-          character
## outcomeTarget 1      -none-          character
## meanY         1      -none-          logical  
## splitmethod   1      -none-          character</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># newvars is in the workspace</span>
newvars</code></pre></div>
<pre><code>##  [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot;
##  [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot;
##  [9] &quot;size_lev_x.7&quot;  &quot;size_lev_x.9&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print dframe and testframe</span>
dframe</code></pre></div>
<pre><code>##    color size popularity
## 1      b   13  1.0785088
## 2      r   11  1.3956245
## 3      r   15  0.9217988
## 4      r   14  1.2025453
## 5      r   13  1.0838662
## 6      b   11  0.8043527
## 7      r    9   1.103544
## 8      g   12  0.8746332
## 9      b    7  0.6947058
## 10     b   12  0.8832502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">testframe</code></pre></div>
<pre><code>##    color size popularity
## 1      b   13  1.0785088
## 2      r   11  1.3956245
## 3      r   15  0.9217988
## 4      r   14  1.2025453
## 5      r   13  1.0838662
## 6      b   11  0.8043527
## 7      r    9   1.103544
## 8      g   12  0.8746332
## 9      b    7  0.6947058
## 10     b   12  0.8832502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use prepare() to one-hot-encode testframe</span>
(testframe.treat &lt;-<span class="st"> </span><span class="kw">prepare</span>(treatplan, testframe, <span class="dt">varRestriction =</span> newvars))</code></pre></div>
<pre><code>##    color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12
## 1              1             0             0             0             0
## 2              0             0             1             1             0
## 3              0             0             1             0             0
## 4              0             0             1             0             0
## 5              0             0             1             0             0
## 6              1             0             0             1             0
## 7              0             0             1             0             0
## 8              0             1             0             0             1
## 9              1             0             0             0             0
## 10             1             0             0             0             1
##    size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9
## 1              1             0             0            0            0
## 2              0             0             0            0            0
## 3              0             0             1            0            0
## 4              0             1             0            0            0
## 5              1             0             0            0            0
## 6              0             0             0            0            0
## 7              0             0             0            0            1
## 8              0             0             0            0            0
## 9              0             0             0            1            0
## 10             0             0             0            0            0</code></pre>
<p>vtreat encodes novel colors like yellow that were not present in the data as all zeros: ‘none of the known colors’. This allows downstream models to accept these novel values without crashing.</p>
<p>Next we will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on. vars defines the variable vars with the list of variable columns for the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The outcome column</span>
(outcome &lt;-<span class="st"> &quot;cnt&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;cnt&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The input columns</span>
(vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;hr&quot;</span>, <span class="st">&quot;holiday&quot;</span>, <span class="st">&quot;workingday&quot;</span>, <span class="st">&quot;weathersit&quot;</span>, <span class="st">&quot;temp&quot;</span>, <span class="st">&quot;atemp&quot;</span>, <span class="st">&quot;hum&quot;</span>, <span class="st">&quot;windspeed&quot;</span>))</code></pre></div>
<pre><code>## [1] &quot;hr&quot;         &quot;holiday&quot;    &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot;      
## [6] &quot;atemp&quot;      &quot;hum&quot;        &quot;windspeed&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package vtreat</span>
<span class="kw">library</span>(vtreat)

<span class="co"># Create the treatment plan from bikesJuly (the training data)</span>
treatplan &lt;-<span class="st"> </span><span class="kw">designTreatmentsZ</span>(bikesJuly, vars, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)

<span class="co"># Get the &quot;clean&quot; and &quot;lev&quot; variables from the scoreFrame</span>
(newvars &lt;-<span class="st"> </span>treatplan <span class="op">%&gt;%</span>
<span class="st">  </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(scoreFrame) <span class="op">%&gt;%</span><span class="st">        </span>
<span class="st">  </span><span class="kw">filter</span>(code <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;clean&quot;</span>, <span class="st">&quot;lev&quot;</span>)) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># get the rows you care about</span>
<span class="st">  </span>magrittr<span class="op">::</span><span class="kw">use_series</span>(varName))           <span class="co"># get the varName column</span></code></pre></div>
<pre><code>##  [1] &quot;hr_lev_x.0&quot;                             
##  [2] &quot;hr_lev_x.1&quot;                             
##  [3] &quot;hr_lev_x.10&quot;                            
##  [4] &quot;hr_lev_x.11&quot;                            
##  [5] &quot;hr_lev_x.12&quot;                            
##  [6] &quot;hr_lev_x.13&quot;                            
##  [7] &quot;hr_lev_x.14&quot;                            
##  [8] &quot;hr_lev_x.15&quot;                            
##  [9] &quot;hr_lev_x.16&quot;                            
## [10] &quot;hr_lev_x.17&quot;                            
## [11] &quot;hr_lev_x.18&quot;                            
## [12] &quot;hr_lev_x.19&quot;                            
## [13] &quot;hr_lev_x.2&quot;                             
## [14] &quot;hr_lev_x.20&quot;                            
## [15] &quot;hr_lev_x.21&quot;                            
## [16] &quot;hr_lev_x.22&quot;                            
## [17] &quot;hr_lev_x.23&quot;                            
## [18] &quot;hr_lev_x.3&quot;                             
## [19] &quot;hr_lev_x.4&quot;                             
## [20] &quot;hr_lev_x.5&quot;                             
## [21] &quot;hr_lev_x.6&quot;                             
## [22] &quot;hr_lev_x.7&quot;                             
## [23] &quot;hr_lev_x.8&quot;                             
## [24] &quot;hr_lev_x.9&quot;                             
## [25] &quot;holiday_clean&quot;                          
## [26] &quot;workingday_clean&quot;                       
## [27] &quot;weathersit_lev_x.Clear.to.partly.cloudy&quot;
## [28] &quot;weathersit_lev_x.Light.Precipitation&quot;   
## [29] &quot;weathersit_lev_x.Misty&quot;                 
## [30] &quot;temp_clean&quot;                             
## [31] &quot;atemp_clean&quot;                            
## [32] &quot;hum_clean&quot;                              
## [33] &quot;windspeed_clean&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prepare the training data</span>
bikesJuly.treat &lt;-<span class="st"> </span><span class="kw">prepare</span>(treatplan, bikesJuly,  <span class="dt">varRestriction =</span> newvars)

<span class="co"># Prepare the test data</span>
bikesAugust.treat &lt;-<span class="st"> </span><span class="kw">prepare</span>(treatplan, bikesAugust,  <span class="dt">varRestriction =</span> newvars)

<span class="co"># Call str() on the treated data</span>
<span class="kw">str</span>(bikesAugust.treat)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    744 obs. of  33 variables:
##  $ hr_lev_x.0                             : num  1 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.1                             : num  0 1 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.10                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.11                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.12                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.13                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.14                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.15                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.16                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.17                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.18                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.19                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.2                             : num  0 0 1 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.20                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.21                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.22                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.23                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.3                             : num  0 0 0 1 0 0 0 0 0 0 ...
##  $ hr_lev_x.4                             : num  0 0 0 0 1 0 0 0 0 0 ...
##  $ hr_lev_x.5                             : num  0 0 0 0 0 1 0 0 0 0 ...
##  $ hr_lev_x.6                             : num  0 0 0 0 0 0 1 0 0 0 ...
##  $ hr_lev_x.7                             : num  0 0 0 0 0 0 0 1 0 0 ...
##  $ hr_lev_x.8                             : num  0 0 0 0 0 0 0 0 1 0 ...
##  $ hr_lev_x.9                             : num  0 0 0 0 0 0 0 0 0 1 ...
##  $ holiday_clean                          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ workingday_clean                       : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ weathersit_lev_x.Clear.to.partly.cloudy: num  1 1 1 1 0 0 1 0 0 0 ...
##  $ weathersit_lev_x.Light.Precipitation   : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ weathersit_lev_x.Misty                 : num  0 0 0 0 1 1 0 1 1 1 ...
##  $ temp_clean                             : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
##  $ atemp_clean                            : num  0.636 0.606 0.576 0.576 0.591 ...
##  $ hum_clean                              : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
##  $ windspeed_clean                        : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(bikesJuly.treat)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    744 obs. of  33 variables:
##  $ hr_lev_x.0                             : num  1 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.1                             : num  0 1 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.10                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.11                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.12                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.13                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.14                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.15                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.16                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.17                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.18                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.19                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.2                             : num  0 0 1 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.20                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.21                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.22                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.23                            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hr_lev_x.3                             : num  0 0 0 1 0 0 0 0 0 0 ...
##  $ hr_lev_x.4                             : num  0 0 0 0 1 0 0 0 0 0 ...
##  $ hr_lev_x.5                             : num  0 0 0 0 0 1 0 0 0 0 ...
##  $ hr_lev_x.6                             : num  0 0 0 0 0 0 1 0 0 0 ...
##  $ hr_lev_x.7                             : num  0 0 0 0 0 0 0 1 0 0 ...
##  $ hr_lev_x.8                             : num  0 0 0 0 0 0 0 0 1 0 ...
##  $ hr_lev_x.9                             : num  0 0 0 0 0 0 0 0 0 1 ...
##  $ holiday_clean                          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ workingday_clean                       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ weathersit_lev_x.Clear.to.partly.cloudy: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ weathersit_lev_x.Light.Precipitation   : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ weathersit_lev_x.Misty                 : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ temp_clean                             : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
##  $ atemp_clean                            : num  0.727 0.697 0.697 0.712 0.667 ...
##  $ hum_clean                              : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
##  $ windspeed_clean                        : num  0 0.1343 0.0896 0.1343 0.194 ...</code></pre>
<p>The bike data is now in completely numeric form, ready to use with xgboost. Note that the treated data does not include the outcome column.</p>
</div>
<div id="gradient-boosting-machines" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Gradient Boosting Machines</h3>
<p>Gradient boosting is an interative ensemble method, by improving the model each time. We start the model with a usually shallow tree. Next, we fit another model to the residuals of the model, then find the weighted sum of the second and first models that give the best fit. We can regualrise the learning by the factor eta, eta = 1 gives fast learning but with overfitting risk, smaller eta reduces speed of learning but reduces the risk of overfitting. We then repeat this process until the stopping condition is met.</p>
<p>Gradient boosting works on the training data, so it can be easy to overfit. The best approach then is to use OOB and cross validation (CV) for each model, then determine how many trees to use.</p>
<p>xgb.cv() is the function we use and has a number of diagnostic measures. One such measure is the</p>
<ul>
<li>xgb.cv()$evaluation_log: records estimated RMSE for each round - find the number that minimises the RMSE</li>
</ul>
<p>Inputs to xgb.cv() and xgboost() are:</p>
<ul>
<li>data: input data as matrix ; label: outcome</li>
<li>label: vector of outcomes (also numeric)</li>
<li>objective: for regression - “reg:linear”</li>
<li>nrounds: maximum number of trees to fit</li>
<li>eta: learning rate</li>
<li>max_depth: depth of trees</li>
<li>early_stopping_rounds: after this many rounds without improvement, stop</li>
<li>nfold (xgb.cv() only): number of folds for cross validation. 5 is a good number</li>
<li>verbose: 0 to stay silent.</li>
</ul>
<p>Then we use</p>
<p>elog &lt;- as.data.frame(cv<span class="math inline">\(evaluation_log) nrounds &lt;- which.min(elog\)</span>test_rmse_mean)</p>
<p>With the resulting number being the best number of trees. We then use xbgoost with this number (nrounds &lt;- n) to get the final model.</p>
<p>Next we will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. We will train the model on data from the month of July.</p>
<p>The July data is loaded into your workspace. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt.</p>
<p>We will use the xgboost package to fit the random forest model. The function xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The July data is in the workspace</span>
<span class="kw">ls</span>()</code></pre></div>
<pre><code>##  [1] &quot;bike_model&quot;          &quot;bike_model_rf&quot;       &quot;bikesAugust&quot;        
##  [4] &quot;bikesAugust.treat&quot;   &quot;bikesJuly&quot;           &quot;bikesJuly.treat&quot;    
##  [7] &quot;bloodpressure&quot;       &quot;bloodpressure_model&quot; &quot;color&quot;              
## [10] &quot;dframe&quot;              &quot;dframe.treat&quot;        &quot;fdata&quot;              
## [13] &quot;fdata2&quot;              &quot;fe_mean&quot;             &quot;flower_model&quot;       
## [16] &quot;flowers&quot;             &quot;fmla&quot;                &quot;fmla.gam&quot;           
## [19] &quot;fmla.lin&quot;            &quot;fmla.log&quot;            &quot;fmla_sqr&quot;           
## [22] &quot;gp&quot;                  &quot;houseprice&quot;          &quot;houseprice_long&quot;    
## [25] &quot;i&quot;                   &quot;incometest&quot;          &quot;incometrain&quot;        
## [28] &quot;k&quot;                   &quot;mean_bikes&quot;          &quot;mmat&quot;               
## [31] &quot;model&quot;               &quot;model.gam&quot;           &quot;model.lin&quot;          
## [34] &quot;model.log&quot;           &quot;model_lin&quot;           &quot;model_sqr&quot;          
## [37] &quot;mpg&quot;                 &quot;mpg_model&quot;           &quot;mpg_test&quot;           
## [40] &quot;mpg_train&quot;           &quot;N&quot;                   &quot;newrates&quot;           
## [43] &quot;newvars&quot;             &quot;nRows&quot;               &quot;outcome&quot;            
## [46] &quot;perf&quot;                &quot;popularity&quot;          &quot;pred&quot;               
## [49] &quot;pseudoR2&quot;            &quot;Quasipoissonmodel&quot;   &quot;randomforest_plot&quot;  
## [52] &quot;res&quot;                 &quot;rho&quot;                 &quot;rho2&quot;               
## [55] &quot;rmse&quot;                &quot;rmse_test&quot;           &quot;rmse_train&quot;         
## [58] &quot;rsq&quot;                 &quot;rsq_glance&quot;          &quot;rsq_test&quot;           
## [61] &quot;rsq_train&quot;           &quot;rss&quot;                 &quot;scoreFrame&quot;         
## [64] &quot;sd_unemployment&quot;     &quot;seed&quot;                &quot;size&quot;               
## [67] &quot;soybean_long&quot;        &quot;soybean_test&quot;        &quot;soybean_train&quot;      
## [70] &quot;sparrow&quot;             &quot;sparrow_model&quot;       &quot;split&quot;              
## [73] &quot;splitPlan&quot;           &quot;target&quot;              &quot;testframe&quot;          
## [76] &quot;testframe.treat&quot;     &quot;treatplan&quot;           &quot;tss&quot;                
## [79] &quot;unemployment&quot;        &quot;unemployment_model&quot;  &quot;var_bikes&quot;          
## [82] &quot;vars&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package xgboost</span>
<span class="kw">library</span>(xgboost)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;xgboost&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Run xgb.cv</span>
cv &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(bikesJuly.treat), 
            <span class="dt">label =</span> bikesJuly<span class="op">$</span>cnt,
            <span class="dt">nrounds =</span> <span class="dv">100</span>,
            <span class="dt">nfold =</span> <span class="dv">5</span>,
            <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,
            <span class="dt">eta =</span> <span class="fl">0.3</span>,
            <span class="dt">max_depth =</span> <span class="dv">6</span>,
            <span class="dt">early_stopping_rounds =</span> <span class="dv">10</span>,
            <span class="dt">verbose =</span> <span class="dv">0</span>    <span class="co"># silent</span>
)

<span class="co"># Get the evaluation log </span>
elog &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(cv<span class="op">$</span>evaluation_log)

<span class="co"># Determine and print how many trees minimize training and test error</span>
elog <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">   </span><span class="kw">summarize</span>(<span class="dt">ntrees.train =</span> <span class="kw">which.min</span>(elog<span class="op">$</span>train_rmse_mean),   <span class="co"># find the index of min(train_rmse_mean)</span>
             <span class="dt">ntrees.test  =</span> <span class="kw">which.min</span>(elog<span class="op">$</span>test_rmse_mean))   <span class="co"># find the index of min(test_rmse_mean)</span></code></pre></div>
<pre><code>##   ntrees.train ntrees.test
## 1           77          67</code></pre>
<p>In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It’s important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model.</p>
<p>Next we will fit a gradient boosting model using xgboost() to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. We will train the model on data from the month of July and predict on data for the month of August.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The number of trees to use, as determined by xgb.cv</span>
ntrees &lt;-<span class="st"> </span><span class="dv">84</span>

<span class="co"># Run xgboost</span>
bike_model_xgb &lt;-<span class="st"> </span><span class="kw">xgboost</span>(<span class="dt">data =</span> <span class="kw">as.matrix</span>(bikesJuly.treat), <span class="co"># training data as matrix</span>
                   <span class="dt">label =</span> bikesJuly<span class="op">$</span>cnt,  <span class="co"># column of outcomes</span>
                   <span class="dt">nrounds =</span> ntrees,       <span class="co"># number of trees to build</span>
                   <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, <span class="co"># objective</span>
                   <span class="dt">eta =</span> <span class="fl">0.3</span>,
                   <span class="dt">depth =</span> <span class="dv">6</span>,
                   <span class="dt">verbose =</span> <span class="dv">0</span>  <span class="co"># silent</span>
)

<span class="co"># Make predictions</span>
bikesAugust<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(bike_model_xgb, <span class="kw">as.matrix</span>(bikesAugust.treat))

<span class="co"># Plot predictions (on x axis) vs actual bike rental count</span>
<span class="kw">ggplot</span>(bikesAugust, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> cnt)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-39-1.png" width="672" /> Overall, the scatterplot looked pretty good, but notice that the model made some negative predictions.</p>
<p>In the next exercise, you’ll compare this model’s RMSE to the previous bike models that you’ve built.</p>
<p>Finally we can calculate the RMSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate RMSE</span>
bikesAugust <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residuals =</span> cnt <span class="op">-</span><span class="st"> </span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>(residuals <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)))</code></pre></div>
<pre><code>##       rmse
## 1 76.36407</code></pre>
<p>Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous two models. Perhaps rounding negative predictions up to zero is a reasonable tradeoff.</p>
<p>Finally we print our previous plots and create a new one for the xgb and compare the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Quasipoissonmodel</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">randomforest_plot</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-41-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot predictions and actual bike rentals as a function of time (days)</span>
bikesAugust <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">instant =</span> (instant <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(instant))<span class="op">/</span><span class="dv">24</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># set start to 0, convert unit to days</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> valuetype, <span class="dt">value =</span> value, cnt, pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(instant <span class="op">&lt;</span><span class="st"> </span><span class="dv">14</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># first two weeks</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> instant, <span class="dt">y =</span> value, <span class="dt">color =</span> valuetype, <span class="dt">linetype =</span> valuetype)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Day&quot;</span>, <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>, <span class="dt">labels =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">14</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Predicted August bike rentals, Gradient Boosting model&quot;</span>)</code></pre></div>
<p><img src="SLinRRegression_files/figure-html/unnamed-chunk-41-3.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="supervised-learning-in-r-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machine-learning-toolbox.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["BI-Notes.pdf", "BI-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
