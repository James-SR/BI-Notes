<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>BI Notes</title>
  <meta name="description" content="Study notes taken from BI courses and self learning.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="BI Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/BI-Notes" />
  
  <meta property="og:description" content="Study notes taken from BI courses and self learning." />
  <meta name="github-repo" content="James-SR/BI-Notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="BI Notes" />
  
  <meta name="twitter:description" content="Study notes taken from BI courses and self learning." />
  

<meta name="author" content="James Solomon-Rounce">


<meta name="date" content="2018-04-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="supervised-learning-in-r-regression.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Business Intelligence Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html"><i class="fa fa-check"></i><b>1</b> Querying Data with Transact-SQL</a><ul>
<li class="chapter" data-level="1.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#loading-required-package-methods"><i class="fa fa-check"></i><b>1.1</b> Loading required package: methods</a></li>
<li class="chapter" data-level="1.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#introduction-to-transact-sql"><i class="fa fa-check"></i><b>1.2</b> Introduction to Transact-SQL</a><ul>
<li class="chapter" data-level="1.2.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#data-types"><i class="fa fa-check"></i><b>1.2.1</b> Data Types</a></li>
<li class="chapter" data-level="1.2.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#working-with-nulls"><i class="fa fa-check"></i><b>1.2.2</b> Working with NULLs</a></li>
<li class="chapter" data-level="1.2.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises"><i class="fa fa-check"></i><b>1.2.3</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#querying-tables-with-select"><i class="fa fa-check"></i><b>1.3</b> Querying Tables with SELECT</a><ul>
<li class="chapter" data-level="1.3.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#removing-duplicates"><i class="fa fa-check"></i><b>1.3.1</b> Removing Duplicates</a></li>
<li class="chapter" data-level="1.3.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#sorting-results"><i class="fa fa-check"></i><b>1.3.2</b> Sorting Results</a></li>
<li class="chapter" data-level="1.3.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#paging-through-results"><i class="fa fa-check"></i><b>1.3.3</b> Paging through results</a></li>
<li class="chapter" data-level="1.3.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#filtering-and-using-predicates"><i class="fa fa-check"></i><b>1.3.4</b> Filtering and Using Predicates</a></li>
<li class="chapter" data-level="1.3.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-1"><i class="fa fa-check"></i><b>1.3.5</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#querying-tables-with-joins"><i class="fa fa-check"></i><b>1.4</b> Querying Tables with Joins</a><ul>
<li class="chapter" data-level="1.4.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#inner-joins"><i class="fa fa-check"></i><b>1.4.1</b> INNER Joins</a></li>
<li class="chapter" data-level="1.4.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#outer-joins"><i class="fa fa-check"></i><b>1.4.2</b> OUTER Joins</a></li>
<li class="chapter" data-level="1.4.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#cross-joins"><i class="fa fa-check"></i><b>1.4.3</b> Cross Joins</a></li>
<li class="chapter" data-level="1.4.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#self-joins"><i class="fa fa-check"></i><b>1.4.4</b> Self Joins</a></li>
<li class="chapter" data-level="1.4.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-2"><i class="fa fa-check"></i><b>1.4.5</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-set-operators"><i class="fa fa-check"></i><b>1.5</b> Using Set Operators</a><ul>
<li class="chapter" data-level="1.5.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#intersect-and-except-queries"><i class="fa fa-check"></i><b>1.5.1</b> INTERSECT and EXCEPT Queries</a></li>
<li class="chapter" data-level="1.5.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-3"><i class="fa fa-check"></i><b>1.5.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-functions-and-aggregating-data"><i class="fa fa-check"></i><b>1.6</b> Using Functions and Aggregating Data</a><ul>
<li class="chapter" data-level="1.6.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#scalar-functions"><i class="fa fa-check"></i><b>1.6.1</b> Scalar Functions</a></li>
<li class="chapter" data-level="1.6.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#logical-functions"><i class="fa fa-check"></i><b>1.6.2</b> Logical Functions</a></li>
<li class="chapter" data-level="1.6.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#window-functions"><i class="fa fa-check"></i><b>1.6.3</b> Window Functions</a></li>
<li class="chapter" data-level="1.6.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#aggregate-functions"><i class="fa fa-check"></i><b>1.6.4</b> Aggregate Functions</a></li>
<li class="chapter" data-level="1.6.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#filtering-groups"><i class="fa fa-check"></i><b>1.6.5</b> Filtering Groups</a></li>
<li class="chapter" data-level="1.6.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-4"><i class="fa fa-check"></i><b>1.6.6</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#sub-queries-and-apply"><i class="fa fa-check"></i><b>1.7</b> Sub-queries and Apply</a><ul>
<li class="chapter" data-level="1.7.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#self-contained-or-correlated-query"><i class="fa fa-check"></i><b>1.7.1</b> Self Contained or Correlated Query</a></li>
<li class="chapter" data-level="1.7.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#the-apply-operator"><i class="fa fa-check"></i><b>1.7.2</b> The Apply Operator</a></li>
<li class="chapter" data-level="1.7.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-5"><i class="fa fa-check"></i><b>1.7.3</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-table-expressions"><i class="fa fa-check"></i><b>1.8</b> Using Table Expressions</a><ul>
<li class="chapter" data-level="1.8.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#views"><i class="fa fa-check"></i><b>1.8.1</b> Views</a></li>
<li class="chapter" data-level="1.8.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-temporary-tables-and-table-variables"><i class="fa fa-check"></i><b>1.8.2</b> Using Temporary Tables and Table Variables</a></li>
<li class="chapter" data-level="1.8.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#table-value-functions-tvf"><i class="fa fa-check"></i><b>1.8.3</b> Table Value Functions (TVF)</a></li>
<li class="chapter" data-level="1.8.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#derived-tables"><i class="fa fa-check"></i><b>1.8.4</b> Derived Tables</a></li>
<li class="chapter" data-level="1.8.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#using-common-table-expressions-ctes"><i class="fa fa-check"></i><b>1.8.5</b> Using Common Table Expressions (CTEs)</a></li>
<li class="chapter" data-level="1.8.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-6"><i class="fa fa-check"></i><b>1.8.6</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#grouping-sets-and-pivoting-data"><i class="fa fa-check"></i><b>1.9</b> Grouping Sets and Pivoting Data</a><ul>
<li class="chapter" data-level="1.9.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#pivoting-data"><i class="fa fa-check"></i><b>1.9.1</b> Pivoting Data</a></li>
<li class="chapter" data-level="1.9.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-7"><i class="fa fa-check"></i><b>1.9.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#modifying-data"><i class="fa fa-check"></i><b>1.10</b> Modifying Data</a><ul>
<li class="chapter" data-level="1.10.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#updating-and-deleting-data"><i class="fa fa-check"></i><b>1.10.1</b> Updating and Deleting Data</a></li>
<li class="chapter" data-level="1.10.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-8"><i class="fa fa-check"></i><b>1.10.2</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#programming-with-transact-sql"><i class="fa fa-check"></i><b>1.11</b> Programming with Transact-SQL</a><ul>
<li class="chapter" data-level="1.11.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#batches"><i class="fa fa-check"></i><b>1.11.1</b> Batches</a></li>
<li class="chapter" data-level="1.11.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#comments"><i class="fa fa-check"></i><b>1.11.2</b> Comments</a></li>
<li class="chapter" data-level="1.11.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#variables"><i class="fa fa-check"></i><b>1.11.3</b> Variables</a></li>
<li class="chapter" data-level="1.11.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#conditional-branching"><i class="fa fa-check"></i><b>1.11.4</b> Conditional Branching</a></li>
<li class="chapter" data-level="1.11.5" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#looping"><i class="fa fa-check"></i><b>1.11.5</b> Looping</a></li>
<li class="chapter" data-level="1.11.6" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#stored-procedures"><i class="fa fa-check"></i><b>1.11.6</b> Stored Procedures</a></li>
<li class="chapter" data-level="1.11.7" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-9"><i class="fa fa-check"></i><b>1.11.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#error-handling-and-transactions"><i class="fa fa-check"></i><b>1.12</b> Error Handling and Transactions</a><ul>
<li class="chapter" data-level="1.12.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#raising-or-throwing-errors"><i class="fa fa-check"></i><b>1.12.1</b> Raising or Throwing Errors</a></li>
<li class="chapter" data-level="1.12.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#catching-and-handling-errors"><i class="fa fa-check"></i><b>1.12.2</b> Catching and Handling Errors</a></li>
<li class="chapter" data-level="1.12.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#transactions"><i class="fa fa-check"></i><b>1.12.3</b> Transactions</a></li>
<li class="chapter" data-level="1.12.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#lab-exercises-10"><i class="fa fa-check"></i><b>1.12.4</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#final-assessment"><i class="fa fa-check"></i><b>1.13</b> Final Assessment</a><ul>
<li class="chapter" data-level="1.13.1" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-1"><i class="fa fa-check"></i><b>1.13.1</b> Section 1</a></li>
<li class="chapter" data-level="1.13.2" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-2"><i class="fa fa-check"></i><b>1.13.2</b> Section 2</a></li>
<li class="chapter" data-level="1.13.3" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-3"><i class="fa fa-check"></i><b>1.13.3</b> Section 3</a></li>
<li class="chapter" data-level="1.13.4" data-path="querying-data-with-transact-sql.html"><a href="querying-data-with-transact-sql.html#section-4"><i class="fa fa-check"></i><b>1.13.4</b> Section 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html"><i class="fa fa-check"></i><b>2</b> Supervised Learning In R Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>2.1</b> k-Nearest Neighbors (kNN)</a></li>
<li class="chapter" data-level="2.2" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#data.frame-145-obs.-of-49-variables"><i class="fa fa-check"></i><b>2.2</b> ‘data.frame’: 145 obs. of 49 variables:</a></li>
<li class="chapter" data-level="2.3" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#sign_type-chr-pedestrian-pedestrian-pedestrian-pedestrian"><i class="fa fa-check"></i><b>2.3</b> $ sign_type: chr “pedestrian” “pedestrian” “pedestrian” “pedestrian” …</a></li>
<li class="chapter" data-level="2.4" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r1-int-155-142-57-22-169-75-136-149-13-123"><i class="fa fa-check"></i><b>2.4</b> $ r1 : int 155 142 57 22 169 75 136 149 13 123 …</a></li>
<li class="chapter" data-level="2.5" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g1-int-228-217-54-35-179-67-149-225-34-124"><i class="fa fa-check"></i><b>2.5</b> $ g1 : int 228 217 54 35 179 67 149 225 34 124 …</a></li>
<li class="chapter" data-level="2.6" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b1-int-251-242-50-41-170-60-157-241-28-107"><i class="fa fa-check"></i><b>2.6</b> $ b1 : int 251 242 50 41 170 60 157 241 28 107 …</a></li>
<li class="chapter" data-level="2.7" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r2-int-135-166-187-171-231-131-200-34-5-83"><i class="fa fa-check"></i><b>2.7</b> $ r2 : int 135 166 187 171 231 131 200 34 5 83 …</a></li>
<li class="chapter" data-level="2.8" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g2-int-188-204-201-178-254-89-203-45-21-61"><i class="fa fa-check"></i><b>2.8</b> $ g2 : int 188 204 201 178 254 89 203 45 21 61 …</a></li>
<li class="chapter" data-level="2.9" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b2-int-101-44-68-26-27-53-107-1-11-26"><i class="fa fa-check"></i><b>2.9</b> $ b2 : int 101 44 68 26 27 53 107 1 11 26 …</a></li>
<li class="chapter" data-level="2.10" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r3-int-156-142-51-19-97-214-150-155-123-116"><i class="fa fa-check"></i><b>2.10</b> $ r3 : int 156 142 51 19 97 214 150 155 123 116 …</a></li>
<li class="chapter" data-level="2.11" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g3-int-227-217-51-27-107-144-167-226-154-124"><i class="fa fa-check"></i><b>2.11</b> $ g3 : int 227 217 51 27 107 144 167 226 154 124 …</a></li>
<li class="chapter" data-level="2.12" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b3-int-245-242-45-29-99-75-134-238-140-115"><i class="fa fa-check"></i><b>2.12</b> $ b3 : int 245 242 45 29 99 75 134 238 140 115 …</a></li>
<li class="chapter" data-level="2.13" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r4-int-145-147-59-19-123-156-171-147-21-67"><i class="fa fa-check"></i><b>2.13</b> $ r4 : int 145 147 59 19 123 156 171 147 21 67 …</a></li>
<li class="chapter" data-level="2.14" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g4-int-211-219-62-27-147-169-218-222-46-67"><i class="fa fa-check"></i><b>2.14</b> $ g4 : int 211 219 62 27 147 169 218 222 46 67 …</a></li>
<li class="chapter" data-level="2.15" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b4-int-228-242-65-29-152-190-252-242-41-52"><i class="fa fa-check"></i><b>2.15</b> $ b4 : int 228 242 65 29 152 190 252 242 41 52 …</a></li>
<li class="chapter" data-level="2.16" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r5-int-166-164-156-42-221-67-171-170-36-70"><i class="fa fa-check"></i><b>2.16</b> $ r5 : int 166 164 156 42 221 67 171 170 36 70 …</a></li>
<li class="chapter" data-level="2.17" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g5-int-233-228-171-37-236-50-158-191-60-53"><i class="fa fa-check"></i><b>2.17</b> $ g5 : int 233 228 171 37 236 50 158 191 60 53 …</a></li>
<li class="chapter" data-level="2.18" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b5-int-245-229-50-3-117-36-108-113-26-26"><i class="fa fa-check"></i><b>2.18</b> $ b5 : int 245 229 50 3 117 36 108 113 26 26 …</a></li>
<li class="chapter" data-level="2.19" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r6-int-212-84-254-217-205-37-157-26-75-26"><i class="fa fa-check"></i><b>2.19</b> $ r6 : int 212 84 254 217 205 37 157 26 75 26 …</a></li>
<li class="chapter" data-level="2.20" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g6-int-254-116-255-228-225-36-186-37-108-26"><i class="fa fa-check"></i><b>2.20</b> $ g6 : int 254 116 255 228 225 36 186 37 108 26 …</a></li>
<li class="chapter" data-level="2.21" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b6-int-52-17-36-19-80-42-11-12-44-21"><i class="fa fa-check"></i><b>2.21</b> $ b6 : int 52 17 36 19 80 42 11 12 44 21 …</a></li>
<li class="chapter" data-level="2.22" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r7-int-212-217-211-221-235-44-26-34-13-52"><i class="fa fa-check"></i><b>2.22</b> $ r7 : int 212 217 211 221 235 44 26 34 13 52 …</a></li>
<li class="chapter" data-level="2.23" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g7-int-254-254-226-235-254-42-35-45-27-45"><i class="fa fa-check"></i><b>2.23</b> $ g7 : int 254 254 226 235 254 42 35 45 27 45 …</a></li>
<li class="chapter" data-level="2.24" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b7-int-11-26-70-20-60-44-10-19-25-27"><i class="fa fa-check"></i><b>2.24</b> $ b7 : int 11 26 70 20 60 44 10 19 25 27 …</a></li>
<li class="chapter" data-level="2.25" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r8-int-188-155-78-181-90-192-180-221-133-117"><i class="fa fa-check"></i><b>2.25</b> $ r8 : int 188 155 78 181 90 192 180 221 133 117 …</a></li>
<li class="chapter" data-level="2.26" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g8-int-229-203-73-183-110-131-211-249-163-109"><i class="fa fa-check"></i><b>2.26</b> $ g8 : int 229 203 73 183 110 131 211 249 163 109 …</a></li>
<li class="chapter" data-level="2.27" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b8-int-117-128-64-73-9-73-236-184-126-83"><i class="fa fa-check"></i><b>2.27</b> $ b8 : int 117 128 64 73 9 73 236 184 126 83 …</a></li>
<li class="chapter" data-level="2.28" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r9-int-170-213-220-237-216-123-129-226-83-110"><i class="fa fa-check"></i><b>2.28</b> $ r9 : int 170 213 220 237 216 123 129 226 83 110 …</a></li>
<li class="chapter" data-level="2.29" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g9-int-216-253-234-234-236-74-109-246-125-74"><i class="fa fa-check"></i><b>2.29</b> $ g9 : int 216 253 234 234 236 74 109 246 125 74 …</a></li>
<li class="chapter" data-level="2.30" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b9-int-120-51-59-44-66-22-73-59-19-12"><i class="fa fa-check"></i><b>2.30</b> $ b9 : int 120 51 59 44 66 22 73 59 19 12 …</a></li>
<li class="chapter" data-level="2.31" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r10-int-211-217-254-251-229-36-161-30-13-98"><i class="fa fa-check"></i><b>2.31</b> $ r10 : int 211 217 254 251 229 36 161 30 13 98 …</a></li>
<li class="chapter" data-level="2.32" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g10-int-254-255-255-254-255-34-190-40-27-70"><i class="fa fa-check"></i><b>2.32</b> $ g10 : int 254 255 255 254 255 34 190 40 27 70 …</a></li>
<li class="chapter" data-level="2.33" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b10-int-3-21-51-2-12-37-10-34-25-26"><i class="fa fa-check"></i><b>2.33</b> $ b10 : int 3 21 51 2 12 37 10 34 25 26 …</a></li>
<li class="chapter" data-level="2.34" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r11-int-212-217-253-235-235-44-161-34-9-20"><i class="fa fa-check"></i><b>2.34</b> $ r11 : int 212 217 253 235 235 44 161 34 9 20 …</a></li>
<li class="chapter" data-level="2.35" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g11-int-254-255-255-243-254-42-190-44-23-21"><i class="fa fa-check"></i><b>2.35</b> $ g11 : int 254 255 255 243 254 42 190 44 23 21 …</a></li>
<li class="chapter" data-level="2.36" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b11-int-19-21-44-12-60-44-6-35-18-20"><i class="fa fa-check"></i><b>2.36</b> $ b11 : int 19 21 44 12 60 44 6 35 18 20 …</a></li>
<li class="chapter" data-level="2.37" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r12-int-172-158-66-19-163-197-187-241-85-113"><i class="fa fa-check"></i><b>2.37</b> $ r12 : int 172 158 66 19 163 197 187 241 85 113 …</a></li>
<li class="chapter" data-level="2.38" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g12-int-235-225-68-27-168-114-215-255-128-76"><i class="fa fa-check"></i><b>2.38</b> $ g12 : int 235 225 68 27 168 114 215 255 128 76 …</a></li>
<li class="chapter" data-level="2.39" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b12-int-244-237-68-29-152-21-236-54-21-14"><i class="fa fa-check"></i><b>2.39</b> $ b12 : int 244 237 68 29 152 21 236 54 21 14 …</a></li>
<li class="chapter" data-level="2.40" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r13-int-172-164-69-20-124-171-141-205-83-106"><i class="fa fa-check"></i><b>2.40</b> $ r13 : int 172 164 69 20 124 171 141 205 83 106 …</a></li>
<li class="chapter" data-level="2.41" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g13-int-235-227-65-29-117-102-142-229-125-69"><i class="fa fa-check"></i><b>2.41</b> $ g13 : int 235 227 65 29 117 102 142 229 125 69 …</a></li>
<li class="chapter" data-level="2.42" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b13-int-244-237-59-34-91-26-140-46-19-9"><i class="fa fa-check"></i><b>2.42</b> $ b13 : int 244 237 59 34 91 26 140 46 19 9 …</a></li>
<li class="chapter" data-level="2.43" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r14-int-172-182-76-64-188-197-189-226-85-102"><i class="fa fa-check"></i><b>2.43</b> $ r14 : int 172 182 76 64 188 197 189 226 85 102 …</a></li>
<li class="chapter" data-level="2.44" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g14-int-228-228-84-61-205-114-171-246-128-67"><i class="fa fa-check"></i><b>2.44</b> $ g14 : int 228 228 84 61 205 114 171 246 128 67 …</a></li>
<li class="chapter" data-level="2.45" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b14-int-235-143-22-4-78-21-140-59-21-6"><i class="fa fa-check"></i><b>2.45</b> $ b14 : int 235 143 22 4 78 21 140 59 21 6 …</a></li>
<li class="chapter" data-level="2.46" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r15-int-177-171-82-211-125-123-214-235-85-106"><i class="fa fa-check"></i><b>2.46</b> $ r15 : int 177 171 82 211 125 123 214 235 85 106 …</a></li>
<li class="chapter" data-level="2.47" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g15-int-235-228-93-222-147-74-221-252-128-69"><i class="fa fa-check"></i><b>2.47</b> $ g15 : int 235 228 93 222 147 74 221 252 128 69 …</a></li>
<li class="chapter" data-level="2.48" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b15-int-244-196-17-78-20-22-201-67-21-9"><i class="fa fa-check"></i><b>2.48</b> $ b15 : int 244 196 17 78 20 22 201 67 21 9 …</a></li>
<li class="chapter" data-level="2.49" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#r16-int-22-164-58-19-160-180-188-237-83-43"><i class="fa fa-check"></i><b>2.49</b> $ r16 : int 22 164 58 19 160 180 188 237 83 43 …</a></li>
<li class="chapter" data-level="2.50" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#g16-int-52-227-60-27-183-107-211-254-125-29"><i class="fa fa-check"></i><b>2.50</b> $ g16 : int 52 227 60 27 183 107 211 254 125 29 …</a></li>
<li class="chapter" data-level="2.51" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#b16-int-53-237-60-29-187-26-227-53-19-11"><i class="fa fa-check"></i><b>2.51</b> $ b16 : int 53 237 60 29 187 26 227 53 19 11 …</a></li>
<li class="chapter" data-level="2.52" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section"><i class="fa fa-check"></i><b>2.52</b> </a></li>
<li class="chapter" data-level="2.53" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#pedestrian-speed-stop"><i class="fa fa-check"></i><b>2.53</b> pedestrian speed stop</a></li>
<li class="chapter" data-level="2.54" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-5"><i class="fa fa-check"></i><b>2.54</b> 46 49 50</a></li>
<li class="chapter" data-level="2.55" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#sign_type-r10"><i class="fa fa-check"></i><b>2.55</b> sign_type r10</a></li>
<li class="chapter" data-level="2.56" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#pedestrian-113.71739"><i class="fa fa-check"></i><b>2.56</b> 1 pedestrian 113.71739</a></li>
<li class="chapter" data-level="2.57" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#speed-80.63265"><i class="fa fa-check"></i><b>2.57</b> 2 speed 80.63265</a></li>
<li class="chapter" data-level="2.58" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#stop-131.98000"><i class="fa fa-check"></i><b>2.58</b> 3 stop 131.98000</a></li>
<li class="chapter" data-level="2.59" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.59</b> Naive Bayes</a></li>
<li class="chapter" data-level="2.60" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-6"><i class="fa fa-check"></i><b>2.60</b> [1] 0.6</a></li>
<li class="chapter" data-level="2.61" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#warning-package-naivebayes-was-built-under-r-version-3.4.3"><i class="fa fa-check"></i><b>2.61</b> Warning: package ‘naivebayes’ was built under R version 3.4.3</a></li>
<li class="chapter" data-level="2.62" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#office"><i class="fa fa-check"></i><b>2.62</b> [1] office</a></li>
<li class="chapter" data-level="2.63" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#levels-appointment-campus-home-office"><i class="fa fa-check"></i><b>2.63</b> Levels: appointment campus home office</a></li>
<li class="chapter" data-level="2.64" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#home"><i class="fa fa-check"></i><b>2.64</b> [1] home</a></li>
<li class="chapter" data-level="2.65" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#levels-appointment-campus-home-office-1"><i class="fa fa-check"></i><b>2.65</b> Levels: appointment campus home office</a></li>
<li class="chapter" data-level="2.66" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-7"><i class="fa fa-check"></i><b>2.66</b> </a></li>
<li class="chapter" data-level="2.67" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#daytype-appointment-campus-home-office"><i class="fa fa-check"></i><b>2.67</b> daytype appointment campus home office</a></li>
<li class="chapter" data-level="2.68" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#weekday-1.0000000-1.0000000-0.3658537-1.0000000"><i class="fa fa-check"></i><b>2.68</b> weekday 1.0000000 1.0000000 0.3658537 1.0000000</a></li>
<li class="chapter" data-level="2.69" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#weekend-0.0000000-0.0000000-0.6341463-0.0000000"><i class="fa fa-check"></i><b>2.69</b> weekend 0.0000000 0.0000000 0.6341463 0.0000000</a></li>
<li class="chapter" data-level="2.70" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-8"><i class="fa fa-check"></i><b>2.70</b> </a></li>
<li class="chapter" data-level="2.71" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office"><i class="fa fa-check"></i><b>2.71</b> appointment campus home office</a></li>
<li class="chapter" data-level="2.72" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-9"><i class="fa fa-check"></i><b>2.72</b> 0.01098901 0.10989011 0.45054945 0.42857143</a></li>
<li class="chapter" data-level="2.73" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-1"><i class="fa fa-check"></i><b>2.73</b> appointment campus home office</a></li>
<li class="chapter" data-level="2.74" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-10"><i class="fa fa-check"></i><b>2.74</b> [1,] 0.01538462 0.1538462 0.2307692 0.6</a></li>
<li class="chapter" data-level="2.75" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-2"><i class="fa fa-check"></i><b>2.75</b> appointment campus home office</a></li>
<li class="chapter" data-level="2.76" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-11"><i class="fa fa-check"></i><b>2.76</b> [1,] 0 0 1 0</a><ul>
<li class="chapter" data-level="2.76.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#putting-the-naivety-in-naive-bayes"><i class="fa fa-check"></i><b>2.76.1</b> Putting the Naivety in Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="2.77" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#month-day-weekday-daytype-hour-hourtype-location"><i class="fa fa-check"></i><b>2.77</b> month day weekday daytype hour hourtype location</a></li>
<li class="chapter" data-level="2.78" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-0-night-home"><i class="fa fa-check"></i><b>2.78</b> 1 1 4 wednesday weekday 0 night home</a></li>
<li class="chapter" data-level="2.79" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-1-night-home"><i class="fa fa-check"></i><b>2.79</b> 2 1 4 wednesday weekday 1 night home</a></li>
<li class="chapter" data-level="2.80" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-2-night-home"><i class="fa fa-check"></i><b>2.80</b> 3 1 4 wednesday weekday 2 night home</a></li>
<li class="chapter" data-level="2.81" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-3-night-home"><i class="fa fa-check"></i><b>2.81</b> 4 1 4 wednesday weekday 3 night home</a></li>
<li class="chapter" data-level="2.82" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-4-night-home"><i class="fa fa-check"></i><b>2.82</b> 5 1 4 wednesday weekday 4 night home</a></li>
<li class="chapter" data-level="2.83" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-5-night-home"><i class="fa fa-check"></i><b>2.83</b> 6 1 4 wednesday weekday 5 night home</a></li>
<li class="chapter" data-level="2.84" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-6-morning-home"><i class="fa fa-check"></i><b>2.84</b> 7 1 4 wednesday weekday 6 morning home</a></li>
<li class="chapter" data-level="2.85" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-7-morning-home"><i class="fa fa-check"></i><b>2.85</b> 8 1 4 wednesday weekday 7 morning home</a></li>
<li class="chapter" data-level="2.86" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-8-morning-home"><i class="fa fa-check"></i><b>2.86</b> 9 1 4 wednesday weekday 8 morning home</a></li>
<li class="chapter" data-level="2.87" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wednesday-weekday-9-morning-office"><i class="fa fa-check"></i><b>2.87</b> 10 1 4 wednesday weekday 9 morning office</a></li>
<li class="chapter" data-level="2.88" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#office-1"><i class="fa fa-check"></i><b>2.88</b> [1] office</a></li>
<li class="chapter" data-level="2.89" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#levels-appointment-campus-home-office-restaurant-store-theater"><i class="fa fa-check"></i><b>2.89</b> Levels: appointment campus home office restaurant store theater</a></li>
<li class="chapter" data-level="2.90" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#home-1"><i class="fa fa-check"></i><b>2.90</b> [1] home</a></li>
<li class="chapter" data-level="2.91" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#levels-appointment-campus-home-office-restaurant-store-theater-1"><i class="fa fa-check"></i><b>2.91</b> Levels: appointment campus home office restaurant store theater</a></li>
<li class="chapter" data-level="2.92" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-restaurant-store"><i class="fa fa-check"></i><b>2.92</b> appointment campus home office restaurant store</a></li>
<li class="chapter" data-level="2.93" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-12"><i class="fa fa-check"></i><b>2.93</b> [1,] 0.004300045 0.08385089 0.2482618 0.5848062 0.07304769 0.005733394</a></li>
<li class="chapter" data-level="2.94" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#theater"><i class="fa fa-check"></i><b>2.94</b> theater</a></li>
<li class="chapter" data-level="2.95" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-13"><i class="fa fa-check"></i><b>2.95</b> [1,] 0</a></li>
<li class="chapter" data-level="2.96" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-restaurant-store-1"><i class="fa fa-check"></i><b>2.96</b> appointment campus home office restaurant store</a></li>
<li class="chapter" data-level="2.97" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-14"><i class="fa fa-check"></i><b>2.97</b> [1,] 0.004283788 0.004283788 0.7903588 0.05997303 0.04400138 0.09709919</a></li>
<li class="chapter" data-level="2.98" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#theater-1"><i class="fa fa-check"></i><b>2.98</b> theater</a></li>
<li class="chapter" data-level="2.99" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-15"><i class="fa fa-check"></i><b>2.99</b> [1,] 0</a></li>
<li class="chapter" data-level="2.100" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-restaurant-store-theater"><i class="fa fa-check"></i><b>2.100</b> appointment campus home office restaurant store theater</a></li>
<li class="chapter" data-level="2.101" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-16"><i class="fa fa-check"></i><b>2.101</b> [1,] 0.02472535 0 0.8472217 0 0.1115693 0.01648357 0</a></li>
<li class="chapter" data-level="2.102" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#appointment-campus-home-office-restaurant-store-2"><i class="fa fa-check"></i><b>2.102</b> appointment campus home office restaurant store</a></li>
<li class="chapter" data-level="2.103" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-17"><i class="fa fa-check"></i><b>2.103</b> [1,] 0.01107985 0.005752078 0.8527053 0.008023444 0.1032598 0.01608175</a></li>
<li class="chapter" data-level="2.104" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#theater-2"><i class="fa fa-check"></i><b>2.104</b> theater</a></li>
<li class="chapter" data-level="2.105" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-18"><i class="fa fa-check"></i><b>2.105</b> [1,] 0.003097769</a><ul>
<li class="chapter" data-level="2.105.1" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#applying-naive-bayes-nb-to-other-problems"><i class="fa fa-check"></i><b>2.105.1</b> Applying Naive Bayes (NB) to other problems</a></li>
</ul></li>
<li class="chapter" data-level="2.106" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#logistic-regression---binary-predictions-with-regression"><i class="fa fa-check"></i><b>2.106</b> Logistic regression - binary predictions with regression</a></li>
<li class="chapter" data-level="2.107" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#data.frame-93462-obs.-of-13-variables"><i class="fa fa-check"></i><b>2.107</b> ‘data.frame’: 93462 obs. of 13 variables:</a></li>
<li class="chapter" data-level="2.108" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#donated-int-0-0-0-0-0-0-0-0-0-0"><i class="fa fa-check"></i><b>2.108</b> $ donated : int 0 0 0 0 0 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.109" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#veteran-int-0-0-0-0-0-0-0-0-0-0"><i class="fa fa-check"></i><b>2.109</b> $ veteran : int 0 0 0 0 0 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.110" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#bad_address-int-0-0-0-0-0-0-0-0-0-0"><i class="fa fa-check"></i><b>2.110</b> $ bad_address : int 0 0 0 0 0 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.111" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#age-int-60-46-na-70-78-na-38-na-na-65"><i class="fa fa-check"></i><b>2.111</b> $ age : int 60 46 NA 70 78 NA 38 NA NA 65 …</a></li>
<li class="chapter" data-level="2.112" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#has_children-int-0-1-0-0-1-0-1-0-0-0"><i class="fa fa-check"></i><b>2.112</b> $ has_children : int 0 1 0 0 1 0 1 0 0 0 …</a></li>
<li class="chapter" data-level="2.113" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#wealth_rating-int-0-3-1-2-1-0-2-3-1-0"><i class="fa fa-check"></i><b>2.113</b> $ wealth_rating : int 0 3 1 2 1 0 2 3 1 0 …</a></li>
<li class="chapter" data-level="2.114" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#interest_veterans-int-0-0-0-0-0-0-0-0-0-0"><i class="fa fa-check"></i><b>2.114</b> $ interest_veterans: int 0 0 0 0 0 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.115" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#interest_religion-int-0-0-0-0-1-0-0-0-0-0"><i class="fa fa-check"></i><b>2.115</b> $ interest_religion: int 0 0 0 0 1 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.116" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#pet_owner-int-0-0-0-0-0-0-1-0-0-0"><i class="fa fa-check"></i><b>2.116</b> $ pet_owner : int 0 0 0 0 0 0 1 0 0 0 …</a></li>
<li class="chapter" data-level="2.117" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#catalog_shopper-int-0-0-0-0-1-0-0-0-0-0"><i class="fa fa-check"></i><b>2.117</b> $ catalog_shopper : int 0 0 0 0 1 0 0 0 0 0 …</a></li>
<li class="chapter" data-level="2.118" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#recency-factor-w-2-levels-currentlapsed-1-1-1-1-1-1-1-1-1-1"><i class="fa fa-check"></i><b>2.118</b> $ recency : Factor w/ 2 levels “CURRENT”,“LAPSED”: 1 1 1 1 1 1 1 1 1 1 …</a></li>
<li class="chapter" data-level="2.119" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#frequency-factor-w-2-levels-frequentinfrequent-1-1-1-1-1-2-2-1-2-2"><i class="fa fa-check"></i><b>2.119</b> $ frequency : Factor w/ 2 levels “FREQUENT”,“INFREQUENT”: 1 1 1 1 1 2 2 1 2 2 …</a></li>
<li class="chapter" data-level="2.120" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#money-factor-w-2-levels-highmedium-2-1-2-2-2-2-2-2-2-2"><i class="fa fa-check"></i><b>2.120</b> $ money : Factor w/ 2 levels “HIGH”,“MEDIUM”: 2 1 2 2 2 2 2 2 2 2 …</a></li>
<li class="chapter" data-level="2.121" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-19"><i class="fa fa-check"></i><b>2.121</b> </a></li>
<li class="chapter" data-level="2.122" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-20"><i class="fa fa-check"></i><b>2.122</b> 0 1</a></li>
<li class="chapter" data-level="2.123" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-21"><i class="fa fa-check"></i><b>2.123</b> 88751 4711</a></li>
<li class="chapter" data-level="2.124" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-22"><i class="fa fa-check"></i><b>2.124</b> </a></li>
<li class="chapter" data-level="2.125" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#call"><i class="fa fa-check"></i><b>2.125</b> Call:</a></li>
<li class="chapter" data-level="2.126" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#glmformula-donated-bad_address-interest_religion-interest_veterans"><i class="fa fa-check"></i><b>2.126</b> glm(formula = donated ~ bad_address + interest_religion + interest_veterans,</a></li>
<li class="chapter" data-level="2.127" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#family-binomial-data-donors"><i class="fa fa-check"></i><b>2.127</b> family = “binomial”, data = donors)</a></li>
<li class="chapter" data-level="2.128" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-23"><i class="fa fa-check"></i><b>2.128</b> </a></li>
<li class="chapter" data-level="2.129" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#deviance-residuals"><i class="fa fa-check"></i><b>2.129</b> Deviance Residuals:</a></li>
<li class="chapter" data-level="2.130" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#min-1q-median-3q-max"><i class="fa fa-check"></i><b>2.130</b> Min 1Q Median 3Q Max</a></li>
<li class="chapter" data-level="2.131" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-24"><i class="fa fa-check"></i><b>2.131</b> -0.3480 -0.3192 -0.3192 -0.3192 2.5678</a></li>
<li class="chapter" data-level="2.132" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-25"><i class="fa fa-check"></i><b>2.132</b> </a></li>
<li class="chapter" data-level="2.133" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#coefficients"><i class="fa fa-check"></i><b>2.133</b> Coefficients:</a></li>
<li class="chapter" data-level="2.134" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#estimate-std.-error-z-value-prz"><i class="fa fa-check"></i><b>2.134</b> Estimate Std. Error z value Pr(&gt;|z|)</a></li>
<li><a href="supervised-learning-in-r-classification.html#intercept--2.95139-0.01652--178.664-2e-16-bad_address--0.30780-0.14348--2.145-0.0319-interest_religion-0.06724-0.05069-1.327-0.1847-interest_veterans-0.11009-0.04676-2.354-0.0186-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-binomial-family-taken-to-be-1-null-deviance-37330-on-93461-degrees-of-freedom-residual-deviance-37316-on-93458-degrees-of-freedom-aic-37324-number-of-fisher-scoring-iterations-5-1-0.05040551-1-0.794815-warning-package-proc-was-built-under-r-version-3.4.4-type-citationproc-for-a-citation.-attaching-package-proc-the-following-objects-are-masked-from-packagestats-cov-smooth-var-area-under-the-curve-0.5102-dummy-variables-missing-data-and-interactions-call-glmformula-donated-wealth_rating-family-binomial-data-donors-deviance-residuals-min-1q-median-3q-max--0.3320--0.3243--0.3175--0.3175-2.4582-coefficients-estimate-std.-error-z-value-prz-intercept--2.91894-0.03614--80.772-2e-16-wealth_ratingunknown--0.04373-0.04243--1.031-0.303-wealth_ratinglow--0.05245-0.05332--0.984-0.325-wealth_ratinghigh-0.04804-0.04768-1.008-0.314-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-binomial-family-taken-to-be-1-null-deviance-37330-on-93461-degrees-of-freedom-residual-deviance-37323-on-93458-degrees-of-freedom-aic-37331-number-of-fisher-scoring-iterations-5-min.-1st-qu.-median-mean-3rd-qu.-max.-nas-1.00-48.00-62.00-61.65-75.00-98.00-22546-call-glmformula-donated-money-recency-frequency-family-binomial"><span class="toc-section-number">2.135</span> (Intercept) -2.95139 0.01652 -178.664 &lt;2e-16 <strong><em> ## bad_address -0.30780 0.14348 -2.145 0.0319 </em><br />
## interest_religion 0.06724 0.05069 1.327 0.1847<br />
## interest_veterans 0.11009 0.04676 2.354 0.0186 *<br />
## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 37316 on 93458 degrees of freedom ## AIC: 37324 ## ## Number of Fisher Scoring iterations: 5 ## [1] 0.05040551 ## [1] 0.794815 ## Warning: package ‘pROC’ was built under R version 3.4.4 ## Type ‘citation(“pROC”)’ for a citation. ## ## Attaching package: ‘pROC’ ## The following objects are masked from ‘package:stats’: ## ## cov, smooth, var ## Area under the curve: 0.5102 ### Dummy variables, missing data, and interactions ## ## Call: ## glm(formula = donated ~ wealth_rating, family = “binomial”, data = donors) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max<br />
## -0.3320 -0.3243 -0.3175 -0.3175 2.4582<br />
## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|)<br />
## (Intercept) -2.91894 0.03614 -80.772 &lt;2e-16 </strong><em> ## wealth_ratingUnknown -0.04373 0.04243 -1.031 0.303<br />
## wealth_ratingLow -0.05245 0.05332 -0.984 0.325<br />
## wealth_ratingHigh 0.04804 0.04768 1.008 0.314<br />
## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 37323 on 93458 degrees of freedom ## AIC: 37331 ## ## Number of Fisher Scoring iterations: 5 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA’s ## 1.00 48.00 62.00 61.65 75.00 98.00 22546 ## ## Call: ## glm(formula = donated ~ money + recency </em> frequency, family = “binomial”,</a></li>
<li class="chapter" data-level="2.136" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#data-donors"><i class="fa fa-check"></i><b>2.136</b> data = donors)</a></li>
<li class="chapter" data-level="2.137" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-26"><i class="fa fa-check"></i><b>2.137</b> </a></li>
<li class="chapter" data-level="2.138" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#deviance-residuals-1"><i class="fa fa-check"></i><b>2.138</b> Deviance Residuals:</a></li>
<li class="chapter" data-level="2.139" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#min-1q-median-3q-max-1"><i class="fa fa-check"></i><b>2.139</b> Min 1Q Median 3Q Max</a></li>
<li class="chapter" data-level="2.140" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-27"><i class="fa fa-check"></i><b>2.140</b> -0.3696 -0.3696 -0.2895 -0.2895 2.7924</a></li>
<li class="chapter" data-level="2.141" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#section-28"><i class="fa fa-check"></i><b>2.141</b> </a></li>
<li class="chapter" data-level="2.142" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#coefficients-1"><i class="fa fa-check"></i><b>2.142</b> Coefficients:</a></li>
<li class="chapter" data-level="2.143" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#estimate-std.-error-z-value-prz-1"><i class="fa fa-check"></i><b>2.143</b> Estimate Std. Error z value Pr(&gt;|z|)</a></li>
<li class="chapter" data-level="2.144" data-path="supervised-learning-in-r-classification.html"><a href="supervised-learning-in-r-classification.html#intercept--3.01142-0.04279--70.375-2e-16-moneymedium-0.36186-0.04300-8.415-2e-16"><i class="fa fa-check"></i><b>2.144</b> (Intercept) -3.01142 0.04279 -70.375 &lt;2e-16 <strong><em> ## moneyMEDIUM 0.36186 0.04300 8.415 &lt;2e-16 </em></strong></a></li>
<li><a href="supervised-learning-in-r-classification.html#recencylapsed--0.86677-0.41434--2.092-0.0364-frequencyinfrequent--0.50148-0.03107--16.143-2e-16-recencylapsedfrequencyinfrequent-1.01787-0.51713-1.968-0.0490-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-binomial-family-taken-to-be-1-null-deviance-37330-on-93461-degrees-of-freedom-residual-deviance-36938-on-93457-degrees-of-freedom-aic-36948-number-of-fisher-scoring-iterations-6-area-under-the-curve-0.5785-automatic-feature-selection-start-aic37332.13-donated-1-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-frequency-1-28502-37122-money-1-28621-37241-has_children-1-28705-37326-age-1-28707-37328-imputed_age-1-28707-37328-wealth_rating-3-28704-37328-interest_veterans-1-28709-37330-donation_prob-1-28710-37330-donation_pred-1-28710-37330-catalog_shopper-1-28710-37330-pet_owner-1-28711-37331-none-28714-37332-interest_religion-1-28712-37333-recency-1-28713-37333-bad_address-1-28714-37334-veteran-1-28714-37334-step-aic37024.77-donated-frequency-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-money-1-28441-36966-wealth_rating-3-28490-37019-has_children-1-28494-37019-donation_prob-1-28498-37023-interest_veterans-1-28498-37023-catalog_shopper-1-28499-37024-donation_pred-1-28499-37024-age-1-28499-37024-imputed_age-1-28499-37024-pet_owner-1-28499-37024-28502-37025-interest_religion-1-28501-37026-recency-1-28501-37026-bad_address-1-28502-37026-veteran-1-28502-37027-step-aic36949.71-donated-frequency-money-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-wealth_rating-3-28427-36942-has_children-1-28432-36943-interest_veterans-1-28438-36948-donation_prob-1-28438-36949-catalog_shopper-1-28438-36949-donation_pred-1-28439-36949-age-1-28439-36949-imputed_age-1-28439-36949-pet_owner-1-28439-36949-none-28441-36950-interest_religion-1-28440-36951-recency-1-28441-36951-bad_address-1-28441-36951-veteran-1-28441-36952-step-aic36945.48-donated-frequency-money-wealth_rating-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-has_children-1-28416-36937-age-1-28424-36944-imputed_age-1-28424-36944-interest_veterans-1-28424-36945-donation_prob-1-28424-36945-catalog_shopper-1-28425-36945-donation_pred-1-28425-36945-28427-36945-pet_owner-1-28425-36946-interest_religion-1-28426-36947-recency-1-28427-36947-bad_address-1-28427-36947-veteran-1-28427-36947-step-aic36938.4-donated-frequency-money-wealth_rating-has_children-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-pet_owner-1-28413-36937-donation_prob-1-28413-36937-catalog_shopper-1-28413-36937-interest_veterans-1-28413-36937-donation_pred-1-28414-36938-none-28416-36938-interest_religion-1-28415-36939-age-1-28416-36940-imputed_age-1-28416-36940-recency-1-28416-36940-bad_address-1-28416-36940-veteran-1-28416-36940-step-aic36932.25-donated-frequency-money-wealth_rating-has_children-pet_owner-warning-in-add1.glmfit-scopeadd-scale-scale-trace-trace-k-k-using-the-7091693462-rows-from-a-combined-fit-df-deviance-aic-28413-36932-donation_prob-1-28411-36932-interest_veterans-1-28411-36932-catalog_shopper-1-28412-36933-donation_pred-1-28412-36933-age-1-28412-36933-imputed_age-1-28412-36933-recency-1-28413-36934-interest_religion-1-28413-36934-bad_address-1-28413-36934-veteran-1-28413-36934-area-under-the-curve-0.5849-classification-trees-n-11312-node-split-n-loss-yval-yprob-denotes-terminal-node-1-root-11312-5654-repaid-0.4998232-0.5001768-2-credit_scoreaveragelow-9490-4437-default-0.5324552-0.4675448-4-credit_scorelow-1667-631-default-0.6214757-0.3785243-5-credit_scoreaverage-7823-3806-default-0.5134859-0.4865141-10-loan_amounthigh-2472-1079-default-0.5635113-0.4364887-11-loan_amountlowmedium-5351-2624-repaid-0.4903756-0.5096244-22-loan_amountlow-1810-874-default-0.5171271-0.4828729-23-loan_amountmedium-3541-1688-repaid-0.4767015-0.5232985-3-credit_scorehigh-1822-601-repaid-0.3298573-0.6701427-warning-package-rpart.plot-was-built-under-r-version-3.4.4-default-repaid-default-822-644-repaid-615-747-1-0.5548091-tending-to-classification-trees-1-0.5880481-1-0.5933522-1-0.5548091-1-0.5859264-randomforest-4.6-12-type-rfnews-to-see-new-featureschangesbug-fixes.-1-0.5880481-1-0.9048798"><span class="toc-section-number">2.145</span> recencyLAPSED -0.86677 0.41434 -2.092 0.0364 *<br />
## frequencyINFREQUENT -0.50148 0.03107 -16.143 &lt;2e-16 **<em> ## recencyLAPSED:frequencyINFREQUENT 1.01787 0.51713 1.968 0.0490 </em><br />
## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 36938 on 93457 degrees of freedom ## AIC: 36948 ## ## Number of Fisher Scoring iterations: 6 ## Area under the curve: 0.5785 ### Automatic feature selection ## Start: AIC=37332.13 ## donated ~ 1 ## Warning in add1.glm(fit, scope<span class="math inline">\(add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + frequency 1 28502 37122 ## + money 1 28621 37241 ## + has_children 1 28705 37326 ## + age 1 28707 37328 ## + imputed_age 1 28707 37328 ## + wealth_rating 3 28704 37328 ## + interest_veterans 1 28709 37330 ## + donation_prob 1 28710 37330 ## + donation_pred 1 28710 37330 ## + catalog_shopper 1 28710 37330 ## + pet_owner 1 28711 37331 ## &lt;none&gt; 28714 37332 ## + interest_religion 1 28712 37333 ## + recency 1 28713 37333 ## + bad_address 1 28714 37334 ## + veteran 1 28714 37334 ## ## Step: AIC=37024.77 ## donated ~ frequency ## Warning in add1.glm(fit, scope\)</span>add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + money 1 28441 36966 ## + wealth_rating 3 28490 37019 ## + has_children 1 28494 37019 ## + donation_prob 1 28498 37023 ## + interest_veterans 1 28498 37023 ## + catalog_shopper 1 28499 37024 ## + donation_pred 1 28499 37024 ## + age 1 28499 37024 ## + imputed_age 1 28499 37024 ## + pet_owner 1 28499 37024 ## <none> 28502 37025 ## + interest_religion 1 28501 37026 ## + recency 1 28501 37026 ## + bad_address 1 28502 37026 ## + veteran 1 28502 37027 ## ## Step: AIC=36949.71 ## donated ~ frequency + money ## Warning in add1.glm(fit, scope<span class="math inline">\(add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + wealth_rating 3 28427 36942 ## + has_children 1 28432 36943 ## + interest_veterans 1 28438 36948 ## + donation_prob 1 28438 36949 ## + catalog_shopper 1 28438 36949 ## + donation_pred 1 28439 36949 ## + age 1 28439 36949 ## + imputed_age 1 28439 36949 ## + pet_owner 1 28439 36949 ## &lt;none&gt; 28441 36950 ## + interest_religion 1 28440 36951 ## + recency 1 28441 36951 ## + bad_address 1 28441 36951 ## + veteran 1 28441 36952 ## ## Step: AIC=36945.48 ## donated ~ frequency + money + wealth_rating ## Warning in add1.glm(fit, scope\)</span>add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + has_children 1 28416 36937 ## + age 1 28424 36944 ## + imputed_age 1 28424 36944 ## + interest_veterans 1 28424 36945 ## + donation_prob 1 28424 36945 ## + catalog_shopper 1 28425 36945 ## + donation_pred 1 28425 36945 ## <none> 28427 36945 ## + pet_owner 1 28425 36946 ## + interest_religion 1 28426 36947 ## + recency 1 28427 36947 ## + bad_address 1 28427 36947 ## + veteran 1 28427 36947 ## ## Step: AIC=36938.4 ## donated ~ frequency + money + wealth_rating + has_children ## Warning in add1.glm(fit, scope<span class="math inline">\(add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + pet_owner 1 28413 36937 ## + donation_prob 1 28413 36937 ## + catalog_shopper 1 28413 36937 ## + interest_veterans 1 28413 36937 ## + donation_pred 1 28414 36938 ## &lt;none&gt; 28416 36938 ## + interest_religion 1 28415 36939 ## + age 1 28416 36940 ## + imputed_age 1 28416 36940 ## + recency 1 28416 36940 ## + bad_address 1 28416 36940 ## + veteran 1 28416 36940 ## ## Step: AIC=36932.25 ## donated ~ frequency + money + wealth_rating + has_children + ## pet_owner ## Warning in add1.glm(fit, scope\)</span>add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## <none> 28413 36932 ## + donation_prob 1 28411 36932 ## + interest_veterans 1 28411 36932 ## + catalog_shopper 1 28412 36933 ## + donation_pred 1 28412 36933 ## + age 1 28412 36933 ## + imputed_age 1 28412 36933 ## + recency 1 28413 36934 ## + interest_religion 1 28413 36934 ## + bad_address 1 28413 36934 ## + veteran 1 28413 36934 ## Area under the curve: 0.5849 ## Classification Trees ## n= 11312 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 11312 5654 repaid (0.4998232 0.5001768)<br />
## 2) credit_score=AVERAGE,LOW 9490 4437 default (0.5324552 0.4675448)<br />
## 4) credit_score=LOW 1667 631 default (0.6214757 0.3785243) <em> ## 5) credit_score=AVERAGE 7823 3806 default (0.5134859 0.4865141)<br />
## 10) loan_amount=HIGH 2472 1079 default (0.5635113 0.4364887) </em> ## 11) loan_amount=LOW,MEDIUM 5351 2624 repaid (0.4903756 0.5096244)<br />
## 22) loan_amount=LOW 1810 874 default (0.5171271 0.4828729) <em> ## 23) loan_amount=MEDIUM 3541 1688 repaid (0.4767015 0.5232985) </em> ## 3) credit_score=HIGH 1822 601 repaid (0.3298573 0.6701427) * ## Warning: package ‘rpart.plot’ was built under R version 3.4.4 ##<br />
## default repaid ## default 822 644 ## repaid 615 747 ## [1] 0.5548091 ### Tending to classification trees ## [1] 0.5880481 ## [1] 0.5933522 ## [1] 0.5548091 ## [1] 0.5859264 ## randomForest 4.6-12 ## Type rfNews() to see new features/changes/bug fixes. ## [1] 0.5880481 ## [1] 0.9048798</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html"><i class="fa fa-check"></i><b>3</b> Supervised Learning In R Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#what-is-regression"><i class="fa fa-check"></i><b>3.1</b> What is Regression?</a></li>
<li class="chapter" data-level="3.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#male_unemployment-female_unemployment"><i class="fa fa-check"></i><b>3.2</b> male_unemployment female_unemployment</a></li>
<li class="chapter" data-level="3.3" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-2.900-min.-4.000"><i class="fa fa-check"></i><b>3.3</b> Min. :2.900 Min. :4.000</a></li>
<li class="chapter" data-level="3.4" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.4.900-1st-qu.4.400"><i class="fa fa-check"></i><b>3.4</b> 1st Qu.:4.900 1st Qu.:4.400</a></li>
<li class="chapter" data-level="3.5" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-6.000-median-5.200"><i class="fa fa-check"></i><b>3.5</b> Median :6.000 Median :5.200</a></li>
<li class="chapter" data-level="3.6" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-5.954-mean-5.569"><i class="fa fa-check"></i><b>3.6</b> Mean :5.954 Mean :5.569</a></li>
<li class="chapter" data-level="3.7" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.6.700-3rd-qu.6.100"><i class="fa fa-check"></i><b>3.7</b> 3rd Qu.:6.700 3rd Qu.:6.100</a></li>
<li class="chapter" data-level="3.8" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-9.800-max.-7.900"><i class="fa fa-check"></i><b>3.8</b> Max. :9.800 Max. :7.900</a></li>
<li class="chapter" data-level="3.9" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#female_unemployment-male_unemployment"><i class="fa fa-check"></i><b>3.9</b> female_unemployment ~ male_unemployment</a></li>
<li class="chapter" data-level="3.10" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-29"><i class="fa fa-check"></i><b>3.10</b> </a></li>
<li class="chapter" data-level="3.11" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#call-1"><i class="fa fa-check"></i><b>3.11</b> Call:</a></li>
<li class="chapter" data-level="3.12" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#lmformula-fmla-data-unemployment"><i class="fa fa-check"></i><b>3.12</b> lm(formula = fmla, data = unemployment)</a></li>
<li class="chapter" data-level="3.13" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-30"><i class="fa fa-check"></i><b>3.13</b> </a></li>
<li class="chapter" data-level="3.14" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#coefficients-2"><i class="fa fa-check"></i><b>3.14</b> Coefficients:</a></li>
<li class="chapter" data-level="3.15" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intercept-male_unemployment"><i class="fa fa-check"></i><b>3.15</b> (Intercept) male_unemployment</a></li>
<li class="chapter" data-level="3.16" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-31"><i class="fa fa-check"></i><b>3.16</b> 1.4341 0.6945</a></li>
<li class="chapter" data-level="3.17" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-32"><i class="fa fa-check"></i><b>3.17</b> </a></li>
<li class="chapter" data-level="3.18" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#call-2"><i class="fa fa-check"></i><b>3.18</b> Call:</a></li>
<li class="chapter" data-level="3.19" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#lmformula-fmla-data-unemployment-1"><i class="fa fa-check"></i><b>3.19</b> lm(formula = fmla, data = unemployment)</a></li>
<li class="chapter" data-level="3.20" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-33"><i class="fa fa-check"></i><b>3.20</b> </a></li>
<li class="chapter" data-level="3.21" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#coefficients-3"><i class="fa fa-check"></i><b>3.21</b> Coefficients:</a></li>
<li class="chapter" data-level="3.22" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intercept-male_unemployment-1"><i class="fa fa-check"></i><b>3.22</b> (Intercept) male_unemployment</a></li>
<li class="chapter" data-level="3.23" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-34"><i class="fa fa-check"></i><b>3.23</b> 1.4341 0.6945</a></li>
<li class="chapter" data-level="3.24" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-35"><i class="fa fa-check"></i><b>3.24</b> </a></li>
<li class="chapter" data-level="3.25" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#call-3"><i class="fa fa-check"></i><b>3.25</b> Call:</a></li>
<li class="chapter" data-level="3.26" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#lmformula-fmla-data-unemployment-2"><i class="fa fa-check"></i><b>3.26</b> lm(formula = fmla, data = unemployment)</a></li>
<li class="chapter" data-level="3.27" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-36"><i class="fa fa-check"></i><b>3.27</b> </a></li>
<li class="chapter" data-level="3.28" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#residuals"><i class="fa fa-check"></i><b>3.28</b> Residuals:</a></li>
<li class="chapter" data-level="3.29" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min-1q-median-3q-max-2"><i class="fa fa-check"></i><b>3.29</b> Min 1Q Median 3Q Max</a></li>
<li class="chapter" data-level="3.30" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-37"><i class="fa fa-check"></i><b>3.30</b> -0.77621 -0.34050 -0.09004 0.27911 1.31254</a></li>
<li class="chapter" data-level="3.31" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-38"><i class="fa fa-check"></i><b>3.31</b> </a></li>
<li class="chapter" data-level="3.32" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#coefficients-4"><i class="fa fa-check"></i><b>3.32</b> Coefficients:</a></li>
<li class="chapter" data-level="3.33" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#estimate-std.-error-t-value-prt"><i class="fa fa-check"></i><b>3.33</b> Estimate Std. Error t value Pr(&gt;|t|)</a></li>
<li><a href="supervised-learning-in-r-regression.html#intercept-1.43411-0.60340-2.377-0.0367-male_unemployment-0.69453-0.09767-7.111-1.97e-05-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-residual-standard-error-0.5803-on-11-degrees-of-freedom-multiple-r-squared-0.8213-adjusted-r-squared-0.8051-f-statistic-50.56-on-1-and-11-df-p-value-1.966e-05-r.squared-adj.r.squared-sigma-statistic-p.value-df-loglik-1-0.8213157-0.8050716-0.5802596-50.56108-1.965985e-05-2--10.28471-aic-bic-deviance-df.residual-1-26.56943-28.26428-3.703714-11-1-f-test-summary-r20.821-f11150.6-p2e-05.-1-4.906757-blood_pressure-age-weight-min.-128.0-min.-46.00-min.-167-1st-qu.140.0-1st-qu.56.50-1st-qu.186-median-153.0-median-64.00-median-194-mean-150.1-mean-62.45-mean-195-3rd-qu.160.5-3rd-qu.69.50-3rd-qu.209-max.-168.0-max.-74.00-max.-220-blood_pressure-age-weight-call-lmformula-fmla-data-bloodpressure-coefficients-intercept-age-weight-30.9941-0.8614-0.3349-call-lmformula-fmla-data-bloodpressure-residuals-min-1q-median-3q-max--3.4640--1.1949--0.4078-1.8511-2.6981-coefficients-estimate-std.-error-t-value-prt-intercept-30.9941-11.9438-2.595-0.03186-age-0.8614-0.2482-3.470-0.00844"><span class="toc-section-number">3.34</span> (Intercept) 1.43411 0.60340 2.377 0.0367 *<br />
## male_unemployment 0.69453 0.09767 7.111 1.97e-05 <strong><em> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## Residual standard error: 0.5803 on 11 degrees of freedom ## Multiple R-squared: 0.8213, Adjusted R-squared: 0.8051 ## F-statistic: 50.56 on 1 and 11 DF, p-value: 1.966e-05 ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.8213157 0.8050716 0.5802596 50.56108 1.965985e-05 2 -10.28471 ## AIC BIC deviance df.residual ## 1 26.56943 28.26428 3.703714 11 ## [1] “F Test summary: (R2=0.821, F(1,11)=50.6, p=2e-05).” ## 1 ## 4.906757 ## blood_pressure age weight<br />
## Min. :128.0 Min. :46.00 Min. :167<br />
## 1st Qu.:140.0 1st Qu.:56.50 1st Qu.:186<br />
## Median :153.0 Median :64.00 Median :194<br />
## Mean :150.1 Mean :62.45 Mean :195<br />
## 3rd Qu.:160.5 3rd Qu.:69.50 3rd Qu.:209<br />
## Max. :168.0 Max. :74.00 Max. :220 ## blood_pressure ~ age + weight ## ## Call: ## lm(formula = fmla, data = bloodpressure) ## ## Coefficients: ## (Intercept) age weight<br />
## 30.9941 0.8614 0.3349 ## ## Call: ## lm(formula = fmla, data = bloodpressure) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4640 -1.1949 -0.4078 1.8511 2.6981 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|)<br />
## (Intercept) 30.9941 11.9438 2.595 0.03186 </em> ## age 0.8614 0.2482 3.470 0.00844 </strong></a></li>
<li class="chapter" data-level="3.35" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#weight-0.3349-0.1307-2.563-0.03351"><i class="fa fa-check"></i><b>3.35</b> weight 0.3349 0.1307 2.563 0.03351 *</a></li>
<li class="chapter" data-level="3.36" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-39"><i class="fa fa-check"></i><b>3.36</b> —</a></li>
<li class="chapter" data-level="3.37" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#signif.-codes-0-0.001-0.01-0.05-.-0.1-1"><i class="fa fa-check"></i><b>3.37</b> Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1</a></li>
<li class="chapter" data-level="3.38" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-40"><i class="fa fa-check"></i><b>3.38</b> </a></li>
<li class="chapter" data-level="3.39" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#residual-standard-error-2.318-on-8-degrees-of-freedom"><i class="fa fa-check"></i><b>3.39</b> Residual standard error: 2.318 on 8 degrees of freedom</a></li>
<li class="chapter" data-level="3.40" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#multiple-r-squared-0.9768-adjusted-r-squared-0.9711"><i class="fa fa-check"></i><b>3.40</b> Multiple R-squared: 0.9768, Adjusted R-squared: 0.9711</a></li>
<li class="chapter" data-level="3.41" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#f-statistic-168.8-on-2-and-8-df-p-value-2.874e-07"><i class="fa fa-check"></i><b>3.41</b> F-statistic: 168.8 on 2 and 8 DF, p-value: 2.874e-07</a></li>
<li class="chapter" data-level="3.42" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#training-and-evaluating-regression-models"><i class="fa fa-check"></i><b>3.42</b> Training and Evaluating Regression Models</a></li>
<li class="chapter" data-level="3.43" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#male_unemployment-female_unemployment-prediction"><i class="fa fa-check"></i><b>3.43</b> male_unemployment female_unemployment prediction</a></li>
<li class="chapter" data-level="3.44" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-2.900-min.-4.000-min.-3.448"><i class="fa fa-check"></i><b>3.44</b> Min. :2.900 Min. :4.000 Min. :3.448</a></li>
<li class="chapter" data-level="3.45" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.4.900-1st-qu.4.400-1st-qu.4.837"><i class="fa fa-check"></i><b>3.45</b> 1st Qu.:4.900 1st Qu.:4.400 1st Qu.:4.837</a></li>
<li class="chapter" data-level="3.46" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-6.000-median-5.200-median-5.601"><i class="fa fa-check"></i><b>3.46</b> Median :6.000 Median :5.200 Median :5.601</a></li>
<li class="chapter" data-level="3.47" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-5.954-mean-5.569-mean-5.569"><i class="fa fa-check"></i><b>3.47</b> Mean :5.954 Mean :5.569 Mean :5.569</a></li>
<li class="chapter" data-level="3.48" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.6.700-3rd-qu.6.100-3rd-qu.6.087"><i class="fa fa-check"></i><b>3.48</b> 3rd Qu.:6.700 3rd Qu.:6.100 3rd Qu.:6.087</a></li>
<li class="chapter" data-level="3.49" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-9.800-max.-7.900-max.-8.240"><i class="fa fa-check"></i><b>3.49</b> Max. :9.800 Max. :7.900 Max. :8.240</a></li>
<li class="chapter" data-level="3.50" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#warning-package-wvplots-was-built-under-r-version-3.4.4"><i class="fa fa-check"></i><b>3.50</b> Warning: package ‘WVPlots’ was built under R version 3.4.4</a></li>
<li class="chapter" data-level="3.51" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-41"><i class="fa fa-check"></i><b>3.51</b> [1] 0.5337612</a></li>
<li class="chapter" data-level="3.52" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-42"><i class="fa fa-check"></i><b>3.52</b> [1] 1.314271</a></li>
<li class="chapter" data-level="3.53" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-43"><i class="fa fa-check"></i><b>3.53</b> [1] 0.4061273</a></li>
<li class="chapter" data-level="3.54" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-44"><i class="fa fa-check"></i><b>3.54</b> [1] 5.569231</a></li>
<li class="chapter" data-level="3.55" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-45"><i class="fa fa-check"></i><b>3.55</b> [1] 20.72769</a></li>
<li class="chapter" data-level="3.56" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-46"><i class="fa fa-check"></i><b>3.56</b> [1] 3.703714</a></li>
<li class="chapter" data-level="3.57" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-47"><i class="fa fa-check"></i><b>3.57</b> [1] 0.8213157</a></li>
<li class="chapter" data-level="3.58" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-48"><i class="fa fa-check"></i><b>3.58</b> [1] 0.8213157</a></li>
<li class="chapter" data-level="3.59" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-49"><i class="fa fa-check"></i><b>3.59</b> [1] 0.9062647</a></li>
<li class="chapter" data-level="3.60" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-50"><i class="fa fa-check"></i><b>3.60</b> [1] 0.9062647</a></li>
<li class="chapter" data-level="3.61" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-51"><i class="fa fa-check"></i><b>3.61</b> [1] 0.8213157</a></li>
<li class="chapter" data-level="3.62" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-52"><i class="fa fa-check"></i><b>3.62</b> [1] 0.8213157</a></li>
<li class="chapter" data-level="3.63" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#manufacturer-model-displ-year"><i class="fa fa-check"></i><b>3.63</b> manufacturer model displ year</a></li>
<li class="chapter" data-level="3.64" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#length234-length234-min.-1.600-min.-1999"><i class="fa fa-check"></i><b>3.64</b> Length:234 Length:234 Min. :1.600 Min. :1999</a></li>
<li class="chapter" data-level="3.65" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#class-character-class-character-1st-qu.2.400-1st-qu.1999"><i class="fa fa-check"></i><b>3.65</b> Class :character Class :character 1st Qu.:2.400 1st Qu.:1999</a></li>
<li class="chapter" data-level="3.66" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mode-character-mode-character-median-3.300-median-2004"><i class="fa fa-check"></i><b>3.66</b> Mode :character Mode :character Median :3.300 Median :2004</a></li>
<li class="chapter" data-level="3.67" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-3.472-mean-2004"><i class="fa fa-check"></i><b>3.67</b> Mean :3.472 Mean :2004</a></li>
<li class="chapter" data-level="3.68" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.4.600-3rd-qu.2008"><i class="fa fa-check"></i><b>3.68</b> 3rd Qu.:4.600 3rd Qu.:2008</a></li>
<li class="chapter" data-level="3.69" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-7.000-max.-2008"><i class="fa fa-check"></i><b>3.69</b> Max. :7.000 Max. :2008</a></li>
<li class="chapter" data-level="3.70" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#cyl-trans-drv-cty"><i class="fa fa-check"></i><b>3.70</b> cyl trans drv cty</a></li>
<li class="chapter" data-level="3.71" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-4.000-length234-length234-min.-9.00"><i class="fa fa-check"></i><b>3.71</b> Min. :4.000 Length:234 Length:234 Min. : 9.00</a></li>
<li class="chapter" data-level="3.72" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.4.000-class-character-class-character-1st-qu.14.00"><i class="fa fa-check"></i><b>3.72</b> 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00</a></li>
<li class="chapter" data-level="3.73" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-6.000-mode-character-mode-character-median-17.00"><i class="fa fa-check"></i><b>3.73</b> Median :6.000 Mode :character Mode :character Median :17.00</a></li>
<li class="chapter" data-level="3.74" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-5.889-mean-16.86"><i class="fa fa-check"></i><b>3.74</b> Mean :5.889 Mean :16.86</a></li>
<li class="chapter" data-level="3.75" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.8.000-3rd-qu.19.00"><i class="fa fa-check"></i><b>3.75</b> 3rd Qu.:8.000 3rd Qu.:19.00</a></li>
<li class="chapter" data-level="3.76" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-8.000-max.-35.00"><i class="fa fa-check"></i><b>3.76</b> Max. :8.000 Max. :35.00</a></li>
<li class="chapter" data-level="3.77" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hwy-fl-class"><i class="fa fa-check"></i><b>3.77</b> hwy fl class</a></li>
<li class="chapter" data-level="3.78" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-12.00-length234-length234"><i class="fa fa-check"></i><b>3.78</b> Min. :12.00 Length:234 Length:234</a></li>
<li class="chapter" data-level="3.79" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.18.00-class-character-class-character"><i class="fa fa-check"></i><b>3.79</b> 1st Qu.:18.00 Class :character Class :character</a></li>
<li class="chapter" data-level="3.80" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-24.00-mode-character-mode-character"><i class="fa fa-check"></i><b>3.80</b> Median :24.00 Mode :character Mode :character</a></li>
<li class="chapter" data-level="3.81" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-23.44"><i class="fa fa-check"></i><b>3.81</b> Mean :23.44</a></li>
<li class="chapter" data-level="3.82" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.27.00"><i class="fa fa-check"></i><b>3.82</b> 3rd Qu.:27.00</a></li>
<li class="chapter" data-level="3.83" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-44.00"><i class="fa fa-check"></i><b>3.83</b> Max. :44.00</a></li>
<li class="chapter" data-level="3.84" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-53"><i class="fa fa-check"></i><b>3.84</b> [1] 234 11</a></li>
<li class="chapter" data-level="3.85" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-54"><i class="fa fa-check"></i><b>3.85</b> [1] 234</a></li>
<li class="chapter" data-level="3.86" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-55"><i class="fa fa-check"></i><b>3.86</b> [1] 234</a></li>
<li class="chapter" data-level="3.87" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-56"><i class="fa fa-check"></i><b>3.87</b> [1] 176</a></li>
<li class="chapter" data-level="3.88" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-57"><i class="fa fa-check"></i><b>3.88</b> [1] 176</a></li>
<li class="chapter" data-level="3.89" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-58"><i class="fa fa-check"></i><b>3.89</b> [1] 178</a></li>
<li class="chapter" data-level="3.90" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-59"><i class="fa fa-check"></i><b>3.90</b> [1] 56</a></li>
<li class="chapter" data-level="3.91" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#manufacturer-model-displ-year-1"><i class="fa fa-check"></i><b>3.91</b> manufacturer model displ year</a></li>
<li class="chapter" data-level="3.92" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#length178-length178-min.-1.600-min.-1999"><i class="fa fa-check"></i><b>3.92</b> Length:178 Length:178 Min. :1.600 Min. :1999</a></li>
<li class="chapter" data-level="3.93" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#class-character-class-character-1st-qu.2.400-1st-qu.1999-1"><i class="fa fa-check"></i><b>3.93</b> Class :character Class :character 1st Qu.:2.400 1st Qu.:1999</a></li>
<li class="chapter" data-level="3.94" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mode-character-mode-character-median-3.300-median-1999"><i class="fa fa-check"></i><b>3.94</b> Mode :character Mode :character Median :3.300 Median :1999</a></li>
<li class="chapter" data-level="3.95" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-3.467-mean-2003"><i class="fa fa-check"></i><b>3.95</b> Mean :3.467 Mean :2003</a></li>
<li class="chapter" data-level="3.96" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.4.600-3rd-qu.2008-1"><i class="fa fa-check"></i><b>3.96</b> 3rd Qu.:4.600 3rd Qu.:2008</a></li>
<li class="chapter" data-level="3.97" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-7.000-max.-2008-1"><i class="fa fa-check"></i><b>3.97</b> Max. :7.000 Max. :2008</a></li>
<li class="chapter" data-level="3.98" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#cyl-trans-drv-cty-1"><i class="fa fa-check"></i><b>3.98</b> cyl trans drv cty</a></li>
<li class="chapter" data-level="3.99" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-4.000-length178-length178-min.-9.00"><i class="fa fa-check"></i><b>3.99</b> Min. :4.000 Length:178 Length:178 Min. : 9.00</a></li>
<li class="chapter" data-level="3.100" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.4.000-class-character-class-character-1st-qu.14.00-1"><i class="fa fa-check"></i><b>3.100</b> 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00</a></li>
<li class="chapter" data-level="3.101" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-6.000-mode-character-mode-character-median-17.00-1"><i class="fa fa-check"></i><b>3.101</b> Median :6.000 Mode :character Mode :character Median :17.00</a></li>
<li class="chapter" data-level="3.102" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-5.904-mean-17.02"><i class="fa fa-check"></i><b>3.102</b> Mean :5.904 Mean :17.02</a></li>
<li class="chapter" data-level="3.103" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.8.000-3rd-qu.19.00-1"><i class="fa fa-check"></i><b>3.103</b> 3rd Qu.:8.000 3rd Qu.:19.00</a></li>
<li class="chapter" data-level="3.104" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-8.000-max.-35.00-1"><i class="fa fa-check"></i><b>3.104</b> Max. :8.000 Max. :35.00</a></li>
<li class="chapter" data-level="3.105" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hwy-fl-class-1"><i class="fa fa-check"></i><b>3.105</b> hwy fl class</a></li>
<li class="chapter" data-level="3.106" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-12.00-length178-length178"><i class="fa fa-check"></i><b>3.106</b> Min. :12.00 Length:178 Length:178</a></li>
<li class="chapter" data-level="3.107" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.18.00-class-character-class-character-1"><i class="fa fa-check"></i><b>3.107</b> 1st Qu.:18.00 Class :character Class :character</a></li>
<li class="chapter" data-level="3.108" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-25.00-mode-character-mode-character"><i class="fa fa-check"></i><b>3.108</b> Median :25.00 Mode :character Mode :character</a></li>
<li class="chapter" data-level="3.109" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-23.58"><i class="fa fa-check"></i><b>3.109</b> Mean :23.58</a></li>
<li class="chapter" data-level="3.110" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.27.00-1"><i class="fa fa-check"></i><b>3.110</b> 3rd Qu.:27.00</a></li>
<li class="chapter" data-level="3.111" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-44.00-1"><i class="fa fa-check"></i><b>3.111</b> Max. :44.00</a></li>
<li class="chapter" data-level="3.112" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#cty-hwy"><i class="fa fa-check"></i><b>3.112</b> cty ~ hwy</a></li>
<li class="chapter" data-level="3.113" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-60"><i class="fa fa-check"></i><b>3.113</b> </a></li>
<li class="chapter" data-level="3.114" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#call-4"><i class="fa fa-check"></i><b>3.114</b> Call:</a></li>
<li class="chapter" data-level="3.115" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#lmformula-fmla-data-mpg_train"><i class="fa fa-check"></i><b>3.115</b> lm(formula = fmla, data = mpg_train)</a></li>
<li class="chapter" data-level="3.116" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-61"><i class="fa fa-check"></i><b>3.116</b> </a></li>
<li class="chapter" data-level="3.117" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#residuals-1"><i class="fa fa-check"></i><b>3.117</b> Residuals:</a></li>
<li class="chapter" data-level="3.118" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min-1q-median-3q-max-3"><i class="fa fa-check"></i><b>3.118</b> Min 1Q Median 3Q Max</a></li>
<li class="chapter" data-level="3.119" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-62"><i class="fa fa-check"></i><b>3.119</b> -2.9957 -0.6870 -0.0077 0.6216 4.4733</a></li>
<li class="chapter" data-level="3.120" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-63"><i class="fa fa-check"></i><b>3.120</b> </a></li>
<li class="chapter" data-level="3.121" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#coefficients-5"><i class="fa fa-check"></i><b>3.121</b> Coefficients:</a></li>
<li class="chapter" data-level="3.122" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#estimate-std.-error-t-value-prt-1"><i class="fa fa-check"></i><b>3.122</b> Estimate Std. Error t value Pr(&gt;|t|)</a></li>
<li class="chapter" data-level="3.123" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intercept-0.71118-0.39277-1.811-0.0719-."><i class="fa fa-check"></i><b>3.123</b> (Intercept) 0.71118 0.39277 1.811 0.0719 .</a></li>
<li><a href="supervised-learning-in-r-regression.html#hwy-0.69138-0.01615-42.813-2e-16-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-residual-standard-error-1.281-on-176-degrees-of-freedom-multiple-r-squared-0.9124-adjusted-r-squared-0.9119-f-statistic-1833-on-1-and-176-df-p-value-2.2e-16-1-1.273458-1-1.169409-1-0.9039807-1-0.9181344-list-of-5-list-of-2-..-train-int-1188-2-3-4-5-6-7-8-9-10-11-..-app-int-146-122-151-47-103-75-82-25-174-200-46-list-of-2-..-train-int-1187-1-2-3-4-6-7-8-10-11-12-..-app-int-147-115-18-171-192-209-147-57-88-208-129-list-of-2-..-train-int-1187-1-2-3-4-5-6-7-8-9-10-..-app-int-147-164-61-187-11-55-211-125-191-60-127-list-of-2-..-train-int-1187-1-2-3-5-7-8-9-10-11-12-..-app-int-147-58-213-160-157-56-114-14-198-188-31-list-of-2-..-train-int-1187-1-4-5-6-9-11-12-13-14-15-..-app-int-147-2-69-42-74-154-8-205-40-33-101---attr-splitmethod-chr-kwaycross-manufacturer-model-displ-year-length234-length234-min.-1.600-min.-1999-class-character-class-character-1st-qu.2.400-1st-qu.1999-mode-character-mode-character-median-3.300-median-2004-mean-3.472-mean-2004-3rd-qu.4.600-3rd-qu.2008-max.-7.000-max.-2008-cyl-trans-drv-cty-min.-4.000-length234-length234-min.-9.00-1st-qu.4.000-class-character-class-character-1st-qu.14.00-median-6.000-mode-character-mode-character-median-17.00-mean-5.889-mean-16.86-3rd-qu.8.000-3rd-qu.19.00-max.-8.000-max.-35.00-hwy-fl-class-min.-12.00-length234-length234-1st-qu.18.00-class-character-class-character-median-24.00-mode-character-mode-character-mean-23.44-3rd-qu.27.00-max.-44.00-list-of-5-list-of-2-..-train-int-1188-2-3-4-5-6-7-8-9-10-11-..-app-int-146-122-151-47-103-75-82-25-174-200-46-list-of-2-..-train-int-1187-1-2-3-4-6-7-8-10-11-12-..-app-int-147-115-18-171-192-209-147-57-88-208-129-list-of-2-..-train-int-1187-1-2-3-4-5-6-7-8-9-10-..-app-int-147-164-61-187-11-55-211-125-191-60-127-list-of-2-..-train-int-1187-1-2-3-5-7-8-9-10-11-12-..-app-int-147-58-213-160-157-56-114-14-198-188-31-list-of-2-..-train-int-1187-1-4-5-6-9-11-12-13-14-15-..-app-int-147-2-69-42-74-154-8-205-40-33-101---attr-splitmethod-chr-kwaycross-1-1.247045-1-1.266766-issues-to-consider-warning-package-sleuth3-was-built-under-r-version-3.4.3-flowers-time-intensity-1-62.3-1-150-2-77.4-1-150-3-55.3-1-300-4-54.2-1-300-5-49.6-1-450-6-61.9-1-450-7-39.4-1-600-8-45.7-1-600-9-31.3-1-750-10-44.9-1-750-11-36.8-1-900-12-41.9-1-900-13-77.8-2-150-14-75.6-2-150-15-69.1-2-300-16-78.0-2-300-17-57.0-2-450-18-71.1-2-450-19-62.9-2-600-20-52.2-2-600-21-60.3-2-750-22-45.6-2-750-23-52.6-2-900-24-44.4-2-900-data.frame-24-obs.-of-3-variables-flowers-num-62.3-77.4-55.3-54.2-49.6-61.9-39.4-45.7-31.3-44.9-time-chr-late-late-late-late-intensity-int-150-150-300-300-450-450-600-600-750-750-1-late-early-flowers-intensity-time-flowers-time-intensity-1-62.3-late-150-2-77.4-late-150-3-55.3-late-300-4-54.2-late-300-5-49.6-late-450-6-61.9-late-450-7-39.4-late-600-8-45.7-late-600-9-31.3-late-750-10-44.9-late-750-11-36.8-late-900-12-41.9-late-900-13-77.8-early-150-14-75.6-early-150-15-69.1-early-300-16-78.0-early-300-17-57.0-early-450-18-71.1-early-450-19-62.9-early-600-20-52.2-early-600-intercept-intensity-timelate-1-1-150-1-2-1-150-1-3-1-300-1-4-1-300-1-5-1-450-1-6-1-450-1-7-1-600-1-8-1-600-1-9-1-750-1-10-1-750-1-11-1-900-1-12-1-900-1-13-1-150-0-14-1-150-0-15-1-300-0-16-1-300-0-17-1-450-0-18-1-450-0-19-1-600-0-20-1-600-0-data.frame-24-obs.-of-3-variables-flowers-num-62.3-77.4-55.3-54.2-49.6-61.9-39.4-45.7-31.3-44.9-time-chr-late-late-late-late-intensity-int-150-150-300-300-450-450-600-600-750-750-flowers-intensity-time-intercept-intensity-timelate-min.-1-min.-150-min.-0.0-1st-qu.1-1st-qu.300-1st-qu.0.0-median-1-median-525-median-0.5-mean-1-mean-525-mean-0.5-3rd-qu.1-3rd-qu.750-3rd-qu.1.0-max.-1-max.-900-max.-1.0-call-lmformula-fmla-data-flowers-residuals-min-1q-median-3q-max--9.652--4.139--1.558-5.632-12.165-coefficients-estimate-std.-error-t-value-prt-intercept-83.464167-3.273772-25.495-2e-16"><span class="toc-section-number">3.124</span> hwy 0.69138 0.01615 42.813 &lt;2e-16 <strong><em> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## Residual standard error: 1.281 on 176 degrees of freedom ## Multiple R-squared: 0.9124, Adjusted R-squared: 0.9119 ## F-statistic: 1833 on 1 and 176 DF, p-value: &lt; 2.2e-16 ## [1] 1.273458 ## [1] 1.169409 ## [1] 0.9039807 ## [1] 0.9181344 ## List of 5 ## $ :List of 2 ## ..$ train: int [1:188] 2 3 4 5 6 7 8 9 10 11 … ## ..$ app : int [1:46] 122 151 47 103 75 82 25 174 200 46 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 4 6 7 8 10 11 12 … ## ..$ app : int [1:47] 115 18 171 192 209 147 57 88 208 129 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 4 5 6 7 8 9 10 … ## ..$ app : int [1:47] 164 61 187 11 55 211 125 191 60 127 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 5 7 8 9 10 11 12 … ## ..$ app : int [1:47] 58 213 160 157 56 114 14 198 188 31 … ## $ :List of 2 ## ..$ train: int [1:187] 1 4 5 6 9 11 12 13 14 15 … ## ..$ app : int [1:47] 2 69 42 74 154 8 205 40 33 101 … ## - attr(</em>, “splitmethod”)= chr “kwaycross” ## manufacturer model displ year<br />
## Length:234 Length:234 Min. :1.600 Min. :1999<br />
## Class :character Class :character 1st Qu.:2.400 1st Qu.:1999<br />
## Mode :character Mode :character Median :3.300 Median :2004<br />
## Mean :3.472 Mean :2004<br />
## 3rd Qu.:4.600 3rd Qu.:2008<br />
## Max. :7.000 Max. :2008<br />
## cyl trans drv cty<br />
## Min. :4.000 Length:234 Length:234 Min. : 9.00<br />
## 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00<br />
## Median :6.000 Mode :character Mode :character Median :17.00<br />
## Mean :5.889 Mean :16.86<br />
## 3rd Qu.:8.000 3rd Qu.:19.00<br />
## Max. :8.000 Max. :35.00<br />
## hwy fl class<br />
## Min. :12.00 Length:234 Length:234<br />
## 1st Qu.:18.00 Class :character Class :character<br />
## Median :24.00 Mode :character Mode :character<br />
## Mean :23.44<br />
## 3rd Qu.:27.00<br />
## Max. :44.00 ## List of 5 ## $ :List of 2 ## ..$ train: int [1:188] 2 3 4 5 6 7 8 9 10 11 … ## ..$ app : int [1:46] 122 151 47 103 75 82 25 174 200 46 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 4 6 7 8 10 11 12 … ## ..$ app : int [1:47] 115 18 171 192 209 147 57 88 208 129 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 4 5 6 7 8 9 10 … ## ..$ app : int [1:47] 164 61 187 11 55 211 125 191 60 127 … ## $ :List of 2 ## ..$ train: int [1:187] 1 2 3 5 7 8 9 10 11 12 … ## ..$ app : int [1:47] 58 213 160 157 56 114 14 198 188 31 … ## $ :List of 2 ## ..$ train: int [1:187] 1 4 5 6 9 11 12 13 14 15 … ## ..$ app : int [1:47] 2 69 42 74 154 8 205 40 33 101 … ## - attr(<em>, “splitmethod”)= chr “kwaycross” ## [1] 1.247045 ## [1] 1.266766 ## Issues to Consider ## Warning: package ‘Sleuth3’ was built under R version 3.4.3 ## Flowers Time Intensity ## 1 62.3 1 150 ## 2 77.4 1 150 ## 3 55.3 1 300 ## 4 54.2 1 300 ## 5 49.6 1 450 ## 6 61.9 1 450 ## 7 39.4 1 600 ## 8 45.7 1 600 ## 9 31.3 1 750 ## 10 44.9 1 750 ## 11 36.8 1 900 ## 12 41.9 1 900 ## 13 77.8 2 150 ## 14 75.6 2 150 ## 15 69.1 2 300 ## 16 78.0 2 300 ## 17 57.0 2 450 ## 18 71.1 2 450 ## 19 62.9 2 600 ## 20 52.2 2 600 ## 21 60.3 2 750 ## 22 45.6 2 750 ## 23 52.6 2 900 ## 24 44.4 2 900 ## ‘data.frame’: 24 obs. of 3 variables: ## $ Flowers : num 62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 … ## $ Time : chr “Late” “Late” “Late” “Late” … ## $ Intensity: int 150 150 300 300 450 450 600 600 750 750 … ## [1] “Late” “Early” ## Flowers ~ Intensity + Time ## Flowers Time Intensity ## 1 62.3 Late 150 ## 2 77.4 Late 150 ## 3 55.3 Late 300 ## 4 54.2 Late 300 ## 5 49.6 Late 450 ## 6 61.9 Late 450 ## 7 39.4 Late 600 ## 8 45.7 Late 600 ## 9 31.3 Late 750 ## 10 44.9 Late 750 ## 11 36.8 Late 900 ## 12 41.9 Late 900 ## 13 77.8 Early 150 ## 14 75.6 Early 150 ## 15 69.1 Early 300 ## 16 78.0 Early 300 ## 17 57.0 Early 450 ## 18 71.1 Early 450 ## 19 62.9 Early 600 ## 20 52.2 Early 600 ## (Intercept) Intensity TimeLate ## 1 1 150 1 ## 2 1 150 1 ## 3 1 300 1 ## 4 1 300 1 ## 5 1 450 1 ## 6 1 450 1 ## 7 1 600 1 ## 8 1 600 1 ## 9 1 750 1 ## 10 1 750 1 ## 11 1 900 1 ## 12 1 900 1 ## 13 1 150 0 ## 14 1 150 0 ## 15 1 300 0 ## 16 1 300 0 ## 17 1 450 0 ## 18 1 450 0 ## 19 1 600 0 ## 20 1 600 0 ## ‘data.frame’: 24 obs. of 3 variables: ## $ Flowers : num 62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 … ## $ Time : chr “Late” “Late” “Late” “Late” … ## $ Intensity: int 150 150 300 300 450 450 600 600 750 750 … ## Flowers ~ Intensity + Time ## (Intercept) Intensity TimeLate<br />
## Min. :1 Min. :150 Min. :0.0<br />
## 1st Qu.:1 1st Qu.:300 1st Qu.:0.0<br />
## Median :1 Median :525 Median :0.5<br />
## Mean :1 Mean :525 Mean :0.5<br />
## 3rd Qu.:1 3rd Qu.:750 3rd Qu.:1.0<br />
## Max. :1 Max. :900 Max. :1.0 ## ## Call: ## lm(formula = fmla, data = flowers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.652 -4.139 -1.558 5.632 12.165 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|)<br />
## (Intercept) 83.464167 3.273772 25.495 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.125" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intensity--0.040471-0.005132--7.886-1.04e-07-timelate--12.158333-2.629557--4.624-0.000146"><i class="fa fa-check"></i><b>3.125</b> Intensity -0.040471 0.005132 -7.886 1.04e-07 <strong><em> ## TimeLate -12.158333 2.629557 -4.624 0.000146 </em></strong></a></li>
<li class="chapter" data-level="3.126" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-64"><i class="fa fa-check"></i><b>3.126</b> —</a></li>
<li class="chapter" data-level="3.127" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#signif.-codes-0-0.001-0.01-0.05-.-0.1-1-1"><i class="fa fa-check"></i><b>3.127</b> Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1</a></li>
<li class="chapter" data-level="3.128" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-65"><i class="fa fa-check"></i><b>3.128</b> </a></li>
<li class="chapter" data-level="3.129" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#residual-standard-error-6.441-on-21-degrees-of-freedom"><i class="fa fa-check"></i><b>3.129</b> Residual standard error: 6.441 on 21 degrees of freedom</a></li>
<li class="chapter" data-level="3.130" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#multiple-r-squared-0.7992-adjusted-r-squared-0.78"><i class="fa fa-check"></i><b>3.130</b> Multiple R-squared: 0.7992, Adjusted R-squared: 0.78</a></li>
<li class="chapter" data-level="3.131" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#f-statistic-41.78-on-2-and-21-df-p-value-4.786e-08"><i class="fa fa-check"></i><b>3.131</b> F-statistic: 41.78 on 2 and 21 DF, p-value: 4.786e-08</a><ul>
<li class="chapter" data-level="3.131.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#interactions"><i class="fa fa-check"></i><b>3.131.1</b> Interactions</a></li>
<li class="chapter" data-level="3.131.2" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#transforming-the-response-before-modeling"><i class="fa fa-check"></i><b>3.131.2</b> Transforming the response before modeling</a></li>
</ul></li>
<li class="chapter" data-level="3.132" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#data.frame-100-obs.-of-3-variables"><i class="fa fa-check"></i><b>3.132</b> ‘data.frame’: 100 obs. of 3 variables:</a></li>
<li class="chapter" data-level="3.133" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#y-num-9.15-1.9--3.86-2.39-1.54"><i class="fa fa-check"></i><b>3.133</b> $ y : num 9.15 1.9 -3.86 2.39 1.54 …</a></li>
<li class="chapter" data-level="3.134" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#pred-num-6.43-3.47-1.59-3.76-9.51"><i class="fa fa-check"></i><b>3.134</b> $ pred : num 6.43 3.47 1.59 3.76 9.51 …</a></li>
<li class="chapter" data-level="3.135" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#label-factor-w-2-levels-large-purchases..-2-2-2-2-2-2-2-2-2-2"><i class="fa fa-check"></i><b>3.135</b> $ label: Factor w/ 2 levels “large purchases”,..: 2 2 2 2 2 2 2 2 2 2 …</a></li>
<li class="chapter" data-level="3.136" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-66"><i class="fa fa-check"></i><b>3.136</b> </a></li>
<li class="chapter" data-level="3.137" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#attaching-package-dplyr"><i class="fa fa-check"></i><b>3.137</b> Attaching package: ‘dplyr’</a></li>
<li class="chapter" data-level="3.138" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#the-following-objects-are-masked-from-packagestats"><i class="fa fa-check"></i><b>3.138</b> The following objects are masked from ‘package:stats’:</a></li>
<li class="chapter" data-level="3.139" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-67"><i class="fa fa-check"></i><b>3.139</b> </a></li>
<li class="chapter" data-level="3.140" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#filter-lag"><i class="fa fa-check"></i><b>3.140</b> filter, lag</a></li>
<li class="chapter" data-level="3.141" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#the-following-objects-are-masked-from-packagebase"><i class="fa fa-check"></i><b>3.141</b> The following objects are masked from ‘package:base’:</a></li>
<li class="chapter" data-level="3.142" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-68"><i class="fa fa-check"></i><b>3.142</b> </a></li>
<li class="chapter" data-level="3.143" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intersect-setdiff-setequal-union"><i class="fa fa-check"></i><b>3.143</b> intersect, setdiff, setequal, union</a></li>
<li class="chapter" data-level="3.144" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#y-pred-label"><i class="fa fa-check"></i><b>3.144</b> y pred label</a></li>
<li class="chapter" data-level="3.145" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.--5.894-min.-1.072-large-purchases50"><i class="fa fa-check"></i><b>3.145</b> Min. : -5.894 Min. : 1.072 large purchases:50</a></li>
<li class="chapter" data-level="3.146" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.-5.407-1st-qu.-6.373-small-purchases50"><i class="fa fa-check"></i><b>3.146</b> 1st Qu.: 5.407 1st Qu.: 6.373 small purchases:50</a></li>
<li class="chapter" data-level="3.147" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-57.374-median-55.693"><i class="fa fa-check"></i><b>3.147</b> Median : 57.374 Median : 55.693</a></li>
<li class="chapter" data-level="3.148" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-306.204-mean-305.905"><i class="fa fa-check"></i><b>3.148</b> Mean : 306.204 Mean : 305.905</a></li>
<li class="chapter" data-level="3.149" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.-550.903-3rd-qu.-547.886"><i class="fa fa-check"></i><b>3.149</b> 3rd Qu.: 550.903 3rd Qu.: 547.886</a></li>
<li class="chapter" data-level="3.150" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-1101.619-max.-1098.896"><i class="fa fa-check"></i><b>3.150</b> Max. :1101.619 Max. :1098.896</a></li>
<li class="chapter" data-level="3.151" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#a-tibble-2-x-4"><i class="fa fa-check"></i><b>3.151</b> # A tibble: 2 x 4</a></li>
<li class="chapter" data-level="3.152" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#label-min-mean-max"><i class="fa fa-check"></i><b>3.152</b> label min mean max</a></li>
<li class="chapter" data-level="3.153" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-69"><i class="fa fa-check"></i><b>3.153</b> <fctr> <dbl> <dbl> <dbl></a></li>
<li class="chapter" data-level="3.154" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#large-purchases-96.119814-605.928673-1101.61864"><i class="fa fa-check"></i><b>3.154</b> 1 large purchases 96.119814 605.928673 1101.61864</a></li>
<li class="chapter" data-level="3.155" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#small-purchases--5.893499-6.478254-18.62829"><i class="fa fa-check"></i><b>3.155</b> 2 small purchases -5.893499 6.478254 18.62829</a></li>
<li class="chapter" data-level="3.156" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#a-tibble-2-x-3"><i class="fa fa-check"></i><b>3.156</b> # A tibble: 2 x 3</a></li>
<li class="chapter" data-level="3.157" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#label-rmse-rmse.rel"><i class="fa fa-check"></i><b>3.157</b> label rmse rmse.rel</a></li>
<li class="chapter" data-level="3.158" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-70"><i class="fa fa-check"></i><b>3.158</b> <fctr> <dbl> <dbl></a></li>
<li class="chapter" data-level="3.159" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#large-purchases-5.544439-0.01473322"><i class="fa fa-check"></i><b>3.159</b> 1 large purchases 5.544439 0.01473322</a></li>
<li class="chapter" data-level="3.160" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#small-purchases-4.014969-1.24965673"><i class="fa fa-check"></i><b>3.160</b> 2 small purchases 4.014969 1.24965673</a></li>
<li class="chapter" data-level="3.161" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-1st-qu.-median-mean-3rd-qu.-max."><i class="fa fa-check"></i><b>3.161</b> Min. 1st Qu. Median Mean 3rd Qu. Max.</a></li>
<li class="chapter" data-level="3.162" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-71"><i class="fa fa-check"></i><b>3.162</b> 63 23000 39000 49894 61500 703637</a></li>
<li class="chapter" data-level="3.163" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#logincome2005-arith-word-parag-math-afqt"><i class="fa fa-check"></i><b>3.163</b> log(Income2005) ~ Arith + Word + Parag + Math + AFQT</a></li>
<li class="chapter" data-level="3.164" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-1st-qu.-median-mean-3rd-qu.-max.-1"><i class="fa fa-check"></i><b>3.164</b> Min. 1st Qu. Median Mean 3rd Qu. Max.</a></li>
<li class="chapter" data-level="3.165" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-72"><i class="fa fa-check"></i><b>3.165</b> 9.766 10.133 10.423 10.419 10.705 11.006</a></li>
<li class="chapter" data-level="3.166" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-1st-qu.-median-mean-3rd-qu.-max.-2"><i class="fa fa-check"></i><b>3.166</b> Min. 1st Qu. Median Mean 3rd Qu. Max.</a></li>
<li class="chapter" data-level="3.167" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-73"><i class="fa fa-check"></i><b>3.167</b> 17432 25167 33615 35363 44566 60217</a><ul>
<li class="chapter" data-level="3.167.1" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#transforming-input-variables"><i class="fa fa-check"></i><b>3.167.1</b> Transforming Input variables</a></li>
</ul></li>
<li class="chapter" data-level="3.168" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#size-price"><i class="fa fa-check"></i><b>3.168</b> size price</a></li>
<li class="chapter" data-level="3.169" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-44.0-min.-42.0"><i class="fa fa-check"></i><b>3.169</b> Min. : 44.0 Min. : 42.0</a></li>
<li class="chapter" data-level="3.170" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.-73.5-1st-qu.164.5"><i class="fa fa-check"></i><b>3.170</b> 1st Qu.: 73.5 1st Qu.:164.5</a></li>
<li class="chapter" data-level="3.171" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-91.0-median-203.5"><i class="fa fa-check"></i><b>3.171</b> Median : 91.0 Median :203.5</a></li>
<li class="chapter" data-level="3.172" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-94.3-mean-249.2"><i class="fa fa-check"></i><b>3.172</b> Mean : 94.3 Mean :249.2</a></li>
<li class="chapter" data-level="3.173" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.118.5-3rd-qu.287.8"><i class="fa fa-check"></i><b>3.173</b> 3rd Qu.:118.5 3rd Qu.:287.8</a></li>
<li class="chapter" data-level="3.174" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-150.0-max.-573.0"><i class="fa fa-check"></i><b>3.174</b> Max. :150.0 Max. :573.0</a></li>
<li class="chapter" data-level="3.175" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#price-isize2"><i class="fa fa-check"></i><b>3.175</b> price ~ I(size^2)</a></li>
<li class="chapter" data-level="3.176" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#size-price-1"><i class="fa fa-check"></i><b>3.176</b> size price</a></li>
<li class="chapter" data-level="3.177" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-44.0-min.-42.0-1"><i class="fa fa-check"></i><b>3.177</b> Min. : 44.0 Min. : 42.0</a></li>
<li class="chapter" data-level="3.178" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.-73.5-1st-qu.164.5-1"><i class="fa fa-check"></i><b>3.178</b> 1st Qu.: 73.5 1st Qu.:164.5</a></li>
<li class="chapter" data-level="3.179" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-91.0-median-203.5-1"><i class="fa fa-check"></i><b>3.179</b> Median : 91.0 Median :203.5</a></li>
<li class="chapter" data-level="3.180" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-94.3-mean-249.2-1"><i class="fa fa-check"></i><b>3.180</b> Mean : 94.3 Mean :249.2</a></li>
<li class="chapter" data-level="3.181" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.118.5-3rd-qu.287.8-1"><i class="fa fa-check"></i><b>3.181</b> 3rd Qu.:118.5 3rd Qu.:287.8</a></li>
<li class="chapter" data-level="3.182" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-150.0-max.-573.0-1"><i class="fa fa-check"></i><b>3.182</b> Max. :150.0 Max. :573.0</a></li>
<li class="chapter" data-level="3.183" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#price-isize2-1"><i class="fa fa-check"></i><b>3.183</b> price ~ I(size^2)</a></li>
<li class="chapter" data-level="3.184" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#a-tibble-2-x-2"><i class="fa fa-check"></i><b>3.184</b> # A tibble: 2 x 2</a></li>
<li class="chapter" data-level="3.185" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#modeltype-rmse"><i class="fa fa-check"></i><b>3.185</b> modeltype rmse</a></li>
<li class="chapter" data-level="3.186" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-74"><i class="fa fa-check"></i><b>3.186</b> <chr> <dbl></a></li>
<li class="chapter" data-level="3.187" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#pred_lin-74.29993"><i class="fa fa-check"></i><b>3.187</b> 1 pred_lin 74.29993</a></li>
<li class="chapter" data-level="3.188" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#pred_sqr-63.69409"><i class="fa fa-check"></i><b>3.188</b> 2 pred_sqr 63.69409</a></li>
<li class="chapter" data-level="3.189" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#dealing-with-non-linear-responses"><i class="fa fa-check"></i><b>3.189</b> Dealing with Non-Linear Responses</a></li>
<li class="chapter" data-level="3.190" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#status-age-total_length-wingspan-weight-beak_head-humerus-femur"><i class="fa fa-check"></i><b>3.190</b> status age total_length wingspan weight beak_head humerus femur</a></li>
<li class="chapter" data-level="3.191" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-154-241-24.5-31.2-0.69-0.67"><i class="fa fa-check"></i><b>3.191</b> 1 Survived adult 154 241 24.5 31.2 0.69 0.67</a></li>
<li class="chapter" data-level="3.192" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-160-252-26.9-30.8-0.74-0.71"><i class="fa fa-check"></i><b>3.192</b> 2 Survived adult 160 252 26.9 30.8 0.74 0.71</a></li>
<li class="chapter" data-level="3.193" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-155-243-26.9-30.6-0.73-0.70"><i class="fa fa-check"></i><b>3.193</b> 3 Survived adult 155 243 26.9 30.6 0.73 0.70</a></li>
<li class="chapter" data-level="3.194" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-154-245-24.3-31.7-0.74-0.69"><i class="fa fa-check"></i><b>3.194</b> 4 Survived adult 154 245 24.3 31.7 0.74 0.69</a></li>
<li class="chapter" data-level="3.195" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-156-247-24.1-31.5-0.71-0.71"><i class="fa fa-check"></i><b>3.195</b> 5 Survived adult 156 247 24.1 31.5 0.71 0.71</a></li>
<li class="chapter" data-level="3.196" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-adult-161-253-26.5-31.8-0.78-0.74"><i class="fa fa-check"></i><b>3.196</b> 6 Survived adult 161 253 26.5 31.8 0.78 0.74</a></li>
<li class="chapter" data-level="3.197" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#legbone-skull-sternum"><i class="fa fa-check"></i><b>3.197</b> legbone skull sternum</a></li>
<li class="chapter" data-level="3.198" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-75"><i class="fa fa-check"></i><b>3.198</b> 1 1.02 0.59 0.83</a></li>
<li class="chapter" data-level="3.199" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-76"><i class="fa fa-check"></i><b>3.199</b> 2 1.18 0.60 0.84</a></li>
<li class="chapter" data-level="3.200" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-77"><i class="fa fa-check"></i><b>3.200</b> 3 1.15 0.60 0.85</a></li>
<li class="chapter" data-level="3.201" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-78"><i class="fa fa-check"></i><b>3.201</b> 4 1.15 0.58 0.84</a></li>
<li class="chapter" data-level="3.202" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-79"><i class="fa fa-check"></i><b>3.202</b> 5 1.13 0.57 0.82</a></li>
<li class="chapter" data-level="3.203" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-80"><i class="fa fa-check"></i><b>3.203</b> 6 1.14 0.61 0.89</a></li>
<li class="chapter" data-level="3.204" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#status-age-total_length-wingspan"><i class="fa fa-check"></i><b>3.204</b> status age total_length wingspan</a></li>
<li class="chapter" data-level="3.205" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#perished36-length87-min.-153.0-min.-236.0"><i class="fa fa-check"></i><b>3.205</b> Perished:36 Length:87 Min. :153.0 Min. :236.0</a></li>
<li class="chapter" data-level="3.206" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived51-class-character-1st-qu.158.0-1st-qu.245.0"><i class="fa fa-check"></i><b>3.206</b> Survived:51 Class :character 1st Qu.:158.0 1st Qu.:245.0</a></li>
<li class="chapter" data-level="3.207" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mode-character-median-160.0-median-247.0"><i class="fa fa-check"></i><b>3.207</b> Mode :character Median :160.0 Median :247.0</a></li>
<li class="chapter" data-level="3.208" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-160.4-mean-247.5"><i class="fa fa-check"></i><b>3.208</b> Mean :160.4 Mean :247.5</a></li>
<li class="chapter" data-level="3.209" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.162.5-3rd-qu.251.0"><i class="fa fa-check"></i><b>3.209</b> 3rd Qu.:162.5 3rd Qu.:251.0</a></li>
<li class="chapter" data-level="3.210" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-167.0-max.-256.0"><i class="fa fa-check"></i><b>3.210</b> Max. :167.0 Max. :256.0</a></li>
<li class="chapter" data-level="3.211" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#weight-beak_head-humerus-femur"><i class="fa fa-check"></i><b>3.211</b> weight beak_head humerus femur</a></li>
<li class="chapter" data-level="3.212" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-23.2-min.-29.80-min.-0.6600-min.-0.6500"><i class="fa fa-check"></i><b>3.212</b> Min. :23.2 Min. :29.80 Min. :0.6600 Min. :0.6500</a></li>
<li class="chapter" data-level="3.213" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.24.7-1st-qu.31.40-1st-qu.0.7250-1st-qu.0.7000"><i class="fa fa-check"></i><b>3.213</b> 1st Qu.:24.7 1st Qu.:31.40 1st Qu.:0.7250 1st Qu.:0.7000</a></li>
<li class="chapter" data-level="3.214" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-25.8-median-31.70-median-0.7400-median-0.7100"><i class="fa fa-check"></i><b>3.214</b> Median :25.8 Median :31.70 Median :0.7400 Median :0.7100</a></li>
<li class="chapter" data-level="3.215" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-25.8-mean-31.64-mean-0.7353-mean-0.7134"><i class="fa fa-check"></i><b>3.215</b> Mean :25.8 Mean :31.64 Mean :0.7353 Mean :0.7134</a></li>
<li class="chapter" data-level="3.216" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.26.7-3rd-qu.32.10-3rd-qu.0.7500-3rd-qu.0.7300"><i class="fa fa-check"></i><b>3.216</b> 3rd Qu.:26.7 3rd Qu.:32.10 3rd Qu.:0.7500 3rd Qu.:0.7300</a></li>
<li class="chapter" data-level="3.217" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-31.0-max.-33.00-max.-0.7800-max.-0.7600"><i class="fa fa-check"></i><b>3.217</b> Max. :31.0 Max. :33.00 Max. :0.7800 Max. :0.7600</a></li>
<li class="chapter" data-level="3.218" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#legbone-skull-sternum-1"><i class="fa fa-check"></i><b>3.218</b> legbone skull sternum</a></li>
<li class="chapter" data-level="3.219" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min.-1.010-min.-0.5600-min.-0.7700"><i class="fa fa-check"></i><b>3.219</b> Min. :1.010 Min. :0.5600 Min. :0.7700</a></li>
<li class="chapter" data-level="3.220" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#st-qu.1.110-1st-qu.0.5900-1st-qu.0.8300"><i class="fa fa-check"></i><b>3.220</b> 1st Qu.:1.110 1st Qu.:0.5900 1st Qu.:0.8300</a></li>
<li class="chapter" data-level="3.221" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#median-1.130-median-0.6000-median-0.8500"><i class="fa fa-check"></i><b>3.221</b> Median :1.130 Median :0.6000 Median :0.8500</a></li>
<li class="chapter" data-level="3.222" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#mean-1.131-mean-0.6032-mean-0.8511"><i class="fa fa-check"></i><b>3.222</b> Mean :1.131 Mean :0.6032 Mean :0.8511</a></li>
<li class="chapter" data-level="3.223" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#rd-qu.1.160-3rd-qu.0.6100-3rd-qu.0.8800"><i class="fa fa-check"></i><b>3.223</b> 3rd Qu.:1.160 3rd Qu.:0.6100 3rd Qu.:0.8800</a></li>
<li class="chapter" data-level="3.224" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#max.-1.230-max.-0.6400-max.-0.9300"><i class="fa fa-check"></i><b>3.224</b> Max. :1.230 Max. :0.6400 Max. :0.9300</a></li>
<li class="chapter" data-level="3.225" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#survived-total_length-weight-humerus"><i class="fa fa-check"></i><b>3.225</b> survived ~ total_length + weight + humerus</a></li>
<li class="chapter" data-level="3.226" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-81"><i class="fa fa-check"></i><b>3.226</b> </a></li>
<li class="chapter" data-level="3.227" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#call-5"><i class="fa fa-check"></i><b>3.227</b> Call:</a></li>
<li class="chapter" data-level="3.228" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#glmformula-fmla-family-binomial-data-sparrow"><i class="fa fa-check"></i><b>3.228</b> glm(formula = fmla, family = binomial, data = sparrow)</a></li>
<li class="chapter" data-level="3.229" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-82"><i class="fa fa-check"></i><b>3.229</b> </a></li>
<li class="chapter" data-level="3.230" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#deviance-residuals-2"><i class="fa fa-check"></i><b>3.230</b> Deviance Residuals:</a></li>
<li class="chapter" data-level="3.231" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#min-1q-median-3q-max-4"><i class="fa fa-check"></i><b>3.231</b> Min 1Q Median 3Q Max</a></li>
<li class="chapter" data-level="3.232" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-83"><i class="fa fa-check"></i><b>3.232</b> -2.1117 -0.6026 0.2871 0.6577 1.7082</a></li>
<li class="chapter" data-level="3.233" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#section-84"><i class="fa fa-check"></i><b>3.233</b> </a></li>
<li class="chapter" data-level="3.234" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#coefficients-6"><i class="fa fa-check"></i><b>3.234</b> Coefficients:</a></li>
<li class="chapter" data-level="3.235" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#estimate-std.-error-z-value-prz-2"><i class="fa fa-check"></i><b>3.235</b> Estimate Std. Error z value Pr(&gt;|z|)</a></li>
<li class="chapter" data-level="3.236" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#intercept-46.8813-16.9631-2.764-0.005715"><i class="fa fa-check"></i><b>3.236</b> (Intercept) 46.8813 16.9631 2.764 0.005715 **</a></li>
<li><a href="supervised-learning-in-r-regression.html#total_length--0.5435-0.1409--3.858-0.000115-weight--0.5689-0.2771--2.053-0.040060-humerus-75.4610-19.1586-3.939-8.19e-05-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-binomial-family-taken-to-be-1-null-deviance-118.008-on-86-degrees-of-freedom-residual-deviance-75.094-on-83-degrees-of-freedom-aic-83.094-number-of-fisher-scoring-iterations-5-null.deviance-df.null-loglik-aic-bic-deviance-df.residual-1-118.0084-86--37.54718-83.09436-92.95799-75.09436-83-1-0.3636526-status-age-total_length-wingspan-perished36-length87-min.-153.0-min.-236.0-survived51-class-character-1st-qu.158.0-1st-qu.245.0-mode-character-median-160.0-median-247.0-mean-160.4-mean-247.5-3rd-qu.162.5-3rd-qu.251.0-max.-167.0-max.-256.0-weight-beak_head-humerus-femur-min.-23.2-min.-29.80-min.-0.6600-min.-0.6500-1st-qu.24.7-1st-qu.31.40-1st-qu.0.7250-1st-qu.0.7000-median-25.8-median-31.70-median-0.7400-median-0.7100-mean-25.8-mean-31.64-mean-0.7353-mean-0.7134-3rd-qu.26.7-3rd-qu.32.10-3rd-qu.0.7500-3rd-qu.0.7300-max.-31.0-max.-33.00-max.-0.7800-max.-0.7600-legbone-skull-sternum-survived-min.-1.010-min.-0.5600-min.-0.7700-mode-logical-1st-qu.1.110-1st-qu.0.5900-1st-qu.0.8300-false36-median-1.130-median-0.6000-median-0.8500-true-51-mean-1.131-mean-0.6032-mean-0.8511-3rd-qu.1.160-3rd-qu.0.6100-3rd-qu.0.8800-max.-1.230-max.-0.6400-max.-0.9300-call-glmformula-fmla-family-binomial-data-sparrow-deviance-residuals-min-1q-median-3q-max--2.1117--0.6026-0.2871-0.6577-1.7082-coefficients-estimate-std.-error-z-value-prz-intercept-46.8813-16.9631-2.764-0.005715-total_length--0.5435-0.1409--3.858-0.000115"><span class="toc-section-number">3.237</span> total_length -0.5435 0.1409 -3.858 0.000115 <strong><em> ## weight -0.5689 0.2771 -2.053 0.040060 </em><br />
## humerus 75.4610 19.1586 3.939 8.19e-05 </strong><em> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 118.008 on 86 degrees of freedom ## Residual deviance: 75.094 on 83 degrees of freedom ## AIC: 83.094 ## ## Number of Fisher Scoring iterations: 5 ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 118.0084 86 -37.54718 83.09436 92.95799 75.09436 83 ## [1] 0.3636526 ## status age total_length wingspan<br />
## Perished:36 Length:87 Min. :153.0 Min. :236.0<br />
## Survived:51 Class :character 1st Qu.:158.0 1st Qu.:245.0<br />
## Mode :character Median :160.0 Median :247.0<br />
## Mean :160.4 Mean :247.5<br />
## 3rd Qu.:162.5 3rd Qu.:251.0<br />
## Max. :167.0 Max. :256.0<br />
## weight beak_head humerus femur<br />
## Min. :23.2 Min. :29.80 Min. :0.6600 Min. :0.6500<br />
## 1st Qu.:24.7 1st Qu.:31.40 1st Qu.:0.7250 1st Qu.:0.7000<br />
## Median :25.8 Median :31.70 Median :0.7400 Median :0.7100<br />
## Mean :25.8 Mean :31.64 Mean :0.7353 Mean :0.7134<br />
## 3rd Qu.:26.7 3rd Qu.:32.10 3rd Qu.:0.7500 3rd Qu.:0.7300<br />
## Max. :31.0 Max. :33.00 Max. :0.7800 Max. :0.7600<br />
## legbone skull sternum survived<br />
## Min. :1.010 Min. :0.5600 Min. :0.7700 Mode :logical<br />
## 1st Qu.:1.110 1st Qu.:0.5900 1st Qu.:0.8300 FALSE:36<br />
## Median :1.130 Median :0.6000 Median :0.8500 TRUE :51<br />
## Mean :1.131 Mean :0.6032 Mean :0.8511<br />
## 3rd Qu.:1.160 3rd Qu.:0.6100 3rd Qu.:0.8800<br />
## Max. :1.230 Max. :0.6400 Max. :0.9300 ## ## Call: ## glm(formula = fmla, family = binomial, data = sparrow) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max<br />
## -2.1117 -0.6026 0.2871 0.6577 1.7082<br />
## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|)<br />
## (Intercept) 46.8813 16.9631 2.764 0.005715 <strong> ## total_length -0.5435 0.1409 -3.858 0.000115 </strong></em></a></li>
<li><a href="supervised-learning-in-r-regression.html#weight--0.5689-0.2771--2.053-0.040060-humerus-75.4610-19.1586-3.939-8.19e-05-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-binomial-family-taken-to-be-1-null-deviance-118.008-on-86-degrees-of-freedom-residual-deviance-75.094-on-83-degrees-of-freedom-aic-83.094-number-of-fisher-scoring-iterations-5-count-data-with-poisson-and-quasipoisson-regression-data.frame-744-obs.-of-12-variables-hr-factor-w-24-levels-0123..-1-2-3-4-5-6-7-8-9-10-holiday-logi-false-false-false-false-false-false-workingday-logi-false-false-false-false-false-false-weathersit-chr-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-temp-num-0.76-0.74-0.72-0.72-0.7-0.68-0.7-0.74-0.78-0.82-atemp-num-0.727-0.697-0.697-0.712-0.667-hum-num-0.66-0.7-0.74-0.84-0.79-0.79-0.79-0.7-0.62-0.56-windspeed-num-0-0.1343-0.0896-0.1343-0.194-cnt-int-149-93-90-33-4-10-27-50-142-219-instant-int-13004-13005-13006-13007-13008-13009-13010-13011-13012-13013-mnth-int-7-7-7-7-7-7-7-7-7-7-yr-int-1-1-1-1-1-1-1-1-1-1-1-cnt-1-hr-holiday-workingday-weathersit-temp-6-atemp-hum-windspeed-1-cnt-hr-holiday-workingday-weathersit-temp-atemp-hum-windspeed-1-273.6653-1-45863.84-null.deviance-df.null-loglik-aic-bic-deviance-df.residual-1-133364.9-743-na-na-na-28774.9-712-1-0.7842393-call-glmformula-fmla-family-quasipoisson-data-bikesjuly-deviance-residuals-min-1q-median-3q-max--21.6117--4.3121--0.7223-3.5507-16.5079-coefficients-estimate-std.-error-t-value-prt-intercept-5.934986-0.439027-13.519-2e-16"><span class="toc-section-number">3.238</span> weight -0.5689 0.2771 -2.053 0.040060 *<br />
## humerus 75.4610 19.1586 3.939 8.19e-05 <strong><em> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 118.008 on 86 degrees of freedom ## Residual deviance: 75.094 on 83 degrees of freedom ## AIC: 83.094 ## ## Number of Fisher Scoring iterations: 5 ### Count data with poisson and quasipoisson regression ## ‘data.frame’: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels “0”,“1”,“2”,“3”,..: 1 2 3 4 5 6 7 8 9 10 … ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE … ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE … ## $ weathersit: chr “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” … ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 … ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 … ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 … ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 … ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 … ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 … ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 … ## $ yr : int 1 1 1 1 1 1 1 1 1 1 … ## [1] “cnt” ## [1] “hr” “holiday” “workingday” “weathersit” “temp”<br />
## [6] “atemp” “hum” “windspeed” ## [1] “cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed” ## [1] 273.6653 ## [1] 45863.84 ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 133364.9 743 NA NA NA 28774.9 712 ## [1] 0.7842393 ## ## Call: ## glm(formula = fmla, family = quasipoisson, data = bikesJuly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max<br />
## -21.6117 -4.3121 -0.7223 3.5507 16.5079<br />
## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|)<br />
## (Intercept) 5.934986 0.439027 13.519 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.239" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr1--0.580055-0.193354--3.000-0.002794"><i class="fa fa-check"></i><b>3.239</b> hr1 -0.580055 0.193354 -3.000 0.002794 **</a></li>
<li class="chapter" data-level="3.240" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr2--0.892314-0.215452--4.142-3.86e-05-hr3--1.662342-0.290658--5.719-1.58e-08"><i class="fa fa-check"></i><b>3.240</b> hr2 -0.892314 0.215452 -4.142 3.86e-05 <strong><em> ## hr3 -1.662342 0.290658 -5.719 1.58e-08 </em></strong></a></li>
<li class="chapter" data-level="3.241" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr4--2.350204-0.393560--5.972-3.71e-09-hr5--1.084289-0.230130--4.712-2.96e-06"><i class="fa fa-check"></i><b>3.241</b> hr4 -2.350204 0.393560 -5.972 3.71e-09 <strong><em> ## hr5 -1.084289 0.230130 -4.712 2.96e-06 </em></strong></a></li>
<li class="chapter" data-level="3.242" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr6-0.211945-0.156476-1.354-0.176012"><i class="fa fa-check"></i><b>3.242</b> hr6 0.211945 0.156476 1.354 0.176012</a></li>
<li class="chapter" data-level="3.243" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr7-1.211135-0.132332-9.152-2e-16-hr8-1.648361-0.127177-12.961-2e-16"><i class="fa fa-check"></i><b>3.243</b> hr7 1.211135 0.132332 9.152 &lt; 2e-16 <strong><em> ## hr8 1.648361 0.127177 12.961 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.244" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr9-1.155669-0.133927-8.629-2e-16-hr10-0.993913-0.137096-7.250-1.09e-12"><i class="fa fa-check"></i><b>3.244</b> hr9 1.155669 0.133927 8.629 &lt; 2e-16 <strong><em> ## hr10 0.993913 0.137096 7.250 1.09e-12 </em></strong></a></li>
<li class="chapter" data-level="3.245" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr11-1.116547-0.136300-8.192-1.19e-15-hr12-1.282685-0.134769-9.518-2e-16"><i class="fa fa-check"></i><b>3.245</b> hr11 1.116547 0.136300 8.192 1.19e-15 <strong><em> ## hr12 1.282685 0.134769 9.518 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.246" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr13-1.273010-0.135872-9.369-2e-16-hr14-1.237721-0.136386-9.075-2e-16"><i class="fa fa-check"></i><b>3.246</b> hr13 1.273010 0.135872 9.369 &lt; 2e-16 <strong><em> ## hr14 1.237721 0.136386 9.075 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.247" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr15-1.260647-0.136144-9.260-2e-16-hr16-1.515893-0.132727-11.421-2e-16"><i class="fa fa-check"></i><b>3.247</b> hr15 1.260647 0.136144 9.260 &lt; 2e-16 <strong><em> ## hr16 1.515893 0.132727 11.421 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.248" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr17-1.948404-0.128080-15.212-2e-16-hr18-1.893915-0.127812-14.818-2e-16"><i class="fa fa-check"></i><b>3.248</b> hr17 1.948404 0.128080 15.212 &lt; 2e-16 <strong><em> ## hr18 1.893915 0.127812 14.818 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.249" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr19-1.669277-0.128471-12.993-2e-16-hr20-1.420732-0.131004-10.845-2e-16"><i class="fa fa-check"></i><b>3.249</b> hr19 1.669277 0.128471 12.993 &lt; 2e-16 <strong><em> ## hr20 1.420732 0.131004 10.845 &lt; 2e-16 </em></strong></a></li>
<li class="chapter" data-level="3.250" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr21-1.146763-0.134042-8.555-2e-16-hr22-0.856182-0.138982-6.160-1.21e-09"><i class="fa fa-check"></i><b>3.250</b> hr21 1.146763 0.134042 8.555 &lt; 2e-16 <strong><em> ## hr22 0.856182 0.138982 6.160 1.21e-09 </em></strong></a></li>
<li class="chapter" data-level="3.251" data-path="supervised-learning-in-r-regression.html"><a href="supervised-learning-in-r-regression.html#hr23-0.479197-0.148051-3.237-0.001265"><i class="fa fa-check"></i><b>3.251</b> hr23 0.479197 0.148051 3.237 0.001265 **</a></li>
<li><a href="supervised-learning-in-r-regression.html#holidaytrue-0.201598-0.079039-2.551-0.010961-workingdaytrue-0.116798-0.033510-3.485-0.000521-weathersitlight-precipitation--0.214801-0.072699--2.955-0.003233-weathersitmisty--0.010757-0.038600--0.279-0.780572-temp--3.246001-1.148270--2.827-0.004833-atemp-2.042314-0.953772-2.141-0.032589-hum--0.748557-0.236015--3.172-0.001581-windspeed-0.003277-0.148814-0.022-0.982439-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-dispersion-parameter-for-quasipoisson-family-taken-to-be-38.98949-null-deviance-133365-on-743-degrees-of-freedom-residual-deviance-28775-on-712-degrees-of-freedom-aic-na-number-of-fisher-scoring-iterations-5-rmse-1-112.5815-generalised-additive-model-gam-plot-variety-year-time-weight-1988f6-10-f161-1988124-min.-14.00-min.-0.0290-1988f7-9-p169-1989102-1st-qu.27.00-1st-qu.-0.6663-1988p1-9-1990104-median-42.00-median-3.5233-1988p8-9-mean-43.56-mean-6.1645-1988p2-9-3rd-qu.56.00-3rd-qu.10.3808-1988f3-8-max.-84.00-max.-27.3700-other276-loading-required-package-nlme-attaching-package-nlme-the-following-object-is-masked-from-packagedplyr-collapse-this-is-mgcv-1.8-20.-for-overview-type-helpmgcv-package.-weight-stime-weight-time-call-lmformula-fmla.lin-data-soybean_train-residuals-min-1q-median-3q-max--9.3933--1.7100--0.3909-1.9056-11.4381-coefficients-estimate-std.-error-t-value-prt-intercept--6.559283-0.358527--18.30-2e-16-time-0.292094-0.007444-39.24-2e-16-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-residual-standard-error-2.778-on-328-degrees-of-freedom-multiple-r-squared-0.8244-adjusted-r-squared-0.8238-f-statistic-1540-on-1-and-328-df-p-value-2.2e-16-family-gaussian-link-function-identity-formula-weight-stime-parametric-coefficients-estimate-std.-error-t-value-prt-intercept-6.1645-0.1143-53.93-2e-16-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-approximate-significance-of-smooth-terms-edf-ref.df-f-p-value-stime-8.495-8.93-338.2-2e-16-signif.-codes-0-0.001-0.01-0.05-.-0.1-1-r-sq.adj-0.902-deviance-explained-90.4-gcv-4.4395-scale-est.-4.3117-n-330-plot-variety-year-time-weight-1988f8-4-f43-198832-min.-14.00-min.-0.0380-1988p7-4-p39-198926-1st-qu.23.00-1st-qu.-0.4248-1989f8-4-199024-median-41.00-median-3.0025-1990f8-4-mean-44.09-mean-7.1576-1988f4-3-3rd-qu.69.00-3rd-qu.15.0113-1988f2-3-max.-84.00-max.-30.2717-other60-a-tibble-2-x-2-modeltype-rmse-1-pred.gam-2.286451-2-pred.lin-3.190995-tree-based-methods-random-forests-data.frame-744-obs.-of-12-variables-hr-factor-w-24-levels-0123..-1-2-3-4-5-6-7-8-9-10-holiday-logi-false-false-false-false-false-false-workingday-logi-false-false-false-false-false-false-weathersit-chr-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-temp-num-0.76-0.74-0.72-0.72-0.7-0.68-0.7-0.74-0.78-0.82-atemp-num-0.727-0.697-0.697-0.712-0.667-hum-num-0.66-0.7-0.74-0.84-0.79-0.79-0.79-0.7-0.62-0.56-windspeed-num-0-0.1343-0.0896-0.1343-0.194-cnt-int-149-93-90-33-4-10-27-50-142-219-instant-int-13004-13005-13006-13007-13008-13009-13010-13011-13012-13013-mnth-int-7-7-7-7-7-7-7-7-7-7-yr-int-1-1-1-1-1-1-1-1-1-1-1-cnt-1-hr-holiday-workingday-weathersit-temp-6-atemp-hum-windspeed-1-cnt-hr-holiday-workingday-weathersit-temp-atemp-hum-windspeed-ranger-result-call-rangerfmla-bikesjuly-num.trees-500-respect.unordered.factors-order-seed-seed-type-regression-number-of-trees-500-sample-size-744-number-of-independent-variables-8-mtry-2-target-node-size-5-variable-importance-mode-none-oob-prediction-error-mse-8230.568-r-squared-oob-0.8205434-data.frame-744-obs.-of-13-variables-hr-factor-w-24-levels-0123..-1-2-3-4-5-6-7-8-9-10-holiday-logi-false-false-false-false-false-false-workingday-logi-true-true-true-true-true-true-weathersit-chr-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-clear-to-partly-cloudy-temp-num-0.68-0.66-0.64-0.64-0.64-0.64-0.64-0.64-0.66-0.68-atemp-num-0.636-0.606-0.576-0.576-0.591-hum-num-0.79-0.83-0.83-0.83-0.78-0.78-0.78-0.83-0.78-0.74-windspeed-num-0.1642-0.0896-0.1045-0.1045-0.1343-cnt-int-47-33-13-7-4-49-185-487-681-350-instant-int-13748-13749-13750-13751-13752-13753-13754-13755-13756-13757-mnth-int-8-8-8-8-8-8-8-8-8-8-yr-int-1-1-1-1-1-1-1-1-1-1-pred-num-94.96-51.74-37.98-17.58-9.36-ranger-result-call-rangerfmla-bikesjuly-num.trees-500-respect.unordered.factors-order-seed-seed-type-regression-number-of-trees-500-sample-size-744-number-of-independent-variables-8-mtry-2-target-node-size-5-variable-importance-mode-none-oob-prediction-error-mse-8230.568-r-squared-oob-0.8205434-rmse-1-97.18347-one-hot-encoding-color-size-popularity-1-b-13-1.0785088-2-r-11-1.3956245-3-r-15-0.9217988-4-r-14-1.2025453-5-r-13-1.0838662-6-b-11-0.8043527-7-r-9-1.103544-8-g-12-0.8746332-9-b-7-0.6947058-10-b-12-0.8832502-1-color-size-1-designing-treatments-wed-apr-25-043123-2018-1-designing-treatments-wed-apr-25-043123-2018-1-have-level-statistics-wed-apr-25-043123-2018-1-design-var-color-wed-apr-25-043123-2018-1-design-var-size-wed-apr-25-043123-2018-1-scoring-treatments-wed-apr-25-043123-2018-1-have-treatment-plan-wed-apr-25-043123-2018-varname-origname-code-1-color_lev_x.b-color-lev-2-color_lev_x.g-color-lev-3-color_lev_x.r-color-lev-4-color_catp-color-catp-5-size_lev_x.11-size-lev-6-size_lev_x.12-size-lev-7-size_lev_x.13-size-lev-8-size_lev_x.14-size-lev-9-size_lev_x.15-size-lev-10-size_lev_x.7-size-lev-11-size_lev_x.9-size-lev-12-size_catp-size-catp-1-color_lev_x.b-color_lev_x.g-color_lev_x.r-size_lev_x.11-5-size_lev_x.12-size_lev_x.13-size_lev_x.14-size_lev_x.15-9-size_lev_x.7-size_lev_x.9-color_lev_x.b-color_lev_x.g-color_lev_x.r-size_lev_x.11-size_lev_x.12-1-1-0-0-0-0-2-0-0-1-1-0-3-0-0-1-0-0-4-0-0-1-0-0-5-0-0-1-0-0-6-1-0-0-1-0-7-0-0-1-0-0-8-0-1-0-0-1-9-1-0-0-0-0-10-1-0-0-0-1-size_lev_x.13-size_lev_x.14-size_lev_x.15-size_lev_x.7-size_lev_x.9-1-1-0-0-0-0-2-0-0-0-0-0-3-0-0-1-0-0-4-0-1-0-0-0-5-1-0-0-0-0-6-0-0-0-0-0-7-0-0-0-0-1-8-0-0-0-0-0-9-0-0-0-1-0-10-0-0-0-0-0-length-class-mode-treatments-4--none--list-scoreframe-8-data.frame-list-outcomename-1--none--character-vtreatversion-1-package_version-list-outcometype-1--none--character-outcometarget-1--none--character-meany-1--none--logical-splitmethod-1--none--character-1-color_lev_x.b-color_lev_x.g-color_lev_x.r-size_lev_x.11-5-size_lev_x.12-size_lev_x.13-size_lev_x.14-size_lev_x.15-9-size_lev_x.7-size_lev_x.9-color-size-popularity-1-b-13-1.0785088-2-r-11-1.3956245-3-r-15-0.9217988-4-r-14-1.2025453-5-r-13-1.0838662-6-b-11-0.8043527-7-r-9-1.103544-8-g-12-0.8746332-9-b-7-0.6947058-10-b-12-0.8832502-color-size-popularity-1-b-13-1.0785088-2-r-11-1.3956245-3-r-15-0.9217988-4-r-14-1.2025453-5-r-13-1.0838662-6-b-11-0.8043527-7-r-9-1.103544-8-g-12-0.8746332-9-b-7-0.6947058-10-b-12-0.8832502-color_lev_x.b-color_lev_x.g-color_lev_x.r-size_lev_x.11-size_lev_x.12-1-1-0-0-0-0-2-0-0-1-1-0-3-0-0-1-0-0-4-0-0-1-0-0-5-0-0-1-0-0-6-1-0-0-1-0-7-0-0-1-0-0-8-0-1-0-0-1-9-1-0-0-0-0-10-1-0-0-0-1-size_lev_x.13-size_lev_x.14-size_lev_x.15-size_lev_x.7-size_lev_x.9-1-1-0-0-0-0-2-0-0-0-0-0-3-0-0-1-0-0-4-0-1-0-0-0-5-1-0-0-0-0-6-0-0-0-0-0-7-0-0-0-0-1-8-0-0-0-0-0-9-0-0-0-1-0-10-0-0-0-0-0-1-cnt-1-hr-holiday-workingday-weathersit-temp-6-atemp-hum-windspeed-1-hr_lev_x.0-2-hr_lev_x.1-3-hr_lev_x.10-4-hr_lev_x.11-5-hr_lev_x.12-6-hr_lev_x.13-7-hr_lev_x.14-8-hr_lev_x.15-9-hr_lev_x.16-10-hr_lev_x.17-11-hr_lev_x.18-12-hr_lev_x.19-13-hr_lev_x.2-14-hr_lev_x.20-15-hr_lev_x.21-16-hr_lev_x.22-17-hr_lev_x.23-18-hr_lev_x.3-19-hr_lev_x.4-20-hr_lev_x.5-21-hr_lev_x.6-22-hr_lev_x.7-23-hr_lev_x.8-24-hr_lev_x.9-25-holiday_clean-26-workingday_clean-27-weathersit_lev_x.clear.to.partly.cloudy-28-weathersit_lev_x.light.precipitation-29-weathersit_lev_x.misty-30-temp_clean-31-atemp_clean-32-hum_clean-33-windspeed_clean-data.frame-744-obs.-of-33-variables-hr_lev_x.0-num-1-0-0-0-0-0-0-0-0-0-hr_lev_x.1-num-0-1-0-0-0-0-0-0-0-0-hr_lev_x.10-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.11-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.12-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.13-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.14-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.15-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.16-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.17-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.18-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.19-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.2-num-0-0-1-0-0-0-0-0-0-0-hr_lev_x.20-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.21-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.22-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.23-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.3-num-0-0-0-1-0-0-0-0-0-0-hr_lev_x.4-num-0-0-0-0-1-0-0-0-0-0-hr_lev_x.5-num-0-0-0-0-0-1-0-0-0-0-hr_lev_x.6-num-0-0-0-0-0-0-1-0-0-0-hr_lev_x.7-num-0-0-0-0-0-0-0-1-0-0-hr_lev_x.8-num-0-0-0-0-0-0-0-0-1-0-hr_lev_x.9-num-0-0-0-0-0-0-0-0-0-1-holiday_clean-num-0-0-0-0-0-0-0-0-0-0-workingday_clean-num-1-1-1-1-1-1-1-1-1-1-weathersit_lev_x.clear.to.partly.cloudy-num-1-1-1-1-0-0-1-0-0-0-weathersit_lev_x.light.precipitation-num-0-0-0-0-0-0-0-0-0-0-weathersit_lev_x.misty-num-0-0-0-0-1-1-0-1-1-1-temp_clean-num-0.68-0.66-0.64-0.64-0.64-0.64-0.64-0.64-0.66-0.68-atemp_clean-num-0.636-0.606-0.576-0.576-0.591-hum_clean-num-0.79-0.83-0.83-0.83-0.78-0.78-0.78-0.83-0.78-0.74-windspeed_clean-num-0.1642-0.0896-0.1045-0.1045-0.1343-data.frame-744-obs.-of-33-variables-hr_lev_x.0-num-1-0-0-0-0-0-0-0-0-0-hr_lev_x.1-num-0-1-0-0-0-0-0-0-0-0-hr_lev_x.10-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.11-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.12-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.13-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.14-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.15-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.16-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.17-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.18-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.19-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.2-num-0-0-1-0-0-0-0-0-0-0-hr_lev_x.20-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.21-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.22-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.23-num-0-0-0-0-0-0-0-0-0-0-hr_lev_x.3-num-0-0-0-1-0-0-0-0-0-0-hr_lev_x.4-num-0-0-0-0-1-0-0-0-0-0-hr_lev_x.5-num-0-0-0-0-0-1-0-0-0-0-hr_lev_x.6-num-0-0-0-0-0-0-1-0-0-0-hr_lev_x.7-num-0-0-0-0-0-0-0-1-0-0-hr_lev_x.8-num-0-0-0-0-0-0-0-0-1-0-hr_lev_x.9-num-0-0-0-0-0-0-0-0-0-1-holiday_clean-num-0-0-0-0-0-0-0-0-0-0-workingday_clean-num-0-0-0-0-0-0-0-0-0-0-weathersit_lev_x.clear.to.partly.cloudy-num-1-1-1-1-1-1-1-1-1-1-weathersit_lev_x.light.precipitation-num-0-0-0-0-0-0-0-0-0-0-weathersit_lev_x.misty-num-0-0-0-0-0-0-0-0-0-0-temp_clean-num-0.76-0.74-0.72-0.72-0.7-0.68-0.7-0.74-0.78-0.82-atemp_clean-num-0.727-0.697-0.697-0.712-0.667-hum_clean-num-0.66-0.7-0.74-0.84-0.79-0.79-0.79-0.7-0.62-0.56-windspeed_clean-num-0-0.1343-0.0896-0.1343-0.194-gradient-boosting-machines-1-bike_model-bike_model_rf-bikesaugust-4-bikesaugust.treat-bikesjuly-bikesjuly.treat-7-bloodpressure-bloodpressure_model-color-10-dframe-dframe.treat-fdata-13-fdata2-fe_mean-flower_model-16-flowers-fmla-fmla.gam-19-fmla.lin-fmla.log-fmla_sqr-22-gp-houseprice-houseprice_long-25-i-incometest-incometrain-28-k-mean_bikes-mmat-31-model-model.gam-model.lin-34-model.log-model_lin-model_sqr-37-mpg-mpg_model-mpg_test-40-mpg_train-n-newrates-43-newvars-nrows-outcome-46-perf-popularity-pred-49-pseudor2-quasipoissonmodel-randomforest_plot-52-res-rho-rho2-55-rmse-rmse_test-rmse_train-58-rsq-rsq_glance-rsq_test-61-rsq_train-rss-scoreframe-64-sd_unemployment-seed-size-67-soybean_long-soybean_test-soybean_train-70-sparrow-sparrow_model-split-73-splitplan-target-testframe-76-testframe.treat-treatplan-tss-79-unemployment-unemployment_model-var_bikes-82-vars-attaching-package-xgboost-the-following-object-is-masked-from-packagedplyr-slice-ntrees.train-ntrees.test-1-77-67-rmse-1-76.36407"><span class="toc-section-number">3.252</span> holidayTRUE 0.201598 0.079039 2.551 0.010961 *<br />
## workingdayTRUE 0.116798 0.033510 3.485 0.000521 <em><strong> ## weathersitLight Precipitation -0.214801 0.072699 -2.955 0.003233 </strong> ## weathersitMisty -0.010757 0.038600 -0.279 0.780572<br />
## temp -3.246001 1.148270 -2.827 0.004833 <strong> ## atemp 2.042314 0.953772 2.141 0.032589 *<br />
## hum -0.748557 0.236015 -3.172 0.001581 </strong> ## windspeed 0.003277 0.148814 0.022 0.982439<br />
## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## (Dispersion parameter for quasipoisson family taken to be 38.98949) ## ## Null deviance: 133365 on 743 degrees of freedom ## Residual deviance: 28775 on 712 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 ## rmse ## 1 112.5815 ### Generalised Additive Model (GAM) ## Plot Variety Year Time weight<br />
## 1988F6 : 10 F:161 1988:124 Min. :14.00 Min. : 0.0290<br />
## 1988F7 : 9 P:169 1989:102 1st Qu.:27.00 1st Qu.: 0.6663<br />
## 1988P1 : 9 1990:104 Median :42.00 Median : 3.5233<br />
## 1988P8 : 9 Mean :43.56 Mean : 6.1645<br />
## 1988P2 : 9 3rd Qu.:56.00 3rd Qu.:10.3808<br />
## 1988F3 : 8 Max. :84.00 Max. :27.3700<br />
## (Other):276 ## Loading required package: nlme ## ## Attaching package: ‘nlme’ ## The following object is masked from ‘package:dplyr’: ## ## collapse ## This is mgcv 1.8-20. For overview type ‘help(“mgcv-package”)’. ## weight ~ s(Time) ## weight ~ Time ## ## Call: ## lm(formula = fmla.lin, data = soybean_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3933 -1.7100 -0.3909 1.9056 11.4381 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|)<br />
## (Intercept) -6.559283 0.358527 -18.30 &lt;2e-16 </em><strong> ## Time 0.292094 0.007444 39.24 &lt;2e-16 </strong><em> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## Residual standard error: 2.778 on 328 degrees of freedom ## Multiple R-squared: 0.8244, Adjusted R-squared: 0.8238 ## F-statistic: 1540 on 1 and 328 DF, p-value: &lt; 2.2e-16 ## ## Family: gaussian ## Link function: identity ## ## Formula: ## weight ~ s(Time) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|)<br />
## (Intercept) 6.1645 0.1143 53.93 &lt;2e-16 </em><strong> ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value<br />
## s(Time) 8.495 8.93 338.2 &lt;2e-16 </strong>* ## — ## Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ‘’ 1 ## ## R-sq.(adj) = 0.902 Deviance explained = 90.4% ## GCV = 4.4395 Scale est. = 4.3117 n = 330 ## Plot Variety Year Time weight<br />
## 1988F8 : 4 F:43 1988:32 Min. :14.00 Min. : 0.0380<br />
## 1988P7 : 4 P:39 1989:26 1st Qu.:23.00 1st Qu.: 0.4248<br />
## 1989F8 : 4 1990:24 Median :41.00 Median : 3.0025<br />
## 1990F8 : 4 Mean :44.09 Mean : 7.1576<br />
## 1988F4 : 3 3rd Qu.:69.00 3rd Qu.:15.0113<br />
## 1988F2 : 3 Max. :84.00 Max. :30.2717<br />
## (Other):60 ## # A tibble: 2 x 2 ## modeltype rmse ## <chr> <dbl> ## 1 pred.gam 2.286451 ## 2 pred.lin 3.190995 ## Tree-Based Methods ### Random Forests ## ‘data.frame’: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels “0”,“1”,“2”,“3”,..: 1 2 3 4 5 6 7 8 9 10 … ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE … ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE … ## $ weathersit: chr “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” … ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 … ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 … ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 … ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 … ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 … ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 … ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 … ## $ yr : int 1 1 1 1 1 1 1 1 1 1 … ## [1] “cnt” ## [1] “hr” “holiday” “workingday” “weathersit” “temp”<br />
## [6] “atemp” “hum” “windspeed” ## [1] “cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed” ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = “order”, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 ## ‘data.frame’: 744 obs. of 13 variables: ## $ hr : Factor w/ 24 levels “0”,“1”,“2”,“3”,..: 1 2 3 4 5 6 7 8 9 10 … ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE … ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE … ## $ weathersit: chr “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” “Clear to partly cloudy” … ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 … ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 … ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 … ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 … ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 … ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 … ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 … ## $ yr : int 1 1 1 1 1 1 1 1 1 1 … ## $ pred : num 94.96 51.74 37.98 17.58 9.36 … ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = “order”, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 ## rmse ## 1 97.18347 ### One-hot encoding ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 ## [1] “color” “size” ## [1] “designing treatments Wed Apr 25 04:31:23 2018” ## [1] “designing treatments Wed Apr 25 04:31:23 2018” ## [1] &quot; have level statistics Wed Apr 25 04:31:23 2018&quot; ## [1] “design var color Wed Apr 25 04:31:23 2018” ## [1] “design var size Wed Apr 25 04:31:23 2018” ## [1] &quot; scoring treatments Wed Apr 25 04:31:23 2018&quot; ## [1] “have treatment plan Wed Apr 25 04:31:23 2018” ## varName origName code ## 1 color_lev_x.b color lev ## 2 color_lev_x.g color lev ## 3 color_lev_x.r color lev ## 4 color_catP color catP ## 5 size_lev_x.11 size lev ## 6 size_lev_x.12 size lev ## 7 size_lev_x.13 size lev ## 8 size_lev_x.14 size lev ## 9 size_lev_x.15 size lev ## 10 size_lev_x.7 size lev ## 11 size_lev_x.9 size lev ## 12 size_catP size catP ## [1] “color_lev_x.b” “color_lev_x.g” “color_lev_x.r” “size_lev_x.11” ## [5] “size_lev_x.12” “size_lev_x.13” “size_lev_x.14” “size_lev_x.15” ## [9] “size_lev_x.7” “size_lev_x.9” ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 ## Length Class Mode<br />
## treatments 4 -none- list<br />
## scoreFrame 8 data.frame list<br />
## outcomename 1 -none- character ## vtreatVersion 1 package_version list<br />
## outcomeType 1 -none- character ## outcomeTarget 1 -none- character ## meanY 1 -none- logical<br />
## splitmethod 1 -none- character ## [1] “color_lev_x.b” “color_lev_x.g” “color_lev_x.r” “size_lev_x.11” ## [5] “size_lev_x.12” “size_lev_x.13” “size_lev_x.14” “size_lev_x.15” ## [9] “size_lev_x.7” “size_lev_x.9” ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 ## [1] “cnt” ## [1] “hr” “holiday” “workingday” “weathersit” “temp”<br />
## [6] “atemp” “hum” “windspeed” ## [1] “hr_lev_x.0”<br />
## [2] “hr_lev_x.1”<br />
## [3] “hr_lev_x.10”<br />
## [4] “hr_lev_x.11”<br />
## [5] “hr_lev_x.12”<br />
## [6] “hr_lev_x.13”<br />
## [7] “hr_lev_x.14”<br />
## [8] “hr_lev_x.15”<br />
## [9] “hr_lev_x.16”<br />
## [10] “hr_lev_x.17”<br />
## [11] “hr_lev_x.18”<br />
## [12] “hr_lev_x.19”<br />
## [13] “hr_lev_x.2”<br />
## [14] “hr_lev_x.20”<br />
## [15] “hr_lev_x.21”<br />
## [16] “hr_lev_x.22”<br />
## [17] “hr_lev_x.23”<br />
## [18] “hr_lev_x.3”<br />
## [19] “hr_lev_x.4”<br />
## [20] “hr_lev_x.5”<br />
## [21] “hr_lev_x.6”<br />
## [22] “hr_lev_x.7”<br />
## [23] “hr_lev_x.8”<br />
## [24] “hr_lev_x.9”<br />
## [25] “holiday_clean”<br />
## [26] “workingday_clean”<br />
## [27] “weathersit_lev_x.Clear.to.partly.cloudy” ## [28] “weathersit_lev_x.Light.Precipitation”<br />
## [29] “weathersit_lev_x.Misty”<br />
## [30] “temp_clean”<br />
## [31] “atemp_clean”<br />
## [32] “hum_clean”<br />
## [33] “windspeed_clean” ## ‘data.frame’: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 … ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 … ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 … ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 … ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 … ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 … ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 … ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 … ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 … ## $ workingday_clean : num 1 1 1 1 1 1 1 1 1 1 … ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 0 0 1 0 0 0 … ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 … ## $ weathersit_lev_x.Misty : num 0 0 0 0 1 1 0 1 1 1 … ## $ temp_clean : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 … ## $ atemp_clean : num 0.636 0.606 0.576 0.576 0.591 … ## $ hum_clean : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 … ## $ windspeed_clean : num 0.1642 0.0896 0.1045 0.1045 0.1343 … ## ‘data.frame’: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 … ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 … ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 … ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 … ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 … ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 … ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 … ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 … ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 … ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 … ## $ workingday_clean : num 0 0 0 0 0 0 0 0 0 0 … ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 1 1 1 1 1 1 … ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 … ## $ weathersit_lev_x.Misty : num 0 0 0 0 0 0 0 0 0 0 … ## $ temp_clean : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 … ## $ atemp_clean : num 0.727 0.697 0.697 0.712 0.667 … ## $ hum_clean : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 … ## $ windspeed_clean : num 0 0.1343 0.0896 0.1343 0.194 … ### Gradient Boosting Machines ## [1] “bike_model” “bike_model_rf” “bikesAugust”<br />
## [4] “bikesAugust.treat” “bikesJuly” “bikesJuly.treat”<br />
## [7] “bloodpressure” “bloodpressure_model” “color”<br />
## [10] “dframe” “dframe.treat” “fdata”<br />
## [13] “fdata2” “fe_mean” “flower_model”<br />
## [16] “flowers” “fmla” “fmla.gam”<br />
## [19] “fmla.lin” “fmla.log” “fmla_sqr”<br />
## [22] “gp” “houseprice” “houseprice_long”<br />
## [25] “i” “incometest” “incometrain”<br />
## [28] “k” “mean_bikes” “mmat”<br />
## [31] “model” “model.gam” “model.lin”<br />
## [34] “model.log” “model_lin” “model_sqr”<br />
## [37] “mpg” “mpg_model” “mpg_test”<br />
## [40] “mpg_train” “N” “newrates”<br />
## [43] “newvars” “nRows” “outcome”<br />
## [46] “perf” “popularity” “pred”<br />
## [49] “pseudoR2” “Quasipoissonmodel” “randomforest_plot”<br />
## [52] “res” “rho” “rho2”<br />
## [55] “rmse” “rmse_test” “rmse_train”<br />
## [58] “rsq” “rsq_glance” “rsq_test”<br />
## [61] “rsq_train” “rss” “scoreFrame”<br />
## [64] “sd_unemployment” “seed” “size”<br />
## [67] “soybean_long” “soybean_test” “soybean_train”<br />
## [70] “sparrow” “sparrow_model” “split”<br />
## [73] “splitPlan” “target” “testframe”<br />
## [76] “testframe.treat” “treatplan” “tss”<br />
## [79] “unemployment” “unemployment_model” “var_bikes”<br />
## [82] “vars” ## ## Attaching package: ‘xgboost’ ## The following object is masked from ‘package:dplyr’: ## ## slice ## ntrees.train ntrees.test ## 1 77 67 ## rmse ## 1 76.36407</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html"><i class="fa fa-check"></i><b>4</b> Machine Learning Toolbox</a><ul>
<li class="chapter" data-level="4.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#regression-models-fitting-them-and-evaluating-their-performance"><i class="fa fa-check"></i><b>4.1</b> Regression models: fitting them and evaluating their performance</a><ul>
<li class="chapter" data-level="4.1.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#cross-validation"><i class="fa fa-check"></i><b>4.1.1</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#classification-models-fitting-them-and-evaluating-their-performance"><i class="fa fa-check"></i><b>4.2</b> Classification models: fitting them and evaluating their performance</a><ul>
<li class="chapter" data-level="4.2.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#confusion-matrix"><i class="fa fa-check"></i><b>4.2.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.2.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#the-roc-curve"><i class="fa fa-check"></i><b>4.2.2</b> The ROC Curve</a></li>
<li class="chapter" data-level="4.2.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#tuning-model-parameters-to-improve-performance"><i class="fa fa-check"></i><b>4.2.3</b> Tuning model parameters to improve performance</a></li>
<li class="chapter" data-level="4.2.4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#introducing-glmnet"><i class="fa fa-check"></i><b>4.2.4</b> Introducing glmnet</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#preprocessing-your-data"><i class="fa fa-check"></i><b>4.3</b> Preprocessing your data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#mutiple-pre-processing-steps"><i class="fa fa-check"></i><b>4.3.1</b> Mutiple pre-processing steps</a></li>
<li class="chapter" data-level="4.3.2" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#handling-low-information-predictors"><i class="fa fa-check"></i><b>4.3.2</b> Handling low-Information Predictors</a></li>
<li class="chapter" data-level="4.3.3" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#principle-components-analysis-pca"><i class="fa fa-check"></i><b>4.3.3</b> Principle Components Analysis (PCA)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="machine-learning-toolbox.html"><a href="machine-learning-toolbox.html#selecting-models-a-case-study-in-churn-prediction"><i class="fa fa-check"></i><b>4.4</b> Selecting models: a case study in churn prediction</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">BI Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-toolbox" class="section level1">
<h1><span class="header-section-number">4</span> Machine Learning Toolbox</h1>
<hr />
<p>Notes taken during/inspired by the DataCamp course ‘Machine Learning Toolbox’ by Zachary Deane-Mayer and Max Kuhn.</p>
<p><strong><em>Course Handouts</em></strong></p>
<ul>
<li><a href="./files/MLToolbox/ch1-pdf-slides.pdf">Part 1 - Regression models: fitting them and evaluating their performance</a></li>
<li><a href="./files/MLToolbox/ch2-pdf-slides.pdf">Part 2 - Classification models: fitting them and evaluating their performance</a></li>
<li><a href="./files/MLToolbox/ch3-pdf-slides.pdf">Part 3 - Tuning model parameters to improve performance</a></li>
<li><a href="./files/MLToolbox/ch4-pdf-slides.pdf">Part 4 - Preprocessing your data</a></li>
<li><a href="./files/MLToolbox/ch5-pdf-slides.pdf">Part 5 - Selecting models: a case study in churn prediction</a></li>
</ul>
<p><strong><em>Other useful links</em></strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=dB-JHhEJvQA">Max Kuhn: Applied Predictive Modeling NYC Talk</a></li>
<li><a href="https://www.youtube.com/watch?v=YVMlyOh_eyk">Data Chat - Interview With Max Kuhn</a></li>
<li><a href="https://topepo.github.io/caret/">The caret package website</a></li>
<li><a href="https://www.youtube.com/watch?v=2Mg8QD0F1dQ">Bagging graphical explanation</a></li>
<li><a href="https://www.youtube.com/watch?v=GM3CDQfQ4sw">Boosting graphical explanation</a></li>
<li><a href="https://www.youtube.com/watch?v=s3VmuVPfu0s">Gradient Boosting in Practice: a deep dive into xgboost</a></li>
</ul>
<div id="regression-models-fitting-them-and-evaluating-their-performance" class="section level2">
<h2><span class="header-section-number">4.1</span> Regression models: fitting them and evaluating their performance</h2>
<p>CAret has been developed by Max for over 10 years. It automates supervised machine learning (aka predictive modelling) is ML when you have a target variable or something specific you want to predict like species and churn, these could be classification or regression. We then use metrics to evaluate how accuratley our models predict on new data.</p>
<p>For linear regression we will use RMSE as our evaluation metric. Typically this is done on our training (in-sample) data, however this can be too optomistic and lead to over-fitting. It is better to calculate this out-of-sample using caret.</p>
<p>Next we will calculate in-sample RMSE for the diamonds dataset from ggplot2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the data</span>
diamonds &lt;-<span class="st"> </span>ggplot2<span class="op">::</span>diamonds

<span class="co"># Fit lm model: model</span>
model &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>., diamonds)

<span class="co"># Predict on full data: p</span>
p &lt;-<span class="st"> </span><span class="kw">predict</span>(model, diamonds)

<span class="co"># Compute errors: error</span>
error &lt;-<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>diamonds<span class="op">$</span>price

<span class="co"># Calculate in-sample RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">mean</span>(error <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 1129.843</code></pre>
<p>The course focuses on predictive accuracy, that is to say does the model perform well When presented with new data. The best way to answer this is to test the model on new data using test data. This mimics the real world, where you do not actually know the outcome. We simulate this with a test/train split.</p>
<p>One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set seed</span>
<span class="kw">set.seed</span>(<span class="dv">42</span>)

<span class="co"># Shuffle row indices: rows</span>
rows &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(diamonds))

<span class="co"># Randomly order data</span>
diamonds &lt;-<span class="st"> </span>diamonds[rows,]</code></pre></div>
<p>Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Determine row to split on: split</span>
split &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">nrow</span>(diamonds) <span class="op">*</span><span class="st"> </span>.<span class="dv">80</span>)

<span class="co"># Create train</span>
train &lt;-<span class="st"> </span>diamonds[<span class="dv">1</span><span class="op">:</span>split, ]

<span class="co"># Create test</span>
test &lt;-<span class="st"> </span>diamonds[(split <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(diamonds), ]</code></pre></div>
<p>Now that you have a randomly split training set and test set, you can use the lm() function as you did in the first exercise to fit a model to your training set, rather than the entire dataset. Recall that you can use the formula interface to the linear regression function to fit a model with a specified target variable using all other variables in the dataset as predictors:</p>
<blockquote>
<p>mod &lt;- lm(y ~ ., training_data)</p>
</blockquote>
<p>You can use the predict() function to make predictions from that model on new data. The new dataset must have all of the columns from the training data, but they can be in a different order with different values. Here, rather than re-predicting on the training set, you can predict on the test set, which you did not use for training the model. This will allow you to determine the out-of-sample error for the model in the next exercise:</p>
<blockquote>
<p>p &lt;- predict(model, new_data)</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit lm model on train: model</span>
model &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>., train)

<span class="co"># Predict on test: p</span>
p &lt;-<span class="st"> </span><span class="kw">predict</span>(model,test)</code></pre></div>
<p>Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute errors: error</span>
error &lt;-<span class="st"> </span>(p <span class="op">-</span><span class="st"> </span>test<span class="op">$</span>price)

<span class="co"># Calculate RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">mean</span>(error<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 1136.596</code></pre>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Cross-validation</h3>
<p>Using a simple text:train split can be tricky, particularly if there are some outliers in one of the two datasets. A better approach is to use multiple test sets and average out of sample error. One way of achieving this is cross-validation where we use folds, 10 folds would mean we split our data in to ten sections with a single observation only occuring once. Cross validation is only used to calculate error metrics (out of sample), we then start again using on the full data. With 10 folds, we therefore have 11 models to fit - the 10 re-sampled models plus the final model. An alterntive which is available in caret is the bootstrap, but in practice this yields similar results to cross-validation.</p>
<p>caret package makes this very easy to do:</p>
<blockquote>
<p>model &lt;- train(y ~ ., my_data)</p>
</blockquote>
<p>caret supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the trainControl() function, which you pass to the trControl argument in train():</p>
<blockquote>
<p>model &lt;- train( y ~ ., my_data, method = “lm”, trControl = trainControl( method = “cv”, number = 10, verboseIter = TRUE ) )</p>
</blockquote>
<p>It’s important to note that you pass the method for modeling to the main train() function and the method for cross-validation to the trainControl() function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;ggplot2&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     diamonds</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit lm model using 10-fold CV: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  price <span class="op">~</span><span class="st"> </span>., diamonds,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )
)</code></pre></div>
<pre><code>## + Fold01: intercept=TRUE 
## - Fold01: intercept=TRUE 
## + Fold02: intercept=TRUE 
## - Fold02: intercept=TRUE 
## + Fold03: intercept=TRUE 
## - Fold03: intercept=TRUE 
## + Fold04: intercept=TRUE 
## - Fold04: intercept=TRUE 
## + Fold05: intercept=TRUE 
## - Fold05: intercept=TRUE 
## + Fold06: intercept=TRUE 
## - Fold06: intercept=TRUE 
## + Fold07: intercept=TRUE 
## - Fold07: intercept=TRUE 
## + Fold08: intercept=TRUE 
## - Fold08: intercept=TRUE 
## + Fold09: intercept=TRUE 
## - Fold09: intercept=TRUE 
## + Fold10: intercept=TRUE 
## - Fold10: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
<span class="kw">print</span>(model)</code></pre></div>
<pre><code>## Linear Regression 
## 
## 53940 samples
##     9 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 48547, 48546, 48546, 48545, 48545, 48545, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1130.658  0.9197492  740.4646
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Next we use a different dataset - Boston house prices - then use a 5 fold cross validation in the trainControl.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Boston &lt;-<span class="st"> </span>MASS<span class="op">::</span>Boston

<span class="co"># Fit lm model using 5-fold CV: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  medv <span class="op">~</span><span class="st"> </span>., Boston,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )
)</code></pre></div>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
<span class="kw">print</span>(model)</code></pre></div>
<pre><code>## Linear Regression 
## 
## 506 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 405, 403, 405, 406, 405 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   4.794707  0.7290369  3.372915
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.</p>
<p>One of the awesome things about the train() function in caret is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model’s out-of-sample accuracy</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit lm model using 5 x 5-fold CV: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  medv <span class="op">~</span><span class="st"> </span>., Boston,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">repeats =</span> <span class="dv">5</span>, <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )
)</code></pre></div>
<pre><code>## Warning: `repeats` has no meaning for this resampling method.</code></pre>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
<span class="kw">print</span>(model)</code></pre></div>
<pre><code>## Linear Regression 
## 
## 506 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 405, 405, 404, 405, 405 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   4.875484  0.7316537  3.425015
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Finally, the model you fit with the train() function has the exact same predict() interface as the linear regression models you fit earlier in this chapter.</p>
<p>After fitting a model with train(), you can simply call predict() with new data, e.g:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predict on full Boston dataset and display just the first 20 items</span>
<span class="kw">head</span> (<span class="kw">predict</span> (model, Boston), <span class="dt">n =</span> <span class="dv">20</span>)</code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 30.00384 25.02556 30.56760 28.60704 27.94352 25.25628 23.00181 19.53599 
##        9       10       11       12       13       14       15       16 
## 11.52364 18.92026 18.99950 21.58680 20.90652 19.55290 19.28348 19.29748 
##       17       18       19       20 
## 20.52751 16.91140 16.17801 18.40614</code></pre>
</div>
</div>
<div id="classification-models-fitting-them-and-evaluating-their-performance" class="section level2">
<h2><span class="header-section-number">4.2</span> Classification models: fitting them and evaluating their performance</h2>
<p>In classification models you are trying to predict a categorical target e.g. whether a customer is satisfied or not. It it still a form of supervised learning and we can use a train/test split to evaluate performance. In this sectgion we will use the Sonar dataset which contains charecteristics of a sonar signal for objects that are either rocks or mines.</p>
<p>The variables explain some part of the signal, then the class of either Rock or Mine - the data below shows the first 7 variables and the class, for a random sample of 10 rows. As the dataset is very small, we use a 40% test split.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlbench)
<span class="kw">data</span>(Sonar)
Sonar[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Sonar), <span class="dv">10</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>), <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="dv">61</span>)]</code></pre></div>
<pre><code>##         V1     V2     V3     V4     V5     V6     V7 Class
## 3   0.0262 0.0582 0.1099 0.1083 0.0974 0.2280 0.2431     R
## 31  0.0240 0.0218 0.0324 0.0569 0.0330 0.0513 0.0897     R
## 131 0.0443 0.0446 0.0235 0.1008 0.2252 0.2611 0.2061     M
## 77  0.0239 0.0189 0.0466 0.0440 0.0657 0.0742 0.1380     R
## 118 0.0228 0.0106 0.0130 0.0842 0.1117 0.1506 0.1776     M
## 46  0.0408 0.0653 0.0397 0.0604 0.0496 0.1817 0.1178     R
## 47  0.0308 0.0339 0.0202 0.0889 0.1570 0.1750 0.0920     R
## 130 0.1371 0.1226 0.1385 0.1484 0.1776 0.1428 0.1773     M
## 26  0.0201 0.0026 0.0138 0.0062 0.0133 0.0151 0.0541     R
## 86  0.0365 0.1632 0.1636 0.1421 0.1130 0.1306 0.2112     R</code></pre>
<p>TO mkae our 60/40 split we do the following.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Shuffle row indices: rows</span>
rows &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(Sonar))

<span class="co"># Randomly order data: Sonar</span>
Sonar &lt;-<span class="st"> </span>Sonar[rows, ]

<span class="co"># Identify row to split on: split</span>
split &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">nrow</span>(Sonar) <span class="op">*</span><span class="st"> </span><span class="fl">0.6</span>)

<span class="co"># Create train</span>
train &lt;-<span class="st"> </span>Sonar[<span class="dv">1</span><span class="op">:</span>split, ]

<span class="co"># Create test</span>
test &lt;-<span class="st"> </span>Sonar[(split <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(Sonar), ]</code></pre></div>
<p>Now you can fit a logistic regression model to your training set using the glm() function. glm() is a more advanced version of lm() that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.</p>
<p>Don’t worry about warnings like glm.fit: algorithm did not converge or glm.fit: fitted probabilities numerically 0 or 1 occurred. These are common on smaller datasets and usually don’t cause any issues. They typically mean your dataset is perfectly seperable, which can cause problems for the math behind the model, but R’s glm() function is almost always robust enough to handle this case with no problems.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit glm model: model</span>
model &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> Class <span class="op">~</span><span class="st"> </span>., <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">data =</span> train)</code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predict on test: p</span>
p &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<div id="confusion-matrix" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Confusion Matrix</h3>
<p>A confusion Matrix is a table of the predicted results vs the actual results. It is called a Confusion Matrix as it explains how confused the model is between the two classes and highlights instances where one class is confused for the other. The items on the main diagonal are the cases where the model is correct.</p>
<div class="figure">
<img src="images/MLToolbox/Confusion.png" alt="Confusion Matrix" width="310" />
<p class="caption">
(#fig:Confusion Matrix)Confusion Matrix
</p>
</div>
<p>You could make such a contingency table with the table() function in base R, but confusionMatrix() in caret yields a lot of useful ancillary statistics in addition to the base rates in the table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate class probabilities: p_class</span>
p_class &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;R&quot;</span>)

<span class="co"># Create confusion matrix</span>
<span class="kw">confusionMatrix</span>(p_class, test<span class="op">$</span>Class)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 12 24
##          R 32 15
##                                          
##                Accuracy : 0.3253         
##                  95% CI : (0.2265, 0.437)
##     No Information Rate : 0.5301         
##     P-Value [Acc &gt; NIR] : 0.9999         
##                                          
##                   Kappa : -0.3387        
##  Mcnemar&#39;s Test P-Value : 0.3496         
##                                          
##             Sensitivity : 0.2727         
##             Specificity : 0.3846         
##          Pos Pred Value : 0.3333         
##          Neg Pred Value : 0.3191         
##              Prevalence : 0.5301         
##          Detection Rate : 0.1446         
##    Detection Prevalence : 0.4337         
##       Balanced Accuracy : 0.3287         
##                                          
##        &#39;Positive&#39; Class : M              
## </code></pre>
<p>We see that the No Information rate is around 52%, that is just predicting the dominant class all the time, mines. Our accuracy so far is below this which suggests that using a dummy model that always predicts mines is more accurate than the current model. The other values such as sensitivity and specificity give more elements of the confusion matrix:</p>
<ul>
<li>Specificity - true negative rate<br />
</li>
<li>Sensitivity - true positive rate</li>
</ul>
<p>Setting our threshold rate - currently 50% - is an exercise in deciding what is more important and depends on the cost-benefit analysis of the problem at hand. Do we want to flag more non-mines as mines, or do we want to be more accurate in our prediction but potentially miss some mines?</p>
<p>Now what happens if we apply a 90% threshold to our model?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Apply threshold of 0.9: p_class</span>
p_class &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">9</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;R&quot;</span>)

<span class="co"># Create confusion matrix</span>
<span class="kw">confusionMatrix</span>(p_class, test<span class="op">$</span>Class)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 12 24
##          R 32 15
##                                          
##                Accuracy : 0.3253         
##                  95% CI : (0.2265, 0.437)
##     No Information Rate : 0.5301         
##     P-Value [Acc &gt; NIR] : 0.9999         
##                                          
##                   Kappa : -0.3387        
##  Mcnemar&#39;s Test P-Value : 0.3496         
##                                          
##             Sensitivity : 0.2727         
##             Specificity : 0.3846         
##          Pos Pred Value : 0.3333         
##          Neg Pred Value : 0.3191         
##              Prevalence : 0.5301         
##          Detection Rate : 0.1446         
##    Detection Prevalence : 0.4337         
##       Balanced Accuracy : 0.3287         
##                                          
##        &#39;Positive&#39; Class : M              
## </code></pre>
<p>Or at 10%</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Apply threshold of 0.10: p_class</span>
p_class &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">1</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;R&quot;</span>)

<span class="co"># Create confusion matrix</span>
<span class="kw">confusionMatrix</span>(p_class, test<span class="op">$</span>Class)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 13 24
##          R 31 15
##                                           
##                Accuracy : 0.3373          
##                  95% CI : (0.2372, 0.4495)
##     No Information Rate : 0.5301          
##     P-Value [Acc &gt; NIR] : 0.9999          
##                                           
##                   Kappa : -0.3167         
##  Mcnemar&#39;s Test P-Value : 0.4185          
##                                           
##             Sensitivity : 0.2955          
##             Specificity : 0.3846          
##          Pos Pred Value : 0.3514          
##          Neg Pred Value : 0.3261          
##              Prevalence : 0.5301          
##          Detection Rate : 0.1566          
##    Detection Prevalence : 0.4458          
##       Balanced Accuracy : 0.3400          
##                                           
##        &#39;Positive&#39; Class : M               
## </code></pre>
</div>
<div id="the-roc-curve" class="section level3">
<h3><span class="header-section-number">4.2.2</span> The ROC Curve</h3>
<p>Rather than manually calculating the true positive and true negative rate for many hundreds of different models - at carying classification cut offs as we previously did - we can plot a ROC (Receiver Operator Charecteristic) curve automatically for every possible threshold. Originally the ROC curve was designed to look at the trade off with different thresholds when trying to differentiate between radar signals of flocks of birds vs planes.</p>
<p>The ROC curve plots the false positive rate (X Axis) vs the true positive rate (Y axis) with each point representing a different threshold.</p>
<p>There are a number of different ways of calculating ROC curves, one we will use here is the caTools package and calculates the AUC or Area under the curve.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caTools)

<span class="co"># Predict on test: p</span>
p &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># Make ROC curve</span>
<span class="kw">colAUC</span>(p, test<span class="op">$</span>Class, <span class="dt">plotROC =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre><code>##              [,1]
## M vs. R 0.6564685</code></pre>
<p>Whne looking at the ROC curve, a line close to the diagonal would be considered bad as it is following something not far off random predictions. Perfection seperation would produce a box, with a single point at 1, 0 or the top left hand corner which would be a 100% true positive rate and a 0% false positive rate.</p>
<p>The AUC for the ‘perfect’ model is 1. For the random model it is 0.5. The AUC statistics is a single number accuracy measure, it summarises the performance of the model across all possible classification thresholds. Most models are in the 0.5 to 1 range, some bad models can be in the 0.4 range. We can think of them as an exam grade e.g.</p>
<ul>
<li>A = 0.9<br />
</li>
<li>B = 0.8<br />
</li>
<li>C = 0.7<br />
</li>
<li>…<br />
</li>
<li>F = 0.5</li>
</ul>
<p>0.8 or above is good, some models in the 0.7 range are useful. The caret package will calculate the AUC for us.</p>
<p>You can use the trainControl() function in caret to use AUC (instead of acccuracy), to tune the parameters of your models. The twoClassSummary() convenience function allows you to do this easily.</p>
<p>When using twoClassSummary(), be sure to always include the argument classProbs = TRUE or your model will throw an error. You cannot calculate AUC with just class predictions. You need to have class probabilities as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create trainControl object: myControl</span>
myControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
  <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
  <span class="dt">number =</span> <span class="dv">10</span>,
  <span class="dt">summaryFunction =</span> twoClassSummary,
  <span class="dt">classProbs =</span> <span class="ot">TRUE</span>, <span class="co"># IMPORTANT!</span>
  <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p>Now that you have a custom trainControl object, it’s easy to fit caret models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom trainControl object to the train() function via the trControl argument, e.g.:</p>
<blockquote>
<p>train(<standard arguments here>, trControl = myControl)</p>
</blockquote>
<p>This syntax gives you a convenient way to store a lot of custom modeling parameters and then use them across multiple different calls to train()</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train glm with custom trainControl: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(Class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Sonar, 
                 <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, 
                 <span class="dt">trControl =</span> myControl)</code></pre></div>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre><code>## + Fold01: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold01: parameter=none 
## + Fold02: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold02: parameter=none 
## + Fold03: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold03: parameter=none 
## + Fold04: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold04: parameter=none 
## + Fold05: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold05: parameter=none 
## + Fold06: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold06: parameter=none 
## + Fold07: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold07: parameter=none 
## + Fold08: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold08: parameter=none 
## + Fold09: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold09: parameter=none 
## + Fold10: parameter=none</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 208 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 187, 187, 187, 187, 187, 188, ... 
## Resampling results:
## 
##   ROC        Sens       Spec     
##   0.7539899  0.7295455  0.7522222</code></pre>
</div>
<div id="tuning-model-parameters-to-improve-performance" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Tuning model parameters to improve performance</h3>
<p>Next we will look at random forests as these are robust against over-fitting. They can yield accurate, non-linear models without much additional work. However, unlinke linear models they have so called hyperparameters to tune and they have to be manually specified by the data scientist as inputs in to the model. These parameters can vary from dataset to dataset, the defaults can be ok, but often need adjusting.</p>
<p>With random forests we fit a different tree to each bootstrap sample of the data, called bagging - we can think of this as draw a random ‘bag’ of observations, at random with replacement, from the original TRAIN dataset many times - to which we think fit a tree on this subset of data. This effectivly creates multiple train/test splits on the TRAIN data and trains a different model, so we end up with an ensemble of different models.</p>
<p>Note - boosting is similar to bagging, in that we take a random ‘bag’ of data and fit a model. However we fit the models iteratively with boosting, with subsequent models fitted to those observations which were not previously fitted as well. When we draw another sample or bag, we weight the probability of observation selection based on how well the previous model fitted those observations, if they were poorly fitted, the new model is more likely to sample those and therefore more likely to accuratley predict them. We then re-inputt all the TRAIN data to our two models and see which observations were then, based on this two model ensemble, not predicted as well. When then try to build a new model in which these previously under-predicted observations are better predicted, then build the ensemble again and iterate as neccessary.</p>
<p>Random forests take bagging a step further, by randomly sampling the columns (variables) at each split and helps to yield more accurate models. It does this to try and de-correlate the errors in different bootstrap samples i.e. different models. You make errors everywhere, but they have different origins/models.</p>
<p>Bagging can therefore be run in parallel, since each bag or draw is independent of the others. In Boosting, this is not possible, since subsequent draws are dependent on previous draws/models.</p>
<p>ISR notes:</p>
<p>“While bagging can improve predictions for many regression methods, it is particularly useful for decision trees. To apply bagging to regression trees, we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.” (pg 317)</p>
<p>And</p>
<p>“Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set…Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm.”</p>
<p>So bagging tends to produce large (deep) trees with high variance, where as boosting produces shallow trees with high bias.</p>
<p>Fitting a random forest model is exactly the same as fitting a generalized linear regression model, as you did in the previous chapter. You simply change the method argument in the train function to be “ranger”. The ranger package is a rewrite of R’s classic randomForest package and fits models much faster, but gives almost exactly the same results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the wine data</span>
wine &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./files/MLToolbox/wine_100.rds&quot;</span>)

<span class="co"># Fit random forest: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  quality <span class="op">~</span>.,
  <span class="dt">tuneLength =</span> <span class="dv">1</span>,
  <span class="dt">data =</span> wine, <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>, <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>)
)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## + Fold1: mtry=3, splitrule=variance 
## - Fold1: mtry=3, splitrule=variance 
## + Fold1: mtry=3, splitrule=extratrees 
## - Fold1: mtry=3, splitrule=extratrees 
## + Fold2: mtry=3, splitrule=variance 
## - Fold2: mtry=3, splitrule=variance 
## + Fold2: mtry=3, splitrule=extratrees 
## - Fold2: mtry=3, splitrule=extratrees 
## + Fold3: mtry=3, splitrule=variance 
## - Fold3: mtry=3, splitrule=variance 
## + Fold3: mtry=3, splitrule=extratrees 
## - Fold3: mtry=3, splitrule=extratrees 
## + Fold4: mtry=3, splitrule=variance 
## - Fold4: mtry=3, splitrule=variance 
## + Fold4: mtry=3, splitrule=extratrees 
## - Fold4: mtry=3, splitrule=extratrees 
## + Fold5: mtry=3, splitrule=variance 
## - Fold5: mtry=3, splitrule=variance 
## + Fold5: mtry=3, splitrule=extratrees 
## - Fold5: mtry=3, splitrule=extratrees 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 3, splitrule = variance on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Random Forest 
## 
## 100 samples
##  12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 81, 80, 80, 79, 80 
## Resampling results across tuning parameters:
## 
##   splitrule   RMSE       Rsquared   MAE      
##   variance    0.6579440  0.3157395  0.4978594
##   extratrees  0.6842383  0.2852057  0.5194518
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 3
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were mtry = 3 and splitrule = variance.</code></pre>
<p>A difference between random forests and linear models is that random forests require tuning, this is done using “hyperparameters” and is done before fitting the model. One such variable is <em>mtry</em> which is the number of randomly selected variables used in each split. It is not always possible to know which is better for your data in advance. But caret can help to automate the selection of these parameters using something called <em>grid search</em>. It will select the appropriate hyperparameters based on out-of-sample error.</p>
<p>Random forest models have a primary tuning parameter of mtry, which controls how many variables are exposed to the splitting search routine at each split. For example, suppose that a tree has a total of 10 splits and mtry = 2. This means that there are 10 samples of 2 predictors each time a split is evaluated.</p>
<p>We will use a larger tuning grid this time, but stick to the defaults provided by the train() function. We try a tuneLength of 3, rather than 1, to explore some more potential models, and plot the resulting model using the plot function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit random forest: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  quality <span class="op">~</span>.,
  <span class="dt">tuneLength =</span> <span class="dv">3</span>,
  <span class="dt">data =</span> wine, <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>, <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>)
)</code></pre></div>
<pre><code>## + Fold1: mtry= 2, splitrule=variance 
## - Fold1: mtry= 2, splitrule=variance 
## + Fold1: mtry= 7, splitrule=variance 
## - Fold1: mtry= 7, splitrule=variance 
## + Fold1: mtry=12, splitrule=variance 
## - Fold1: mtry=12, splitrule=variance 
## + Fold1: mtry= 2, splitrule=extratrees 
## - Fold1: mtry= 2, splitrule=extratrees 
## + Fold1: mtry= 7, splitrule=extratrees 
## - Fold1: mtry= 7, splitrule=extratrees 
## + Fold1: mtry=12, splitrule=extratrees 
## - Fold1: mtry=12, splitrule=extratrees 
## + Fold2: mtry= 2, splitrule=variance 
## - Fold2: mtry= 2, splitrule=variance 
## + Fold2: mtry= 7, splitrule=variance 
## - Fold2: mtry= 7, splitrule=variance 
## + Fold2: mtry=12, splitrule=variance 
## - Fold2: mtry=12, splitrule=variance 
## + Fold2: mtry= 2, splitrule=extratrees 
## - Fold2: mtry= 2, splitrule=extratrees 
## + Fold2: mtry= 7, splitrule=extratrees 
## - Fold2: mtry= 7, splitrule=extratrees 
## + Fold2: mtry=12, splitrule=extratrees 
## - Fold2: mtry=12, splitrule=extratrees 
## + Fold3: mtry= 2, splitrule=variance 
## - Fold3: mtry= 2, splitrule=variance 
## + Fold3: mtry= 7, splitrule=variance 
## - Fold3: mtry= 7, splitrule=variance 
## + Fold3: mtry=12, splitrule=variance 
## - Fold3: mtry=12, splitrule=variance 
## + Fold3: mtry= 2, splitrule=extratrees 
## - Fold3: mtry= 2, splitrule=extratrees 
## + Fold3: mtry= 7, splitrule=extratrees 
## - Fold3: mtry= 7, splitrule=extratrees 
## + Fold3: mtry=12, splitrule=extratrees 
## - Fold3: mtry=12, splitrule=extratrees 
## + Fold4: mtry= 2, splitrule=variance 
## - Fold4: mtry= 2, splitrule=variance 
## + Fold4: mtry= 7, splitrule=variance 
## - Fold4: mtry= 7, splitrule=variance 
## + Fold4: mtry=12, splitrule=variance 
## - Fold4: mtry=12, splitrule=variance 
## + Fold4: mtry= 2, splitrule=extratrees 
## - Fold4: mtry= 2, splitrule=extratrees 
## + Fold4: mtry= 7, splitrule=extratrees 
## - Fold4: mtry= 7, splitrule=extratrees 
## + Fold4: mtry=12, splitrule=extratrees 
## - Fold4: mtry=12, splitrule=extratrees 
## + Fold5: mtry= 2, splitrule=variance 
## - Fold5: mtry= 2, splitrule=variance 
## + Fold5: mtry= 7, splitrule=variance 
## - Fold5: mtry= 7, splitrule=variance 
## + Fold5: mtry=12, splitrule=variance 
## - Fold5: mtry=12, splitrule=variance 
## + Fold5: mtry= 2, splitrule=extratrees 
## - Fold5: mtry= 2, splitrule=extratrees 
## + Fold5: mtry= 7, splitrule=extratrees 
## - Fold5: mtry= 7, splitrule=extratrees 
## + Fold5: mtry=12, splitrule=extratrees 
## - Fold5: mtry=12, splitrule=extratrees 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 12, splitrule = variance on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Random Forest 
## 
## 100 samples
##  12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 80, 80, 81, 80, 79 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE       Rsquared   MAE      
##    2    variance    0.6835887  0.2508610  0.5170790
##    2    extratrees  0.7138749  0.2113001  0.5376460
##    7    variance    0.6674687  0.2907502  0.5100175
##    7    extratrees  0.6980645  0.2258272  0.5296820
##   12    variance    0.6621775  0.2977265  0.5070150
##   12    extratrees  0.7014995  0.2111631  0.5355658
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were mtry = 12 and splitrule
##  = variance.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot model</span>
<span class="kw">plot</span>(model)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>We can pass our own custom tuning grids as a data frame to the train control argument. This is the most fleixble approach, however it can take a lot longer to train the model and can considerably increase run time.</p>
<p>You can provide any number of values for mtry, from 2 up to the number of columns in the dataset. In practice, there are diminishing returns for much larger values of mtry, so we will use a custom tuning grid that explores 2 simple models (mtry = 2 and mtry = 3) as well as one more complicated model (mtry = 7).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Add the tune grid options - note ranger requires a . prior to tuning parameters, other models/packages may not: </span>
tgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
  <span class="dt">.mtry =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">7</span>),
  <span class="dt">.splitrule =</span> <span class="st">&quot;extratrees&quot;</span>
)

<span class="co"># Fit random forest: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  quality <span class="op">~</span>.,
  <span class="dt">tuneGrid =</span> tgrid,
  <span class="dt">data =</span> wine, <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>, <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>)
)</code></pre></div>
<pre><code>## + Fold1: mtry=2, splitrule=extratrees 
## - Fold1: mtry=2, splitrule=extratrees 
## + Fold1: mtry=3, splitrule=extratrees 
## - Fold1: mtry=3, splitrule=extratrees 
## + Fold1: mtry=7, splitrule=extratrees 
## - Fold1: mtry=7, splitrule=extratrees 
## + Fold2: mtry=2, splitrule=extratrees 
## - Fold2: mtry=2, splitrule=extratrees 
## + Fold2: mtry=3, splitrule=extratrees 
## - Fold2: mtry=3, splitrule=extratrees 
## + Fold2: mtry=7, splitrule=extratrees 
## - Fold2: mtry=7, splitrule=extratrees 
## + Fold3: mtry=2, splitrule=extratrees 
## - Fold3: mtry=2, splitrule=extratrees 
## + Fold3: mtry=3, splitrule=extratrees 
## - Fold3: mtry=3, splitrule=extratrees 
## + Fold3: mtry=7, splitrule=extratrees 
## - Fold3: mtry=7, splitrule=extratrees 
## + Fold4: mtry=2, splitrule=extratrees 
## - Fold4: mtry=2, splitrule=extratrees 
## + Fold4: mtry=3, splitrule=extratrees 
## - Fold4: mtry=3, splitrule=extratrees 
## + Fold4: mtry=7, splitrule=extratrees 
## - Fold4: mtry=7, splitrule=extratrees 
## + Fold5: mtry=2, splitrule=extratrees 
## - Fold5: mtry=2, splitrule=extratrees 
## + Fold5: mtry=3, splitrule=extratrees 
## - Fold5: mtry=3, splitrule=extratrees 
## + Fold5: mtry=7, splitrule=extratrees 
## - Fold5: mtry=7, splitrule=extratrees 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 7, splitrule = extratrees on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Random Forest 
## 
## 100 samples
##  12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 80, 80, 80, 79, 81 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE      
##   2     0.7159368  0.2351616  0.5258831
##   3     0.7174871  0.2189432  0.5313928
##   7     0.7034861  0.2462823  0.5210960
## 
## Tuning parameter &#39;splitrule&#39; was held constant at a value of extratrees
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were mtry = 7 and splitrule
##  = extratrees.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot model</span>
<span class="kw">plot</span>(model)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="introducing-glmnet" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Introducing glmnet</h3>
<p>This is similar to GLM models but has built in model selection. It linear models better deal with collinearity, which is correlation amungst the predictors in a model and can help reduce overfitting for models with small data. There are two main forms - lasso and ridge regression. Both of these methods shrink the coefficents towards zero and can also be useful for very large datasets also.</p>
<ul>
<li>Lasso - penalises non-zero coefficients (l1 norm)</li>
<li>Ridge - penalises absolute magnitude coefficients (l2 norm)</li>
</ul>
<p>The amount that we penalise the coefficients depends on our tuning parameter. We can use cross-validation to help in the selection of the tuning parameter. At very high values of the tuning parameter (lambda), we penalise the indivdual variables so much they effectively have a value of zero. At very low levels, we penalise the variables so little that they approach their linear (least squared) equivalents. In both instances, we try to end up with parsimonous models. We can fit lasso and ridge together using glmnet using the alpha paramater - 1 is pure lasso, 0 is pure ridge.</p>
<p>The dataset used comes has nearly as many columns as rows. This is a simulated dataset based on the “don’t overfit” competition on Kaggle a number of years ago. Classification problems are a little more complicated than regression problems because you have to provide a custom summaryFunction to the train() function to use the AUC metric to rank your models. Start by making a custom trainControl. Be sure to set classProbs = TRUE, otherwise the twoClassSummary for summaryFunction will break.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create custom trainControl: myControl</span>
myControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
  <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>,
  <span class="dt">summaryFunction =</span> twoClassSummary,
  <span class="dt">classProbs =</span> <span class="ot">TRUE</span>, <span class="co"># IMPORTANT!</span>
  <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<p>glmnet is capable of fitting two different kinds of penalized models, controlled by the alpha parameter:</p>
<p>Ridge regression (or alpha = 0) Lasso regression (or alpha = 1)</p>
<p>We now fit a glmnet model to the “don’t overfit” dataset using the defaults provided by the caret package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">overfit &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./files/MLToolbox/overfit.csv&quot;</span>)

<span class="co"># Fit glmnet model: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  y <span class="op">~</span>., overfit,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">trControl =</span> myControl
)</code></pre></div>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre><code>## Warning: package &#39;glmnet&#39; was built under R version 3.4.4</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loaded glmnet 2.0-13</code></pre>
<pre><code>## + Fold01: alpha=0.10, lambda=0.01013 
## - Fold01: alpha=0.10, lambda=0.01013 
## + Fold01: alpha=0.55, lambda=0.01013 
## - Fold01: alpha=0.55, lambda=0.01013 
## + Fold01: alpha=1.00, lambda=0.01013 
## - Fold01: alpha=1.00, lambda=0.01013 
## + Fold02: alpha=0.10, lambda=0.01013 
## - Fold02: alpha=0.10, lambda=0.01013 
## + Fold02: alpha=0.55, lambda=0.01013 
## - Fold02: alpha=0.55, lambda=0.01013 
## + Fold02: alpha=1.00, lambda=0.01013 
## - Fold02: alpha=1.00, lambda=0.01013 
## + Fold03: alpha=0.10, lambda=0.01013 
## - Fold03: alpha=0.10, lambda=0.01013 
## + Fold03: alpha=0.55, lambda=0.01013 
## - Fold03: alpha=0.55, lambda=0.01013 
## + Fold03: alpha=1.00, lambda=0.01013 
## - Fold03: alpha=1.00, lambda=0.01013 
## + Fold04: alpha=0.10, lambda=0.01013 
## - Fold04: alpha=0.10, lambda=0.01013 
## + Fold04: alpha=0.55, lambda=0.01013 
## - Fold04: alpha=0.55, lambda=0.01013 
## + Fold04: alpha=1.00, lambda=0.01013 
## - Fold04: alpha=1.00, lambda=0.01013 
## + Fold05: alpha=0.10, lambda=0.01013 
## - Fold05: alpha=0.10, lambda=0.01013 
## + Fold05: alpha=0.55, lambda=0.01013 
## - Fold05: alpha=0.55, lambda=0.01013 
## + Fold05: alpha=1.00, lambda=0.01013 
## - Fold05: alpha=1.00, lambda=0.01013 
## + Fold06: alpha=0.10, lambda=0.01013 
## - Fold06: alpha=0.10, lambda=0.01013 
## + Fold06: alpha=0.55, lambda=0.01013 
## - Fold06: alpha=0.55, lambda=0.01013 
## + Fold06: alpha=1.00, lambda=0.01013 
## - Fold06: alpha=1.00, lambda=0.01013 
## + Fold07: alpha=0.10, lambda=0.01013 
## - Fold07: alpha=0.10, lambda=0.01013 
## + Fold07: alpha=0.55, lambda=0.01013 
## - Fold07: alpha=0.55, lambda=0.01013 
## + Fold07: alpha=1.00, lambda=0.01013 
## - Fold07: alpha=1.00, lambda=0.01013 
## + Fold08: alpha=0.10, lambda=0.01013 
## - Fold08: alpha=0.10, lambda=0.01013 
## + Fold08: alpha=0.55, lambda=0.01013 
## - Fold08: alpha=0.55, lambda=0.01013 
## + Fold08: alpha=1.00, lambda=0.01013 
## - Fold08: alpha=1.00, lambda=0.01013 
## + Fold09: alpha=0.10, lambda=0.01013 
## - Fold09: alpha=0.10, lambda=0.01013 
## + Fold09: alpha=0.55, lambda=0.01013 
## - Fold09: alpha=0.55, lambda=0.01013 
## + Fold09: alpha=1.00, lambda=0.01013 
## - Fold09: alpha=1.00, lambda=0.01013 
## + Fold10: alpha=0.10, lambda=0.01013 
## - Fold10: alpha=0.10, lambda=0.01013 
## + Fold10: alpha=0.55, lambda=0.01013 
## - Fold10: alpha=0.55, lambda=0.01013 
## + Fold10: alpha=1.00, lambda=0.01013 
## - Fold10: alpha=1.00, lambda=0.01013 
## Aggregating results
## Selecting tuning parameters
## Fitting alpha = 1, lambda = 0.0101 on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## glmnet 
## 
## 250 samples
## 200 predictors
##   2 classes: &#39;class1&#39;, &#39;class2&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 225, 225, 225, 224, 224, 226, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        ROC        Sens  Spec     
##   0.10   0.0001012745  0.4420290  0     0.9699275
##   0.10   0.0010127448  0.4462862  0     0.9784420
##   0.10   0.0101274483  0.4461051  0     0.9913043
##   0.55   0.0001012745  0.4247283  0     0.9572464
##   0.55   0.0010127448  0.4223732  0     0.9701087
##   0.55   0.0101274483  0.4367754  0     0.9869565
##   1.00   0.0001012745  0.3647645  0     0.9438406
##   1.00   0.0010127448  0.3645833  0     0.9655797
##   1.00   0.0101274483  0.4499094  0     0.9869565
## 
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were alpha = 1 and lambda = 0.01012745.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print maximum ROC statistic</span>
<span class="kw">max</span>(model[[<span class="st">&quot;results&quot;</span>]])</code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>glmnet model have two tuning paramets, alpha (ridge to lasso) and lambda (size of the penalty). However for a single alpha value, glmnet will fit all values of lambda simultaneously. We are fitting a number of different models in effect, then see which fits the best. We could try for instance two values of alpha - 0 and 1 - then have a wide range of lambdas which we can fit using the seqeunce function.</p>
<p>We use a custom tuning grid as the default tuning grid is very small and there are many more potential glmnet models we may want to explore.</p>
<p>As previously mentioned, the glmnet model actually fits many models at once (one of the great things about the package). You can exploit this by passing a large number of lambda values, which control the amount of penalization in the model. train() is smart enough to only fit one model per alpha value and pass all of the lambda values at once for simultaneous fitting.</p>
<p>A good tuning grid for glmnet models is:</p>
<blockquote>
<p>expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 100))</p>
</blockquote>
<p>This grid explores a large number of lambda values (100, in fact), from a very small one to a very large one. (You could increase the maximum lambda to 10, but in this exercise 1 is a good upper bound.)</p>
<p>If you want to explore fewer models, you can use a shorter lambda sequence. For example, lambda = seq(0.0001, 1, length = 10) would fit 10 models per value of alpha.</p>
<p>You also look at the two forms of penalized models with this tuneGrid: ridge regression and lasso regression. alpha = 0 is pure ridge regression, and alpha = 1 is pure lasso regression. You can fit a mixture of the two models (i.e. an elastic net) using an alpha between 0 and 1. For example, alpha = .05 would be 95% ridge regression and 5% lasso regression.</p>
<p>In this problem you’ll just explore the 2 extremes - pure ridge and pure lasso regression - for the purpose of illustrating their differences.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train glmnet with custom trainControl and tuning: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  y <span class="op">~</span>., overfit,
  <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">alpha =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>,
  <span class="dt">lambda =</span> <span class="kw">seq</span>(<span class="fl">0.0001</span>, <span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>)),
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">trControl =</span> myControl
)</code></pre></div>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was
## not in the result set. ROC will be used instead.</code></pre>
<pre><code>## + Fold01: alpha=0, lambda=1 
## - Fold01: alpha=0, lambda=1 
## + Fold01: alpha=1, lambda=1 
## - Fold01: alpha=1, lambda=1 
## + Fold02: alpha=0, lambda=1 
## - Fold02: alpha=0, lambda=1 
## + Fold02: alpha=1, lambda=1 
## - Fold02: alpha=1, lambda=1 
## + Fold03: alpha=0, lambda=1 
## - Fold03: alpha=0, lambda=1 
## + Fold03: alpha=1, lambda=1 
## - Fold03: alpha=1, lambda=1 
## + Fold04: alpha=0, lambda=1 
## - Fold04: alpha=0, lambda=1 
## + Fold04: alpha=1, lambda=1 
## - Fold04: alpha=1, lambda=1 
## + Fold05: alpha=0, lambda=1 
## - Fold05: alpha=0, lambda=1 
## + Fold05: alpha=1, lambda=1 
## - Fold05: alpha=1, lambda=1 
## + Fold06: alpha=0, lambda=1 
## - Fold06: alpha=0, lambda=1 
## + Fold06: alpha=1, lambda=1 
## - Fold06: alpha=1, lambda=1 
## + Fold07: alpha=0, lambda=1 
## - Fold07: alpha=0, lambda=1 
## + Fold07: alpha=1, lambda=1 
## - Fold07: alpha=1, lambda=1 
## + Fold08: alpha=0, lambda=1 
## - Fold08: alpha=0, lambda=1 
## + Fold08: alpha=1, lambda=1 
## - Fold08: alpha=1, lambda=1 
## + Fold09: alpha=0, lambda=1 
## - Fold09: alpha=0, lambda=1 
## + Fold09: alpha=1, lambda=1 
## - Fold09: alpha=1, lambda=1 
## + Fold10: alpha=0, lambda=1 
## - Fold10: alpha=0, lambda=1 
## + Fold10: alpha=1, lambda=1 
## - Fold10: alpha=1, lambda=1 
## Aggregating results
## Selecting tuning parameters
## Fitting alpha = 1, lambda = 0.0527 on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## glmnet 
## 
## 250 samples
## 200 predictors
##   2 classes: &#39;class1&#39;, &#39;class2&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 224, 226, 226, 225, 225, 225, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda      ROC        Sens  Spec     
##   0      0.00010000  0.3849638  0.00  0.9699275
##   0      0.05272632  0.4043478  0.00  1.0000000
##   0      0.10535263  0.4024457  0.00  1.0000000
##   0      0.15797895  0.4002717  0.00  1.0000000
##   0      0.21060526  0.4133152  0.00  1.0000000
##   0      0.26323158  0.4133152  0.00  1.0000000
##   0      0.31585789  0.4133152  0.00  1.0000000
##   0      0.36848421  0.4133152  0.00  1.0000000
##   0      0.42111053  0.4154891  0.00  1.0000000
##   0      0.47373684  0.4196558  0.00  1.0000000
##   0      0.52636316  0.4217391  0.00  1.0000000
##   0      0.57898947  0.4217391  0.00  1.0000000
##   0      0.63161579  0.4217391  0.00  1.0000000
##   0      0.68424211  0.4217391  0.00  1.0000000
##   0      0.73686842  0.4217391  0.00  1.0000000
##   0      0.78949474  0.4196558  0.00  1.0000000
##   0      0.84212105  0.4174819  0.00  1.0000000
##   0      0.89474737  0.4153986  0.00  1.0000000
##   0      0.94737368  0.4153986  0.00  1.0000000
##   0      1.00000000  0.4153986  0.00  1.0000000
##   1      0.00010000  0.3384058  0.05  0.9018116
##   1      0.05272632  0.5086957  0.00  1.0000000
##   1      0.10535263  0.5000000  0.00  1.0000000
##   1      0.15797895  0.5000000  0.00  1.0000000
##   1      0.21060526  0.5000000  0.00  1.0000000
##   1      0.26323158  0.5000000  0.00  1.0000000
##   1      0.31585789  0.5000000  0.00  1.0000000
##   1      0.36848421  0.5000000  0.00  1.0000000
##   1      0.42111053  0.5000000  0.00  1.0000000
##   1      0.47373684  0.5000000  0.00  1.0000000
##   1      0.52636316  0.5000000  0.00  1.0000000
##   1      0.57898947  0.5000000  0.00  1.0000000
##   1      0.63161579  0.5000000  0.00  1.0000000
##   1      0.68424211  0.5000000  0.00  1.0000000
##   1      0.73686842  0.5000000  0.00  1.0000000
##   1      0.78949474  0.5000000  0.00  1.0000000
##   1      0.84212105  0.5000000  0.00  1.0000000
##   1      0.89474737  0.5000000  0.00  1.0000000
##   1      0.94737368  0.5000000  0.00  1.0000000
##   1      1.00000000  0.5000000  0.00  1.0000000
## 
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were alpha = 1 and lambda = 0.05272632.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print maximum ROC statistic</span>
<span class="kw">max</span>(model[[<span class="st">&quot;results&quot;</span>]][[<span class="st">&quot;ROC&quot;</span>]])</code></pre></div>
<pre><code>## [1] 0.5086957</code></pre>
</div>
</div>
<div id="preprocessing-your-data" class="section level2">
<h2><span class="header-section-number">4.3</span> Preprocessing your data</h2>
<p>Real world data has missing values and often models don’t know what to missing data. Models can exclude cases where values are missing and it can lead to bias. A better approach is to use median imputation.</p>
<p>In this section we use a version of the Wisconsin Breast Cancer dataset. This dataset presents a classic binary classification problem: 50% of the samples are benign, 50% are malignant, and the challenge is to identify which are which.</p>
<p>This dataset is interesting because many of the predictors contain missing values and most rows of the dataset have at least one missing value. This presents a modeling challenge, because most machine learning algorithms cannot handle missing values out of the box. For example, your first instinct might be to fit a logistic regression model to this data, but prior to doing this you need a strategy for handling the NAs.</p>
<p>Fortunately, the train() function in caret contains an argument called preProcess, which allows you to specify that median imputation should be used to fill in the missing values. In previous chapters, you created models with the train() function using formulas such as y ~ .. An alternative way is to specify the x and y arguments to train(), where x is an object with samples in rows and features in columns and y is a numeric or factor vector containing the outcomes. Said differently, x is a matrix or data frame that contains the whole dataset you’d use for the data argument to the lm() call, for example, but excludes the response variable column; y is a vector that contains just the response variable column.</p>
<p>For this exercise, the argument x to train() is loaded in your workspace as breast_cancer_x and y as breast_cancer_y.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the breast cancer data - loads two datasets x to train as breast_cancer_x and y as breast_cancer_y.</span>
<span class="kw">load</span>(<span class="st">&quot;./files/MLToolbox/BreastCancer.RData&quot;</span>)

<span class="co"># Apply median imputation: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> breast_cancer_x, <span class="dt">y =</span> breast_cancer_y,
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">trControl =</span> myControl,
  <span class="dt">preProcess =</span> <span class="st">&quot;medianImpute&quot;</span>
)</code></pre></div>
<pre><code>## Warning in train.default(x = breast_cancer_x, y = breast_cancer_y, method =
## &quot;glm&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used
## instead.</code></pre>
<pre><code>## + Fold01: parameter=none 
## - Fold01: parameter=none 
## + Fold02: parameter=none 
## - Fold02: parameter=none 
## + Fold03: parameter=none 
## - Fold03: parameter=none 
## + Fold04: parameter=none 
## - Fold04: parameter=none 
## + Fold05: parameter=none 
## - Fold05: parameter=none 
## + Fold06: parameter=none 
## - Fold06: parameter=none 
## + Fold07: parameter=none 
## - Fold07: parameter=none 
## + Fold08: parameter=none 
## - Fold08: parameter=none 
## + Fold09: parameter=none 
## - Fold09: parameter=none 
## + Fold10: parameter=none 
## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 699 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: median imputation (9) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 630, 630, 629, 629, 629, 629, ... 
## Resampling results:
## 
##   ROC        Sens       Spec     
##   0.9915576  0.9694203  0.9376667</code></pre>
<p>There are some problems with median imputation, particularly if there is systematic bias and the data is missing not at random. In effect this means if there is a pattern in missing values, median imputation can miss this. For instance, we often find that higher income individuals are less likely to report their household income in survey data. Note that tree based models tend to be more robust to missing not at random case. An alternative to median imputation is knn imputation, it imputes based on “similar” non missing rows based on some defined variables which we define. KNN is not always better than median imputation, so it is worthwhile trying both knn and median imputation and keep the one which gives the most accurate model.</p>
<p>While this is a lot more complicated to implement in practice than simple median imputation, it is very easy to explore in caret using the preProcess argument to train(). You can simply use preProcess = “knnImpute” to change the method of imputation used prior to model fitting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;RANN&quot;</span>) <span class="co"># installed from dev verison on github</span>

<span class="co"># Apply KNN imputation: model2</span>
model2 &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> breast_cancer_x, <span class="dt">y =</span> breast_cancer_y,
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">trControl =</span> myControl,
  <span class="dt">preProcess =</span> <span class="st">&quot;knnImpute&quot;</span>
)</code></pre></div>
<pre><code>## Warning in train.default(x = breast_cancer_x, y = breast_cancer_y, method =
## &quot;glm&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used
## instead.</code></pre>
<pre><code>## + Fold01: parameter=none 
## - Fold01: parameter=none 
## + Fold02: parameter=none 
## - Fold02: parameter=none 
## + Fold03: parameter=none 
## - Fold03: parameter=none 
## + Fold04: parameter=none 
## - Fold04: parameter=none 
## + Fold05: parameter=none 
## - Fold05: parameter=none 
## + Fold06: parameter=none 
## - Fold06: parameter=none 
## + Fold07: parameter=none 
## - Fold07: parameter=none 
## + Fold08: parameter=none 
## - Fold08: parameter=none 
## + Fold09: parameter=none 
## - Fold09: parameter=none 
## + Fold10: parameter=none 
## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model2</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 699 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: nearest neighbor imputation (9), centered (9), scaled (9) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 629, 628, 629, 629, 629, 630, ... 
## Resampling results:
## 
##   ROC        Sens       Spec     
##   0.9932061  0.9715459  0.9253333</code></pre>
<p>All of the preprocessing steps in the train() function happen in the training set of each cross-validation fold, so the error metrics reported include the effects of the preprocessing.</p>
<p>This includes the imputation method used (e.g. knnImputate or medianImpute). This is useful because it allows you to compare different methods of imputation and choose the one that performs the best out-of-sample.</p>
<p>We can use resamples to get resampled results of the models, then use dotplot to look at the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a set of resampling results from the dataset for each model</span>
resamples &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">median_model =</span> model, <span class="dt">knn_model =</span> model2))

<span class="co"># Plot the results</span>
<span class="kw">dotplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<div id="mutiple-pre-processing-steps" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Mutiple pre-processing steps</h3>
<p>The preProcess command has mutiple options, and different options can be chained together e.g. medin imputation, centre and scale your data, then fit a GLM model. One set of preprocessing functions that is particularly useful for fitting regression models is standardization: centering and scaling. You first center by subtracting the mean of each column from each value in that column, then you scale by dividing by the standard deviation.</p>
<p>Standardization transforms your data such that for each column, the mean is 0 and the standard deviation is 1. This makes it easier for regression models to find a good solution.</p>
<p>Note that the order of operations matters e.g. centre and scaling should happen after median imputation, PCA should happen after this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit glm with median imputation: model1</span>
model1 &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> breast_cancer_x, <span class="dt">y =</span> breast_cancer_y,
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">trControl =</span> myControl,
  <span class="dt">preProcess =</span> <span class="st">&quot;medianImpute&quot;</span>
)</code></pre></div>
<pre><code>## Warning in train.default(x = breast_cancer_x, y = breast_cancer_y, method =
## &quot;glm&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used
## instead.</code></pre>
<pre><code>## + Fold01: parameter=none 
## - Fold01: parameter=none 
## + Fold02: parameter=none 
## - Fold02: parameter=none 
## + Fold03: parameter=none 
## - Fold03: parameter=none 
## + Fold04: parameter=none 
## - Fold04: parameter=none 
## + Fold05: parameter=none 
## - Fold05: parameter=none 
## + Fold06: parameter=none 
## - Fold06: parameter=none 
## + Fold07: parameter=none 
## - Fold07: parameter=none 
## + Fold08: parameter=none 
## - Fold08: parameter=none 
## + Fold09: parameter=none 
## - Fold09: parameter=none 
## + Fold10: parameter=none 
## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model1</span>
model1</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 699 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: median imputation (9) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 630, 629, 629, 630, 629, 629, ... 
## Resampling results:
## 
##   ROC        Sens       Spec     
##   0.9904964  0.9695652  0.9418333</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit glm with median imputation and standardization: model2</span>
model2 &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> breast_cancer_x, <span class="dt">y =</span> breast_cancer_y,
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">trControl =</span> myControl,
  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;medianImpute&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>)
)</code></pre></div>
<pre><code>## Warning in train.default(x = breast_cancer_x, y = breast_cancer_y, method =
## &quot;glm&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used
## instead.</code></pre>
<pre><code>## + Fold01: parameter=none 
## - Fold01: parameter=none 
## + Fold02: parameter=none 
## - Fold02: parameter=none 
## + Fold03: parameter=none 
## - Fold03: parameter=none 
## + Fold04: parameter=none 
## - Fold04: parameter=none 
## + Fold05: parameter=none 
## - Fold05: parameter=none 
## + Fold06: parameter=none 
## - Fold06: parameter=none 
## + Fold07: parameter=none 
## - Fold07: parameter=none 
## + Fold08: parameter=none 
## - Fold08: parameter=none 
## + Fold09: parameter=none 
## - Fold09: parameter=none 
## + Fold10: parameter=none 
## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model2</span>
model2</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 699 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: median imputation (9), centered (9), scaled (9) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 629, 629, 630, 629, 629, 629, ... 
## Resampling results:
## 
##   ROC        Sens      Spec  
##   0.9919122  0.969372  0.9375</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a set of resampling results from the dataset for each model</span>
resamples &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">median_model =</span> model, <span class="dt">Median_and_standardized =</span> model2))

<span class="co"># Plot the results</span>
<span class="kw">dotplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="handling-low-information-predictors" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Handling low-Information Predictors</h3>
<p>Some variables in our dataset may contain very little information e.g. variables that are constant or close to constant. Constant models can cause problems, as a CV fold may end up with one when performing a split. It can be hard to work out what has gone wrong with such a model, as the results are often meaningless, as the metrics can be missing.</p>
<p>To remove constant value columns, we can use the “zv” or zero variance option or “nzv” to remove the nearly constant columns in the preProcess step. Removing such columns/variables reduces model-fitting time without reducing model accuracy.</p>
<p>caret contains a utility function called nearZeroVar() for removing such variables to save time during modeling.</p>
<p>nearZeroVar() takes in data x, then looks at the ratio of the most common value to the second most common value, freqCut, and the percentage of distinct values out of the number of total samples, uniqueCut. By default, caret uses freqCut = 19 and uniqueCut = 10, which is fairly conservative. I like to be a little more aggressive and use freqCut = 2 and uniqueCut = 20 when calling nearZeroVar().</p>
<p>We will use the blood-brain dataset. This is a biochemical dataset in which the task is to predict the following value for a set of biochemical compounds:</p>
<blockquote>
<p>log((concentration of compound in brain) / (concentration of compound in blood))</p>
</blockquote>
<p>This gives a quantitative metric of the compound’s ability to cross the blood-brain barrier, and is useful for understanding the biological properties of that barrier.</p>
<p>One interesting aspect of this dataset is that it contains many variables and many of these variables have extemely low variances. This means that there is very little information in these variables because they mostly consist of a single value (e.g. zero).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the blood brain dataset</span>
<span class="kw">load</span>(<span class="st">&quot;./files/MLToolbox/BloodBrain.RData&quot;</span>)

<span class="co"># Identify near zero variance predictors: remove_cols</span>
remove_cols &lt;-<span class="st"> </span><span class="kw">nearZeroVar</span>(bloodbrain_x, <span class="dt">names =</span> <span class="ot">TRUE</span>, 
                           <span class="dt">freqCut =</span> <span class="dv">2</span>, <span class="dt">uniqueCut =</span> <span class="dv">20</span>)

<span class="co"># Get all column names from bloodbrain_x: all_cols</span>
all_cols &lt;-<span class="st"> </span><span class="kw">names</span>(bloodbrain_x)

<span class="co"># Remove from data: bloodbrain_x_small</span>
bloodbrain_x_small &lt;-<span class="st"> </span>bloodbrain_x[ , <span class="kw">setdiff</span>(all_cols, remove_cols)]</code></pre></div>
<p>Note you can either do this by hand, prior to modeling, using the nearZeroVar() function as shown above. Or this can also be done using the preProcess argument equal to “nzv”. Doing it by hand using nerZeroVar() gives more fine grained control however.</p>
<p>Next we can can fit a glm model to it using the train() function. This model will run faster than using the full dataset and will yield very similar predictive accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit model on reduced data: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> bloodbrain_x_small, <span class="dt">y =</span> bloodbrain_y, <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>)</code></pre></div>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 208 samples
## 112 predictors
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1.699781  0.1032369  1.142914</code></pre>
</div>
<div id="principle-components-analysis-pca" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Principle Components Analysis (PCA)</h3>
<p>This can be used for pre-processing and combines low variance, correlated variables together. They result in perpendicular predictors.</p>
<p>This is an alternative to removing low-variance predictors is to run PCA on your dataset. This is sometimes preferable because it does not throw out all of your data: many different low variance predictors may end up combined into one high variance PCA variable, which might have a positive impact on your model’s accuracy.</p>
<p>This is an especially good trick for linear models: the pca option in the preProcess argument will center and scale your data, combine low variance variables, and ensure that all of your predictors are orthogonal. This creates an ideal dataset for linear regression modeling, and can often improve the accuracy of your models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit glm model using PCA: model</span>
model &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> bloodbrain_x, <span class="dt">y =</span> bloodbrain_y,
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">preProcess =</span> <span class="st">&quot;pca&quot;</span>
)

<span class="co"># Print model to console</span>
model</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 208 samples
## 132 predictors
## 
## Pre-processing: principal component signal extraction (132),
##  centered (132), scaled (132) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.6021496  0.4360237  0.4553297</code></pre>
</div>
</div>
<div id="selecting-models-a-case-study-in-churn-prediction" class="section level2">
<h2><span class="header-section-number">4.4</span> Selecting models: a case study in churn prediction</h2>
<p>In this section we will work with a more realistic dataset - customer churn in a telecoms company. So we can compare different models, we are going to use the same settup for each model - same same number of train and test splits for instance. To do this we can define a trainControl object which will specify which rows are used for modelling and which rows are used for hold outs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the churn data </span>
<span class="kw">load</span>(<span class="st">&quot;./files/MLToolbox/Churn.RData&quot;</span>)

<span class="co"># Create custom indices: myFolds</span>
myFolds &lt;-<span class="st"> </span><span class="kw">createFolds</span>(churn_y, <span class="dt">k =</span> <span class="dv">5</span>)

<span class="co"># Create reusable trainControl object: myControl</span>
myControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
  <span class="dt">summaryFunction =</span> twoClassSummary,
  <span class="dt">classProbs =</span> <span class="ot">TRUE</span>, <span class="co"># IMPORTANT!</span>
  <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>,
  <span class="dt">savePredictions =</span> <span class="ot">TRUE</span>,
  <span class="dt">index =</span> myFolds
)</code></pre></div>
<p>We will first use the glmnet model. Like linear models, this can be easily interpretable and can make a good starting point for modelling. We can look at the outputs to understand key drivers of churn. IT fits quickly and can provide very accurate models. glmnet models are therefore simple, fast and interpretable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit glmnet model: model_glmnet</span>
model_glmnet &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> churn_x, <span class="dt">y =</span> churn_y,
  <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">trControl =</span> myControl
)</code></pre></div>
<pre><code>## + Fold1: alpha=0.10, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold1: alpha=0.10, lambda=0.01821 
## + Fold1: alpha=0.55, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold1: alpha=0.55, lambda=0.01821 
## + Fold1: alpha=1.00, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold1: alpha=1.00, lambda=0.01821 
## + Fold2: alpha=0.10, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold2: alpha=0.10, lambda=0.01821 
## + Fold2: alpha=0.55, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold2: alpha=0.55, lambda=0.01821 
## + Fold2: alpha=1.00, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold2: alpha=1.00, lambda=0.01821 
## + Fold3: alpha=0.10, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold3: alpha=0.10, lambda=0.01821 
## + Fold3: alpha=0.55, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold3: alpha=0.55, lambda=0.01821 
## + Fold3: alpha=1.00, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold3: alpha=1.00, lambda=0.01821 
## + Fold4: alpha=0.10, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold4: alpha=0.10, lambda=0.01821 
## + Fold4: alpha=0.55, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold4: alpha=0.55, lambda=0.01821 
## + Fold4: alpha=1.00, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold4: alpha=1.00, lambda=0.01821 
## + Fold5: alpha=0.10, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold5: alpha=0.10, lambda=0.01821 
## + Fold5: alpha=0.55, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold5: alpha=0.55, lambda=0.01821 
## + Fold5: alpha=1.00, lambda=0.01821</code></pre>
<pre><code>## Warning in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :
## one multinomial or binomial class has fewer than 8 observations; dangerous
## ground</code></pre>
<pre><code>## - Fold5: alpha=1.00, lambda=0.01821 
## Aggregating results
## Selecting tuning parameters
## Fitting alpha = 1, lambda = 0.0182 on full training set</code></pre>
<p>Next we are going try a random forest model, which can often be a useful second step after glmnet. Random forests combines an ensemble of non-linear decision trees into a highly flexible (and usually quite accurate) model. They are slower than glmnet and a bit more ‘black box’ compared to glmnet, but in some situations they can yield much more accurate parameters without much tuning. They also require very little preprocesing. They also capture threshold effects and interactions by default, both of which occur often in real world situations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit random forest: model_rf</span>
model_rf &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">x =</span> churn_x, <span class="dt">y =</span> churn_y,
  <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">trControl =</span> myControl
)</code></pre></div>
<pre><code>## + Fold1: mtry= 2, splitrule=gini 
## - Fold1: mtry= 2, splitrule=gini 
## + Fold1: mtry=36, splitrule=gini 
## - Fold1: mtry=36, splitrule=gini 
## + Fold1: mtry=70, splitrule=gini 
## - Fold1: mtry=70, splitrule=gini 
## + Fold1: mtry= 2, splitrule=extratrees 
## - Fold1: mtry= 2, splitrule=extratrees 
## + Fold1: mtry=36, splitrule=extratrees 
## - Fold1: mtry=36, splitrule=extratrees 
## + Fold1: mtry=70, splitrule=extratrees 
## - Fold1: mtry=70, splitrule=extratrees 
## + Fold2: mtry= 2, splitrule=gini 
## - Fold2: mtry= 2, splitrule=gini 
## + Fold2: mtry=36, splitrule=gini 
## - Fold2: mtry=36, splitrule=gini 
## + Fold2: mtry=70, splitrule=gini 
## - Fold2: mtry=70, splitrule=gini 
## + Fold2: mtry= 2, splitrule=extratrees 
## - Fold2: mtry= 2, splitrule=extratrees 
## + Fold2: mtry=36, splitrule=extratrees 
## - Fold2: mtry=36, splitrule=extratrees 
## + Fold2: mtry=70, splitrule=extratrees 
## - Fold2: mtry=70, splitrule=extratrees 
## + Fold3: mtry= 2, splitrule=gini 
## - Fold3: mtry= 2, splitrule=gini 
## + Fold3: mtry=36, splitrule=gini 
## - Fold3: mtry=36, splitrule=gini 
## + Fold3: mtry=70, splitrule=gini 
## - Fold3: mtry=70, splitrule=gini 
## + Fold3: mtry= 2, splitrule=extratrees 
## - Fold3: mtry= 2, splitrule=extratrees 
## + Fold3: mtry=36, splitrule=extratrees 
## - Fold3: mtry=36, splitrule=extratrees 
## + Fold3: mtry=70, splitrule=extratrees 
## - Fold3: mtry=70, splitrule=extratrees 
## + Fold4: mtry= 2, splitrule=gini 
## - Fold4: mtry= 2, splitrule=gini 
## + Fold4: mtry=36, splitrule=gini 
## - Fold4: mtry=36, splitrule=gini 
## + Fold4: mtry=70, splitrule=gini 
## - Fold4: mtry=70, splitrule=gini 
## + Fold4: mtry= 2, splitrule=extratrees 
## - Fold4: mtry= 2, splitrule=extratrees 
## + Fold4: mtry=36, splitrule=extratrees 
## - Fold4: mtry=36, splitrule=extratrees 
## + Fold4: mtry=70, splitrule=extratrees 
## - Fold4: mtry=70, splitrule=extratrees 
## + Fold5: mtry= 2, splitrule=gini 
## - Fold5: mtry= 2, splitrule=gini 
## + Fold5: mtry=36, splitrule=gini 
## - Fold5: mtry=36, splitrule=gini 
## + Fold5: mtry=70, splitrule=gini 
## - Fold5: mtry=70, splitrule=gini 
## + Fold5: mtry= 2, splitrule=extratrees 
## - Fold5: mtry= 2, splitrule=extratrees 
## + Fold5: mtry=36, splitrule=extratrees 
## - Fold5: mtry=36, splitrule=extratrees 
## + Fold5: mtry=70, splitrule=extratrees 
## - Fold5: mtry=70, splitrule=extratrees 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 36, splitrule = extratrees on full training set</code></pre>
<p>After fitting our models we have to determine which fits the data the best, ensuring they have been trained and tested on the same data. We want to select the model with the highest AUC, but with as little variation (standard deviation) in AUC. To do this, we use the resamples command from caret, provided they have the same training data and use the same trainControl object with preset cross-validation folds. resamples() takes as input a list of models and can be used to compare dozens of models at once.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create model_list</span>
model_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">glmnet =</span> model_glmnet, <span class="dt">random_forest =</span> model_rf)

<span class="co"># Pass model_list to resamples(): resamples</span>
resamples &lt;-<span class="st"> </span><span class="kw">resamples</span>(model_list)

<span class="co"># Summarize the results</span>
<span class="kw">summary</span>(resamples)</code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: glmnet, random_forest 
## Number of resamples: 5 
## 
## ROC 
##                    Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
## glmnet        0.5125995 0.6162688 0.6386286 0.6393789 0.6938550 0.7355429
## random_forest 0.6198055 0.7046857 0.7057913 0.7014449 0.7340849 0.7428571
##               NA&#39;s
## glmnet           0
## random_forest    0
## 
## Sens 
##                    Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
## glmnet        0.8742857 0.9137931 0.9367816 0.9392447 0.9828571 0.9885057
## random_forest 0.9828571 0.9885057 0.9885057 0.9919737 1.0000000 1.0000000
##               NA&#39;s
## glmnet           0
## random_forest    0
## 
## Spec 
##               Min. 1st Qu. Median       Mean    3rd Qu. Max. NA&#39;s
## glmnet           0       0   0.08 0.12707692 0.11538462 0.44    0
## random_forest    0       0   0.00 0.02369231 0.03846154 0.08    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Boxplot of ROC for the models</span>
<span class="kw">bwplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Resamples provides a lot of functions for comparing models, one of them is the boxplot (bwplot) shown above. Or a dot plot as shown below, which is similar to the box plot but in a simpler and can be used whcn comparing many models. We can also get a full density plot of the results using kernel density scores, which can be useful for looking at outliers in particular folds. We can also use a scatter plot for each resample (fold) which highlights which model had the most accurate prediction for each fold (random forest had them for every fold in this plot, shown below).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Dotplot of ROC for the models</span>
<span class="kw">dotplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Density plot of ROC for the models</span>
<span class="kw">densityplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-36-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Scatterplot of ROC for the models</span>
<span class="kw">xyplot</span>(resamples, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>)</code></pre></div>
<p><img src="MLToolbox_files/figure-html/unnamed-chunk-36-3.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="supervised-learning-in-r-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["BI-Notes.pdf", "BI-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
